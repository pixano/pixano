{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/","title":"Pixano API reference","text":"<p>Here you will find the documentation for all of our Python API.</p> <ul> <li> App</li> </ul> <p>The Pixano application, REST API, and CLI entry points.</p> <p> app</p> <ul> <li> Datasets</li> </ul> <p>Backend to construct, import, export, and manipulate datasets.</p> <p> datasets</p> <ul> <li> Features</li> </ul> <p>Schemas and data types for items, views, annotations, and more.</p> <p> features</p> <ul> <li> Utils</li> </ul> <p>Utility functions at the library level.</p> <p> utils</p>"},{"location":"api_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>index</li> <li>app<ul> <li>cli<ul> <li>_schema_loader</li> <li>data</li> <li>init</li> <li>server</li> </ul> </li> <li>display</li> <li>main</li> <li>models<ul> <li>annotations</li> <li>base_schema</li> <li>dataset_info</li> <li>dataset_items</li> <li>datasets</li> <li>embeddings</li> <li>entities</li> <li>item_info</li> <li>items</li> <li>sources</li> <li>table_info</li> <li>utils</li> <li>views</li> </ul> </li> <li>routers<ul> <li>annotations</li> <li>browser</li> <li>dataset_items</li> <li>datasets</li> <li>embeddings</li> <li>entities</li> <li>inference<ul> <li>conditional_generation</li> <li>mask_generation</li> <li>models</li> <li>utils</li> <li>zero_shot_detection</li> </ul> </li> <li>items</li> <li>items_info</li> <li>models</li> <li>sources</li> <li>thumbnail</li> <li>utils</li> <li>views</li> </ul> </li> <li>serve</li> <li>settings</li> </ul> </li> <li>datasets<ul> <li>builders<ul> <li>dataset_builder</li> <li>folders<ul> <li>base</li> <li>image</li> <li>video</li> <li>vqa</li> </ul> </li> </ul> </li> <li>dataset</li> <li>dataset_features_values</li> <li>dataset_info</li> <li>dataset_schema</li> <li>dataset_stat</li> <li>exporters<ul> <li>coco_dataset_exporter</li> <li>dataset_exporter</li> <li>default_json_dataset_exporter</li> <li>default_jsonl_dataset_exporter</li> </ul> </li> <li>queries<ul> <li>table</li> </ul> </li> <li>utils<ul> <li>errors</li> <li>integrity</li> <li>labels</li> <li>mosaic</li> <li>video</li> </ul> </li> <li>workspaces<ul> <li>dataset_items</li> </ul> </li> </ul> </li> <li>features<ul> <li>pyarrow_utils</li> <li>schemas<ul> <li>annotations<ul> <li>annotation</li> <li>bbox</li> <li>camcalibration</li> <li>classification</li> <li>compressed_rle</li> <li>info_extraction</li> <li>keypoints</li> <li>text_generation</li> <li>tracklet</li> </ul> </li> <li>base_schema</li> <li>embeddings<ul> <li>embedding</li> </ul> </li> <li>entities<ul> <li>conversation</li> <li>entity</li> <li>multi_modal_entity</li> <li>named_entity</li> <li>track</li> </ul> </li> <li>items<ul> <li>item</li> </ul> </li> <li>registry</li> <li>schema_group</li> <li>source</li> <li>views<ul> <li>image</li> <li>point_cloud</li> <li>sequence_frame</li> <li>text</li> <li>video</li> <li>view</li> </ul> </li> </ul> </li> <li>types<ul> <li>base_type</li> <li>nd_array_float</li> <li>registry</li> <li>schema_reference</li> </ul> </li> <li>utils<ul> <li>boxes</li> <li>creators</li> <li>image</li> </ul> </li> </ul> </li> <li>inference<ul> <li>mask_generation</li> <li>text_image_conditional_generation</li> <li>zero_shot_detection</li> </ul> </li> <li>utils<ul> <li>python</li> <li>validation</li> </ul> </li> </ul>"},{"location":"api_reference/app/display/","title":"display","text":""},{"location":"api_reference/app/display/#pixano.app.display","title":"<code>pixano.app.display</code>","text":""},{"location":"api_reference/app/display/#pixano.app.display.display_cli","title":"<code>display_cli(url, port)</code>","text":"<p>Display a Pixano app inside a command line interface.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Pixano app URL.</p> required <code>port</code> <code>int</code> <p>Pixano app port.</p> required Source code in <code>pixano/app/display.py</code> <pre><code>def display_cli(url: str, port: int):\n    \"\"\"Display a Pixano app inside a command line interface.\n\n    Args:\n        url: Pixano app URL.\n        port: Pixano app port.\n    \"\"\"\n    print(f\"Please visit {url}:{port} in a web browser.\")\n</code></pre>"},{"location":"api_reference/app/display/#pixano.app.display.display_colab","title":"<code>display_colab(url, port, height)</code>","text":"<p>Display a Pixano app inside a Google Colab.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Pixano app URL.</p> required <code>port</code> <code>int</code> <p>Pixano app port.</p> required <code>height</code> <code>int</code> <p>Frame height.</p> required Source code in <code>pixano/app/display.py</code> <pre><code>def display_colab(url: str, port: int, height: int):\n    \"\"\"Display a Pixano app inside a Google Colab.\n\n    Args:\n        url: Pixano app URL.\n        port: Pixano app port.\n        height: Frame height.\n    \"\"\"\n    # Define frame template\n    shell = \"\"\"\n        (() =&gt; {\n            const url = new URL(%URL%);\n            const port = %PORT%;\n            if (port) {\n                url.port = port;\n            }\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '%HEIGHT%');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    \"\"\"\n\n    # Replace variables in template\n    replacements = [\n        (\"%HEIGHT%\", f\"{height}\"),\n        (\"%PORT%\", f\"{port}\"),\n        (\"%URL%\", json.dumps(url)),\n    ]\n    for k, v in replacements:\n        shell = shell.replace(k, v)\n\n    # Display frame\n    script = IPython.display.Javascript(shell)\n    IPython.display.display(script)\n</code></pre>"},{"location":"api_reference/app/display/#pixano.app.display.display_ipython","title":"<code>display_ipython(url, port, height)</code>","text":"<p>Display a Pixano app inside a Jupyter notebook.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Pixano app URL.</p> required <code>port</code> <code>int</code> <p>Pixano app port.</p> required <code>height</code> <code>int</code> <p>Frame height.</p> required Source code in <code>pixano/app/display.py</code> <pre><code>def display_ipython(url: str, port: int, height: int):\n    \"\"\"Display a Pixano app inside a Jupyter notebook.\n\n    Args:\n        url: Pixano app URL.\n        port: Pixano app port.\n        height: Frame height.\n    \"\"\"\n    # Define frame template\n    shell = \"\"\"\n        &lt;iframe id=\"%HTML_ID%\" width=\"100%\" height=\"%HEIGHT%\" frameborder=\"0\"&gt;\n        &lt;/iframe&gt;\n        &lt;script&gt;\n            (function() {\n                const frame = document.getElementById(%JSON_ID%);\n                const url = new URL(%URL%, window.location);\n                const port = %PORT%;\n                if (port) {\n                    url.port = port;\n                }\n                frame.src = url;\n            })();\n        &lt;/script&gt;\n    \"\"\"\n\n    # Replace variables in template\n    frame_id = f\"frame-{shortuuid.uuid()}\"\n    replacements = [\n        (\"%HTML_ID%\", html.escape(frame_id, quote=True)),\n        (\"%JSON_ID%\", json.dumps(frame_id)),\n        (\"%HEIGHT%\", f\"{height}\"),\n        (\"%PORT%\", f\"{port}\"),\n        (\"%URL%\", json.dumps(url)),\n    ]\n    for k, v in replacements:\n        shell = shell.replace(k, v)\n\n    # Display frame\n    iframe = IPython.display.HTML(shell)\n    IPython.display.display(iframe)\n</code></pre>"},{"location":"api_reference/app/main/","title":"main","text":""},{"location":"api_reference/app/main/#pixano.app.main","title":"<code>pixano.app.main</code>","text":""},{"location":"api_reference/app/main/#pixano.app.main.create_app","title":"<code>create_app(settings=Settings())</code>","text":"<p>Create and configure the Pixano app.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Settings</code> <p>App settings.</p> <code>Settings()</code> <p>Returns:</p> Type Description <code>FastAPI</code> <p>The Pixano app.</p> Source code in <code>pixano/app/main.py</code> <pre><code>def create_app(settings: Settings = Settings()) -&gt; FastAPI:\n    \"\"\"Create and configure the Pixano app.\n\n    Args:\n        settings: App settings.\n\n    Returns:\n        The Pixano app.\n    \"\"\"\n    # Create app\n    app = FastAPI()\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=False,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Mount folders\n    if isinstance(settings.media_dir, S3Path):\n        # If S3, mount models folder\n        # Check if folder exists\n        if settings.models_dir is None:\n            raise FileNotFoundError(\"Local model directory not provided\")\n        elif not settings.models_dir.exists():\n            raise FileNotFoundError(f\"Local model directory '{settings.models_dir.absolute()}' not found\")\n        # Mount\n        app.mount(\n            \"/models\",\n            StaticFiles(directory=settings.models_dir),\n            name=\"models\",\n        )\n    else:\n        # If local, mount media folder and models folder\n        # Check if folder exists\n        if not settings.media_dir.exists():\n            raise FileNotFoundError(f\"Media directory '{settings.media_dir.absolute()}' not found\")\n        if settings.models_dir is None:\n            raise FileNotFoundError(\"Model directory not provided\")\n        # Create models folder in case it doesn't exist yet\n        if not settings.models_dir.exists():\n            settings.models_dir.mkdir(exist_ok=True)\n        # Mount\n        app.mount(\n            \"/media\",\n            StaticFiles(directory=settings.media_dir),\n            name=\"media\",\n        )\n        app.mount(\n            \"/app_models\",\n            StaticFiles(directory=settings.models_dir),\n            name=\"models\",\n        )\n\n    # Include routers\n    app.include_router(annotations_router)\n    app.include_router(dataset_items_router)\n    app.include_router(datasets_router)\n    app.include_router(thumbnail_router)\n    app.include_router(browser_router)\n    app.include_router(embeddings_router)\n    app.include_router(entities_router)\n    app.include_router(items_router)\n    app.include_router(items_info_router)\n    app.include_router(sources_router)\n    app.include_router(views_router)\n    app.include_router(models_router)\n    app.include_router(inference_router)\n\n    return app\n</code></pre>"},{"location":"api_reference/app/serve/","title":"serve","text":""},{"location":"api_reference/app/serve/#pixano.app.serve","title":"<code>pixano.app.serve</code>","text":""},{"location":"api_reference/app/serve/#pixano.app.serve.App","title":"<code>App(data_dir='.', *, library_dir=None, media_dir=None, models_dir=None, aws_endpoint=None, aws_region=None, aws_access_key=None, aws_secret_key=None, host='127.0.0.1', port=7492, pixano_inference_url=None)</code>","text":"<p>The Pixano app.</p> <p>Attributes:</p> Name Type Description <code>app</code> <code>FastAPI</code> <p>FastAPI App.</p> <code>config</code> <code>Config</code> <p>App config.</p> <code>server</code> <code>Server</code> <p>App server.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Root data directory containing library/, media/, and models/ subdirectories.</p> <code>'.'</code> <code>library_dir</code> <code>str | None</code> <p>Override for the library directory. If not provided, defaults to data_dir/library.</p> <code>None</code> <code>media_dir</code> <code>str | None</code> <p>Override for the media directory. If not provided, defaults to data_dir/media.</p> <code>None</code> <code>models_dir</code> <code>str | None</code> <p>Override for the models directory. If not provided, defaults to data_dir/models.</p> <code>None</code> <code>aws_endpoint</code> <code>str | None</code> <p>S3 endpoint URL, use 'AWS' if not provided. Used if library_dir is an S3 path.</p> <code>None</code> <code>aws_region</code> <code>str | None</code> <p>S3 region name, not always required for private storages. Used if library_dir is an S3 path.</p> <code>None</code> <code>aws_access_key</code> <code>str | None</code> <p>S3 AWS access key. Used if library_dir is an S3 path.</p> <code>None</code> <code>aws_secret_key</code> <code>str | None</code> <p>S3 AWS secret key. Used if library_dir is an S3 path.</p> <code>None</code> <code>host</code> <code>str</code> <p>App host.</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>App port.</p> <code>7492</code> <code>pixano_inference_url</code> <code>str | None</code> <p>Pixano inference URL if any.</p> <code>None</code> Source code in <code>pixano/app/serve.py</code> <pre><code>def __init__(\n    self,\n    data_dir: str = \".\",\n    *,\n    library_dir: str | None = None,\n    media_dir: str | None = None,\n    models_dir: str | None = None,\n    aws_endpoint: str | None = None,\n    aws_region: str | None = None,\n    aws_access_key: str | None = None,\n    aws_secret_key: str | None = None,\n    host: str = \"127.0.0.1\",\n    port: int = 7492,\n    pixano_inference_url: str | None = None,\n):\n    \"\"\"Initialize and serve the Pixano app.\n\n    Args:\n        data_dir: Root data directory containing library/, media/,\n            and models/ subdirectories.\n        library_dir: Override for the library directory. If not\n            provided, defaults to data_dir/library.\n        media_dir: Override for the media directory. If not\n            provided, defaults to data_dir/media.\n        models_dir: Override for the models directory. If not\n            provided, defaults to data_dir/models.\n        aws_endpoint: S3 endpoint URL, use 'AWS' if not provided.\n            Used if library_dir is an S3 path.\n        aws_region: S3 region name, not always required for\n            private storages. Used if library_dir is an S3 path.\n        aws_access_key: S3 AWS access key. Used if library_dir is\n            an S3 path.\n        aws_secret_key: S3 AWS secret key. Used if library_dir\n            is an S3 path.\n        host: App host.\n        port: App port.\n        pixano_inference_url: Pixano inference URL if any.\n    \"\"\"\n\n    # Override app settings\n    @lru_cache\n    def get_settings_override():\n        kwargs: dict = {\n            \"data_dir\": data_dir,\n            \"aws_endpoint\": aws_endpoint,\n            \"aws_region\": aws_region,\n            \"aws_access_key\": aws_access_key,\n            \"aws_secret_key\": aws_secret_key,\n            \"pixano_inference_client\": PixanoInferenceClient.connect(pixano_inference_url)\n            if pixano_inference_url is not None\n            else None,\n        }\n        if library_dir is not None:\n            kwargs[\"library_dir\"] = library_dir\n        if media_dir is not None:\n            kwargs[\"media_dir\"] = media_dir\n        if models_dir is not None:\n            kwargs[\"models_dir\"] = models_dir\n        return Settings(**kwargs)\n\n    # Create app\n    settings = get_settings_override()\n    templates = Jinja2Templates(directory=TEMPLATE_PATH)\n    self.app = create_app(settings=settings)\n    self.app.dependency_overrides[get_settings] = get_settings_override\n\n    @self.app.get(\"/\", response_class=HTMLResponse)\n    def main_page(request: fastapi.Request):\n        return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n    @self.app.get(\"/{ds_id}/dataset\", response_class=HTMLResponse)\n    async def dataset_page(request: fastapi.Request):\n        return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n    @self.app.get(\"/{ds_id}/dashboard\", response_class=HTMLResponse)\n    async def dashboard_page(request: fastapi.Request):\n        return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n    @self.app.get(\"/{ds_id}/dataset/{item_id}\", response_class=HTMLResponse)\n    async def item_page(request: fastapi.Request):\n        return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n    try:\n        self.app.mount(\"/_app\", StaticFiles(directory=ASSETS_PATH), name=\"assets\")\n    # TODO: properly define environment variable for production to raise a RuntimeError accordingly\n    except RuntimeError:\n        warnings.warn(\n            \"Pixano app assets not found. If it is a production environment, this is not expected, \"\n            \"check if you have built the assets for the UI.\"\n        )\n    self.config = uvicorn.Config(self.app, host=host, port=port)\n    self.server = uvicorn.Server(self.config)\n\n    # Serve app\n    task_functions[self.get_env()](self.server.serve())  # type: ignore[operator]\n</code></pre>"},{"location":"api_reference/app/serve/#pixano.app.serve.App.display","title":"<code>display(height=1000)</code>","text":"<p>Display the Pixano app.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Frame height.</p> <code>1000</code> Source code in <code>pixano/app/serve.py</code> <pre><code>def display(self, height: int = 1000) -&gt; None:\n    \"\"\"Display the Pixano app.\n\n    Args:\n        height: Frame height.\n    \"\"\"\n    # Wait for app to be online\n    while not self.server.started:\n        task_functions[self.get_env()](asyncio.wait(0.1))  # type: ignore[operator, call-overload]\n\n    # Display app\n    for server in self.server.servers:\n        for socket in server.sockets:\n            address = socket.getsockname()\n            display_functions[self.get_env()](url=f\"http://{address[0]}\", port=address[1], height=height)  # type: ignore[operator]\n</code></pre>"},{"location":"api_reference/app/serve/#pixano.app.serve.App.get_env","title":"<code>get_env()</code>","text":"<p>Get the app's current running environment.</p> <p>Returns:</p> Type Description <code>str</code> <p>Running environment.</p> Source code in <code>pixano/app/serve.py</code> <pre><code>def get_env(self) -&gt; str:\n    \"\"\"Get the app's current running environment.\n\n    Returns:\n        Running environment.\n    \"\"\"\n    # If Google colab import succeeds\n    try:\n        import google.colab  # noqa: F401, I001 #type: ignore\n        import IPython\n    except ImportError:\n        pass\n    else:\n        if IPython.get_ipython() is not None:\n            return \"colab\"\n\n    # If IPython import succeeds\n    try:\n        import IPython\n    except ImportError:\n        pass\n    else:\n        ipython = IPython.get_ipython()\n        if ipython is not None and ipython.has_trait(\"kernel\"):\n            return \"ipython\"\n\n    # Else\n    return \"none\"\n</code></pre>"},{"location":"api_reference/app/settings/","title":"settings","text":""},{"location":"api_reference/app/settings/#pixano.app.settings","title":"<code>pixano.app.settings</code>","text":""},{"location":"api_reference/app/settings/#pixano.app.settings.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Pixano app settings.</p> <p>Attributes:</p> Name Type Description <code>data_dir</code> <code>Path | S3Path</code> <p>Root data directory. If library_dir, media_dir, or models_dir are not provided, they are derived as data_dir/library, data_dir/media, and data_dir/models respectively.</p> <code>library_dir</code> <code>Path | S3Path</code> <p>Local or S3 path to dataset library. Overrides data_dir/library if provided.</p> <code>media_dir</code> <code>Path | S3Path</code> <p>Local or S3 path to media library. Overrides data_dir/media if provided.</p> <code>models_dir</code> <code>Path | None</code> <p>Models directory as Path. Overrides data_dir/models if provided. Must be provided if library_dir is an S3 path.</p> <code>aws_endpoint</code> <code>str | None</code> <p>S3 endpoint URL, use 'AWS' if not provided. Used if library_dir is an S3 path.</p> <code>aws_region</code> <code>str | None</code> <p>S3 region name, not always required for private storages. Used if library_dir is an S3 path.</p> <code>aws_access_key</code> <code>str | None</code> <p>S3 AWS access key. Used if library_dir is an S3 path.</p> <code>aws_secret_key</code> <code>str | None</code> <p>S3 AWS secret key. Used if library_dir is an S3 path.</p>"},{"location":"api_reference/app/settings/#pixano.app.settings.get_settings","title":"<code>get_settings()</code>  <code>cached</code>","text":"<p>Get app settings.</p> <p>Returns:</p> Type Description <code>Settings</code> <p>App settings.</p> Source code in <code>pixano/app/settings.py</code> <pre><code>@lru_cache\ndef get_settings() -&gt; Settings:\n    \"\"\"Get app settings.\n\n    Returns:\n        App settings.\n    \"\"\"\n    return Settings()\n</code></pre>"},{"location":"api_reference/app/cli/_schema_loader/","title":"_schema_loader","text":""},{"location":"api_reference/app/cli/_schema_loader/#pixano.app.cli._schema_loader","title":"<code>pixano.app.cli._schema_loader</code>","text":""},{"location":"api_reference/app/cli/_schema_loader/#pixano.app.cli._schema_loader.load_schema","title":"<code>load_schema(specifier)</code>","text":"<p>Load a DatasetItem subclass from a <code>path/to/file.py:ClassName</code> specifier.</p> <p>Parameters:</p> Name Type Description Default <code>specifier</code> <code>str</code> <p>String in the format <code>path/to/file.py:ClassName</code>.</p> required <p>Returns:</p> Type Description <code>type[DatasetItem]</code> <p>The loaded DatasetItem subclass.</p> <p>Raises:</p> Type Description <code>BadParameter</code> <p>If the specifier format is invalid, the file cannot be loaded, the class is not found, or the class is not a DatasetItem subclass.</p> Source code in <code>pixano/app/cli/_schema_loader.py</code> <pre><code>def load_schema(specifier: str) -&gt; type[DatasetItem]:\n    \"\"\"Load a DatasetItem subclass from a ``path/to/file.py:ClassName`` specifier.\n\n    Args:\n        specifier: String in the format ``path/to/file.py:ClassName``.\n\n    Returns:\n        The loaded DatasetItem subclass.\n\n    Raises:\n        typer.BadParameter: If the specifier format is invalid, the file\n            cannot be loaded, the class is not found, or the class is not\n            a DatasetItem subclass.\n    \"\"\"\n    if \":\" not in specifier:\n        raise typer.BadParameter(f\"Schema must be in 'path/to/file.py:ClassName' format, got '{specifier}'\")\n\n    file_path_str, class_name = specifier.rsplit(\":\", 1)\n    file_path = Path(file_path_str).resolve()\n\n    if not file_path.is_file():\n        raise typer.BadParameter(f\"Schema file not found: {file_path}\")\n\n    spec = importlib.util.spec_from_file_location(\"_user_schema\", file_path)\n    if spec is None or spec.loader is None:\n        raise typer.BadParameter(f\"Cannot load module from: {file_path}\")\n\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n\n    cls = getattr(module, class_name, None)\n    if cls is None:\n        raise typer.BadParameter(f\"Class '{class_name}' not found in {file_path}\")\n\n    if not (isinstance(cls, type) and issubclass(cls, DatasetItem)):\n        raise typer.BadParameter(f\"'{class_name}' is not a DatasetItem subclass\")\n\n    return cls\n</code></pre>"},{"location":"api_reference/app/cli/data/","title":"data","text":""},{"location":"api_reference/app/cli/data/#pixano.app.cli.data","title":"<code>pixano.app.cli.data</code>","text":""},{"location":"api_reference/app/cli/data/#pixano.app.cli.data.DatasetType","title":"<code>DatasetType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported dataset types for folder-based import.</p>"},{"location":"api_reference/app/cli/data/#pixano.app.cli.data.ImportMode","title":"<code>ImportMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Import modes controlling dataset creation behavior.</p>"},{"location":"api_reference/app/cli/data/#pixano.app.cli.data.import_data","title":"<code>import_data(data_dir=typer.Argument(..., exists=True, dir_okay=True, help='Path to data directory.'), source_dir=typer.Argument(..., exists=True, file_okay=False, dir_okay=True, help='Path to source dataset directory containing split folders.'), name=typer.Option(None, help='Dataset name. Defaults to source directory name.'), description=typer.Option('', help='Dataset description.'), dataset_type=typer.Option(None, '--type', help='Dataset type: image, video, or vqa. Prompted if omitted.'), schema=typer.Option(None, help=\"Custom schema in 'path/to/file.py:ClassName' format. Uses the default schema for the type if omitted.\"), mode=typer.Option(ImportMode.create, help='Import mode: create, overwrite, or add.'), use_image_name_as_id=typer.Option(False, help='Use image file name as item ID.'))</code>","text":"<p>Import a dataset from an external source directory.</p> <p>Copies media from SOURCE_DIR into data_dir/media// and builds the dataset. Source code in <code>pixano/app/cli/data.py</code> <pre><code>@data_app.command(name=\"import\")\ndef import_data(\n    data_dir: Path = typer.Argument(..., exists=True, dir_okay=True, help=\"Path to data directory.\"),\n    source_dir: Path = typer.Argument(\n        ...,\n        exists=True,\n        file_okay=False,\n        dir_okay=True,\n        help=\"Path to source dataset directory containing split folders.\",\n    ),\n    name: Optional[str] = typer.Option(None, help=\"Dataset name. Defaults to source directory name.\"),\n    description: str = typer.Option(\"\", help=\"Dataset description.\"),\n    dataset_type: Optional[DatasetType] = typer.Option(\n        None, \"--type\", help=\"Dataset type: image, video, or vqa. Prompted if omitted.\"\n    ),\n    schema: Optional[str] = typer.Option(\n        None,\n        help=\"Custom schema in 'path/to/file.py:ClassName' format. Uses the default schema for the type if omitted.\",\n    ),\n    mode: ImportMode = typer.Option(ImportMode.create, help=\"Import mode: create, overwrite, or add.\"),\n    use_image_name_as_id: bool = typer.Option(False, help=\"Use image file name as item ID.\"),\n) -&gt; None:\n    \"\"\"Import a dataset from an external source directory.\n\n    Copies media from SOURCE_DIR into data_dir/media/&lt;dataset_name&gt;/ and builds the dataset.\n    \"\"\"\n    from pixano.datasets import DatasetInfo\n\n    builder_map = {\n        DatasetType.image: \"pixano.datasets.builders.folders.image.ImageFolderBuilder\",\n        DatasetType.video: \"pixano.datasets.builders.folders.video.VideoFolderBuilder\",\n        DatasetType.vqa: \"pixano.datasets.builders.folders.vqa.VQAFolderBuilder\",\n    }\n\n    # Resolve dataset name\n    dataset_name = name if name is not None else source_dir.name\n\n    # Interactive type detection when --type is not provided\n    if dataset_type is None:\n        detected = _detect_dataset_type(source_dir)\n        typer.echo(\"\\nDataset type determines how your data is displayed in Pixano.\")\n        typer.echo(\"  image - Image dataset with optional object annotations\")\n        typer.echo(\"  video - Video dataset\")\n        typer.echo(\"  vqa   - Visual Question Answering (image + conversations)\")\n        type_str = typer.prompt(\"Select dataset type\", default=detected.value)\n        try:\n            dataset_type = DatasetType(type_str)\n        except ValueError:\n            typer.echo(f\"Error: Invalid type '{type_str}'. Must be one of: image, video, vqa.\", err=True)\n            raise typer.Exit(code=1)\n        typer.echo(\"\")\n\n    # Derive library_dir and media_dir from data_dir\n    resolved_library_dir = data_dir / \"library\"\n    resolved_media_dir = data_dir / \"media\"\n\n    # Copy media from source_dir into data_dir/media/&lt;dataset_name&gt;/\n    dest_media = resolved_media_dir / dataset_name\n\n    if mode == ImportMode.create:\n        if dest_media.exists():\n            typer.echo(f\"Error: '{dest_media}' already exists. Use --mode overwrite or --mode add.\", err=True)\n            raise typer.Exit(code=1)\n        dest_media.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copytree(source_dir, dest_media)\n    elif mode == ImportMode.overwrite:\n        if dest_media.exists():\n            shutil.rmtree(dest_media)\n        dest_media.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copytree(source_dir, dest_media)\n    elif mode == ImportMode.add:\n        dest_media.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copytree(source_dir, dest_media, dirs_exist_ok=True)\n\n    typer.echo(f\"Copied media from '{source_dir}' to '{dest_media}'.\")\n\n    # Resolve builder class\n    import importlib\n\n    module_path, class_name = builder_map[dataset_type].rsplit(\".\", 1)\n    module = importlib.import_module(module_path)\n    builder_cls = getattr(module, class_name)\n\n    # Resolve schema\n    dataset_item = None\n    if schema is not None:\n        dataset_item = load_schema(schema)\n\n    info = DatasetInfo(name=dataset_name, description=description)\n\n    builder = builder_cls(\n        media_dir=resolved_media_dir,\n        library_dir=resolved_library_dir,\n        dataset_item=dataset_item,\n        info=info,\n        dataset_path=dataset_name,\n        use_image_name_as_id=use_image_name_as_id,\n    )\n\n    dataset = builder.build(mode=mode.value)\n    typer.echo(f\"Dataset '{dataset_name}' built successfully ({dataset.num_rows} items).\")\n</code></pre>"},{"location":"api_reference/app/cli/init/","title":"init","text":""},{"location":"api_reference/app/cli/init/#pixano.app.cli.init","title":"<code>pixano.app.cli.init</code>","text":""},{"location":"api_reference/app/cli/init/#pixano.app.cli.init.init","title":"<code>init(data_dir=typer.Argument(..., help='Path to initialize.'))</code>","text":"<p>Initialize a Pixano data directory.</p> Source code in <code>pixano/app/cli/init.py</code> <pre><code>def init(\n    data_dir: Path = typer.Argument(..., help=\"Path to initialize.\"),\n) -&gt; None:\n    \"\"\"Initialize a Pixano data directory.\"\"\"\n    for subdir in [\"library\", \"media\", \"models\"]:\n        (data_dir / subdir).mkdir(parents=True, exist_ok=True)\n    typer.echo(f\"Initialized Pixano data directory at {data_dir}\")\n</code></pre>"},{"location":"api_reference/app/cli/server/","title":"server","text":""},{"location":"api_reference/app/cli/server/#pixano.app.cli.server","title":"<code>pixano.app.cli.server</code>","text":""},{"location":"api_reference/app/cli/server/#pixano.app.cli.server.run","title":"<code>run(data_dir=typer.Argument(..., help='Path to data directory (contains library/, media/, models/).'), aws_endpoint=typer.Option(None, help=\"S3 endpoint URL, use 'AWS' if not provided. Used if library_dir or media_dir is an S3 path.\"), aws_region=typer.Option(None, help='S3 region name, not always required for private storages. Used if library_dir or media_dir is an S3 path.'), aws_access_key=typer.Option(None, help='S3 AWS access key. Used if library_dir or media_dir is an S3 path.'), aws_secret_key=typer.Option(None, help='S3 AWS secret key. Used if library_dir or media_dir is an S3 path.'), host=typer.Option('127.0.0.1', help='Pixano app URL host.'), port=typer.Option(7492, help='Pixano app URL port.'), pixano_inference_url=typer.Option(None, help='Pixano inference API url.'))</code>","text":"<p>Launch the Pixano annotation server.</p> Source code in <code>pixano/app/cli/server.py</code> <pre><code>@server_app.command()\ndef run(\n    data_dir: str = typer.Argument(..., help=\"Path to data directory (contains library/, media/, models/).\"),\n    aws_endpoint: Optional[str] = typer.Option(\n        None,\n        help=\"S3 endpoint URL, use 'AWS' if not provided. Used if library_dir or media_dir is an S3 path.\",\n    ),\n    aws_region: Optional[str] = typer.Option(\n        None,\n        help=\"S3 region name, not always required for private storages.\"\n        \" Used if library_dir or media_dir is an S3 path.\",\n    ),\n    aws_access_key: Optional[str] = typer.Option(\n        None,\n        help=\"S3 AWS access key. Used if library_dir or media_dir is an S3 path.\",\n    ),\n    aws_secret_key: Optional[str] = typer.Option(\n        None,\n        help=\"S3 AWS secret key. Used if library_dir or media_dir is an S3 path.\",\n    ),\n    host: str = typer.Option(\"127.0.0.1\", help=\"Pixano app URL host.\"),\n    port: int = typer.Option(7492, help=\"Pixano app URL port.\"),\n    pixano_inference_url: Optional[str] = typer.Option(\n        None,\n        help=\"Pixano inference API url.\",\n    ),\n) -&gt; None:\n    \"\"\"Launch the Pixano annotation server.\"\"\"\n    from pixano.app.serve import App\n\n    App(\n        data_dir,\n        aws_endpoint=aws_endpoint,\n        aws_region=aws_region,\n        aws_access_key=aws_access_key,\n        aws_secret_key=aws_secret_key,\n        host=host,\n        port=port,\n        pixano_inference_url=pixano_inference_url,\n    )\n</code></pre>"},{"location":"api_reference/app/models/annotations/","title":"annotations","text":""},{"location":"api_reference/app/models/annotations/#pixano.app.models.annotations","title":"<code>pixano.app.models.annotations</code>","text":""},{"location":"api_reference/app/models/annotations/#pixano.app.models.annotations.AnnotationModel","title":"<code>AnnotationModel(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchemaModel[Annotation]</code></p> <p>Model for the Annotation schema.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/app/models/annotations/#pixano.app.models.annotations.AnnotationModel.from_row","title":"<code>from_row(row, table_info)</code>  <code>classmethod</code>","text":"<p>Create an AnnotationModel from an Annotation.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Annotation</code> <p>The row to create the model from.</p> required <code>table_info</code> <code>TableInfo</code> <p>The table info of the row.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The created model.</p> Source code in <code>pixano/app/models/annotations.py</code> <pre><code>@classmethod\ndef from_row(cls, row: Annotation, table_info: TableInfo) -&gt; Self:\n    \"\"\"Create an AnnotationModel from an [Annotation][pixano.features.Annotation].\n\n    Args:\n        row: The row to create the model from.\n        table_info: The table info of the row.\n\n    Returns:\n        The created model.\n    \"\"\"\n    annotation_model = BaseSchemaModel.from_row(row, table_info)\n    annotation_model.data[\"inference_metadata\"] = json.loads(row.inference_metadata)\n    return cls.model_construct(**annotation_model.__dict__)  # Avoid validation and casting\n</code></pre>"},{"location":"api_reference/app/models/annotations/#pixano.app.models.annotations.AnnotationModel.to_row","title":"<code>to_row(dataset)</code>","text":"<p>Create an Annotation from the model.</p> Source code in <code>pixano/app/models/annotations.py</code> <pre><code>def to_row(self, dataset: Dataset) -&gt; Annotation:\n    \"\"\"Create an [Annotation][pixano.features.Annotation] from the model.\"\"\"\n    if not is_annotation(dataset.schema.schemas[self.table_info.name]):\n        raise ValueError(f\"Schema type must be a subclass of {Annotation.__name__}.\")\n    row = super().to_row(dataset)\n    row.inference_metadata = json.dumps(self.data[\"inference_metadata\"])\n    return row\n</code></pre>"},{"location":"api_reference/app/models/base_schema/","title":"base_schema","text":""},{"location":"api_reference/app/models/base_schema/#pixano.app.models.base_schema","title":"<code>pixano.app.models.base_schema</code>","text":""},{"location":"api_reference/app/models/base_schema/#pixano.app.models.base_schema.BaseSchemaModel","title":"<code>BaseSchemaModel(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[T]</code></p> <p>Base schema model.</p> <p>This class is a base class for all schema models. It provides methods to convert a row to a model and vice versa.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier of the row.</p> <code>created_at</code> <code>datetime</code> <p>The creation date of the row.</p> <code>updated_at</code> <code>datetime</code> <p>The last modification date of the row.</p> <code>table_info</code> <code>TableInfo</code> <p>Information about the table to which the row belongs.</p> <code>data</code> <code>dict[str, Any]</code> <p>Dumped data from the Pixano backend row except the id.</p> <p>Raises <code>ValidationError</code> if the input data cannot be validated to form a valid model.</p> <p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p> <p>Parameters:</p> Name Type Description Default <code>created_at</code> <code>datetime | None</code> <p>The creation date of the object.</p> <code>None</code> <code>updated_at</code> <code>datetime | None</code> <p>The last modification date of the object.</p> <code>None</code> <code>data</code> <code>Any</code> <p>The data of the object validated by Pydantic.</p> <code>{}</code> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/app/models/base_schema/#pixano.app.models.base_schema.BaseSchemaModel.from_row","title":"<code>from_row(row, table_info)</code>  <code>classmethod</code>","text":"<p>Create a model from a row.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>T</code> <p>The row to create the model from.</p> required <code>table_info</code> <code>TableInfo</code> <p>The table info of the row.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The created model.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>@classmethod\ndef from_row(cls, row: T, table_info: TableInfo) -&gt; Self:\n    \"\"\"Create a model from a row.\n\n    Args:\n        row: The row to create the model from.\n        table_info: The table info of the row.\n\n    Returns:\n        The created model.\n    \"\"\"\n    model_dict = {}\n    data = {}\n    for key, value in row.model_dump().items():\n        if key in [\"id\", \"created_at\", \"updated_at\"]:\n            model_dict[key] = value\n            continue\n        data[key] = value\n    model_dict[\"data\"] = data\n    model_dict[\"table_info\"] = table_info\n\n    return cls.model_validate(model_dict)\n</code></pre>"},{"location":"api_reference/app/models/base_schema/#pixano.app.models.base_schema.BaseSchemaModel.from_rows","title":"<code>from_rows(rows, table_info)</code>  <code>classmethod</code>","text":"<p>Create a list of models from a list of schemas.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>list[T]</code> <p>The rows to create the models from.</p> required <code>table_info</code> <code>TableInfo</code> <p>The table info of the rows.</p> required <p>Returns:</p> Type Description <code>list[Self]</code> <p>The list of created models.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>@classmethod\ndef from_rows(cls, rows: list[T], table_info: TableInfo) -&gt; list[Self]:\n    \"\"\"Create a list of models from a list of schemas.\n\n    Args:\n        rows: The rows to create the models from.\n        table_info: The table info of the rows.\n\n    Returns:\n        The list of created models.\n    \"\"\"\n    return [cls.from_row(row, table_info) for row in rows]\n</code></pre>"},{"location":"api_reference/app/models/base_schema/#pixano.app.models.base_schema.BaseSchemaModel.model_dump","title":"<code>model_dump(exclude_timestamps=False, **kwargs)</code>","text":"<p>Dump the model to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_timestamps</code> <code>bool</code> <p>Exclude timestamps \"created_at\" and \"updated_at\" from the model dump. Useful for comparing models without timestamps.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Arguments for pydantic <code>BaseModel.model_dump()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The model dump.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def model_dump(self, exclude_timestamps: bool = False, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Dump the model to a dictionary.\n\n    Args:\n        exclude_timestamps: Exclude timestamps \"created_at\" and \"updated_at\" from the model dump. Useful for\n            comparing models without timestamps.\n        kwargs: Arguments for pydantic `BaseModel.model_dump()`.\n\n    Returns:\n        The model dump.\n    \"\"\"\n    model_dump = super().model_dump(**kwargs)\n    if exclude_timestamps:\n        model_dump.pop(\"created_at\", None)\n        model_dump.pop(\"updated_at\", None)\n    return model_dump\n</code></pre>"},{"location":"api_reference/app/models/base_schema/#pixano.app.models.base_schema.BaseSchemaModel.to_row","title":"<code>to_row(dataset)</code>","text":"<p>Create a row from the model.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset of the row.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The created row.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def to_row(self, dataset: Dataset) -&gt; T:\n    \"\"\"Create a row from the model.\n\n    Args:\n        dataset: The dataset of the row.\n\n    Returns:\n        The created row.\n    \"\"\"\n    schema_dict = self.model_dump()\n    schema = dataset.schema.schemas[self.table_info.name]\n    row = schema.model_validate(\n        {\n            \"id\": schema_dict[\"id\"],\n            \"created_at\": schema_dict[\"created_at\"],\n            \"updated_at\": schema_dict[\"updated_at\"],\n            **schema_dict[\"data\"],\n        }\n    )\n    row.dataset = dataset\n    row.table_name = self.table_info.name\n    return row\n</code></pre>"},{"location":"api_reference/app/models/base_schema/#pixano.app.models.base_schema.BaseSchemaModel.to_rows","title":"<code>to_rows(models, dataset)</code>  <code>staticmethod</code>","text":"<p>Create a list of rows from a list of models.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list[BaseSchemaModel]</code> <p>The models to create the rows from.</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset of the row.</p> required <p>Returns:</p> Type Description <code>list[T]</code> <p>The list of created rows.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>@staticmethod\ndef to_rows(models: list[\"BaseSchemaModel\"], dataset: Dataset) -&gt; list[T]:\n    \"\"\"Create a list of rows from a list of models.\n\n    Args:\n        models: The models to create the rows from.\n        dataset: The dataset of the row.\n\n    Returns:\n        The list of created rows.\n    \"\"\"\n    return [model.to_row(dataset) for model in models]\n</code></pre>"},{"location":"api_reference/app/models/dataset_info/","title":"dataset_info","text":""},{"location":"api_reference/app/models/dataset_info/#pixano.app.models.dataset_info","title":"<code>pixano.app.models.dataset_info</code>","text":""},{"location":"api_reference/app/models/dataset_info/#pixano.app.models.dataset_info.DatasetInfoModel","title":"<code>DatasetInfoModel(**data)</code>","text":"<p>               Bases: <code>DatasetInfo</code></p> <p>Dataset info model.</p> <p>It contains all the information as a DatasetInfo and the number of items in the dataset.</p> <p>Attributes:</p> Name Type Description <code>num_items</code> <code>int</code> <p>Number of items in the dataset.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/models/dataset_info/#pixano.app.models.dataset_info.DatasetInfoModel.from_dataset_info","title":"<code>from_dataset_info(dataset_info, dataset_dir)</code>  <code>staticmethod</code>","text":"<p>Create a dataset info model from a dataset info.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_info</code> <code>DatasetInfo</code> <p>Dataset info.</p> required <code>dataset_dir</code> <code>Path</code> <p>Dataset directory.</p> required <p>Returns:</p> Type Description <code>DatasetInfoModel</code> <p>Dataset info model.</p> Source code in <code>pixano/app/models/dataset_info.py</code> <pre><code>@staticmethod\ndef from_dataset_info(dataset_info: DatasetInfo, dataset_dir: Path) -&gt; \"DatasetInfoModel\":\n    \"\"\"Create a dataset info model from a dataset info.\n\n    Args:\n        dataset_info: Dataset info.\n        dataset_dir: Dataset directory.\n\n    Returns:\n        Dataset info model.\n    \"\"\"\n    db = lancedb.connect(dataset_dir / Dataset._DB_PATH)\n    item_table = db.open_table(SchemaGroup.ITEM.value)\n    num_items = item_table.count_rows()\n    return DatasetInfoModel(\n        num_items=num_items,\n        **dataset_info.model_dump(),\n    )\n</code></pre>"},{"location":"api_reference/app/models/dataset_items/","title":"dataset_items","text":""},{"location":"api_reference/app/models/dataset_items/#pixano.app.models.dataset_items","title":"<code>pixano.app.models.dataset_items</code>","text":""},{"location":"api_reference/app/models/dataset_items/#pixano.app.models.dataset_items.DatasetItemModel","title":"<code>DatasetItemModel(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DatasetItem model.</p> <p>It represents a dataset item with its associated entities, annotations and views.</p> <p>The mappings consist of the table name as key and the corresponding model or list of models as value.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The dataset item id.</p> <code>item</code> <code>ItemModel</code> <p>The item model.</p> <code>entities</code> <code>dict[str, list[EntityModel] | EntityModel | None]</code> <p>The entities models mapping.</p> <code>annotations</code> <code>dict[str, list[AnnotationModel] | AnnotationModel | None]</code> <p>The annotations models mapping.</p> <code>views</code> <code>dict[str, list[ViewModel] | ViewModel | None]</code> <p>The views models mapping.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/models/dataset_items/#pixano.app.models.dataset_items.DatasetItemModel.from_dataset_item","title":"<code>from_dataset_item(dataset_item, dataset_schema)</code>  <code>classmethod</code>","text":"<p>Create a model from a DatasetItem.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_item</code> <code>DatasetItem</code> <p>The dataset item to create the model from.</p> required <code>dataset_schema</code> <code>DatasetSchema</code> <p>The schema of the dataset containing the dataset item.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The created model.</p> Source code in <code>pixano/app/models/dataset_items.py</code> <pre><code>@classmethod\ndef from_dataset_item(cls, dataset_item: DatasetItem, dataset_schema: DatasetSchema) -&gt; Self:\n    \"\"\"Create a model from a [DatasetItem][pixano.datasets.DatasetItem].\n\n    Args:\n        dataset_item: The dataset item to create the model from.\n        dataset_schema: The schema of the dataset containing the dataset item.\n\n    Returns:\n        The created model.\n    \"\"\"\n\n    def _row_or_rows_to_model_or_models(\n        row_or_rows: BaseSchema | list[BaseSchema], name: str, group: SchemaGroup, model: type[BaseSchemaModel]\n    ) -&gt; BaseSchemaModel | list[BaseSchemaModel]:\n        base_schema = get_super_type_from_dict(\n            type(row_or_rows[0]) if isinstance(row_or_rows, list) else type(row_or_rows), _PIXANO_SCHEMA_REGISTRY\n        )\n        if base_schema is None:\n            raise ValueError(f\"Unsupported schema type {type(row_or_rows)}\")\n        table_info = TableInfo(name=name, group=group.value, base_schema=base_schema.__name__)\n        if isinstance(row_or_rows, list):\n            return model.from_rows(row_or_rows, table_info=table_info)\n        else:\n            return model.from_row(row_or_rows, table_info=table_info)\n\n    model_dict: dict[str, Any] = {\n        \"entities\": {},\n        \"annotations\": {},\n        \"views\": {},\n    }\n    for key, value in dataset_item.to_schemas_data(dataset_schema).items():\n        if value is None or value == []:\n            if issubclass(dataset_schema.schemas[key], View):\n                model_dict[\"views\"][key] = None if value is None else []\n            elif issubclass(dataset_schema.schemas[key], Entity):\n                model_dict[\"entities\"][key] = None if value is None else []\n            elif issubclass(dataset_schema.schemas[key], Annotation):\n                model_dict[\"annotations\"][key] = None if value is None else []\n            else:\n                raise ValueError(f\"Unsupported schema type {type(value)}\")\n        elif isinstance(value, Item) or isinstance(value, list) and isinstance(value[0], Item):\n            model_dict[key] = _row_or_rows_to_model_or_models(value, key, SchemaGroup.ITEM, ItemModel)\n        elif isinstance(value, Annotation) or isinstance(value, list) and isinstance(value[0], Annotation):\n            model_dict[\"annotations\"][key] = _row_or_rows_to_model_or_models(\n                value, key, SchemaGroup.ANNOTATION, AnnotationModel\n            )\n        elif isinstance(value, Entity) or isinstance(value, list) and isinstance(value[0], Entity):\n            model_dict[\"entities\"][key] = _row_or_rows_to_model_or_models(\n                value, key, SchemaGroup.ENTITY, EntityModel\n            )\n        elif isinstance(value, View) or isinstance(value, list) and isinstance(value[0], View):\n            model_dict[\"views\"][key] = _row_or_rows_to_model_or_models(value, key, SchemaGroup.VIEW, ViewModel)\n        else:\n            raise ValueError(f\"Unsupported schema type {type(value)}\")\n    model_dict[\"id\"] = dataset_item.id\n    return cls.model_validate(model_dict)\n</code></pre>"},{"location":"api_reference/app/models/dataset_items/#pixano.app.models.dataset_items.DatasetItemModel.from_dataset_items","title":"<code>from_dataset_items(dataset_items, dataset_schema)</code>  <code>classmethod</code>","text":"<p>Create a list of models from a list of DatasetItems.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_items</code> <code>list[DatasetItem]</code> <p>The dataset items to create the models from.</p> required <code>dataset_schema</code> <code>DatasetSchema</code> <p>The schema of the dataset containing the dataset item.</p> required <p>Returns:</p> Type Description <code>list[Self]</code> <p>The list of created models.</p> Source code in <code>pixano/app/models/dataset_items.py</code> <pre><code>@classmethod\ndef from_dataset_items(cls, dataset_items: list[DatasetItem], dataset_schema: DatasetSchema) -&gt; list[Self]:\n    \"\"\"Create a list of models from a list of [DatasetItem][pixano.datasets.DatasetItem]s.\n\n    Args:\n        dataset_items: The dataset items to create the models from.\n        dataset_schema: The schema of the dataset containing the dataset item.\n\n    Returns:\n        The list of created models.\n    \"\"\"\n    return [cls.from_dataset_item(dataset_item, dataset_schema) for dataset_item in dataset_items]\n</code></pre>"},{"location":"api_reference/app/models/dataset_items/#pixano.app.models.dataset_items.DatasetItemModel.model_dump","title":"<code>model_dump(exclude_timestamps=False, **kwargs)</code>","text":"<p>Dump the model to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_timestamps</code> <code>bool</code> <p>Exclude timestamps \"created_at\" and \"updated_at\" from the model dump. Useful for comparing models without timestamps.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Arguments for pydantic <code>BaseModel.model_dump()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The model dump.</p> Source code in <code>pixano/app/models/dataset_items.py</code> <pre><code>def model_dump(self, exclude_timestamps: bool = False, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Dump the model to a dictionary.\n\n    Args:\n        exclude_timestamps: Exclude timestamps \"created_at\" and \"updated_at\" from the model dump. Useful for\n            comparing models without timestamps.\n        kwargs: Arguments for pydantic `BaseModel.model_dump()`.\n\n    Returns:\n        The model dump.\n    \"\"\"\n    model_dump = super().model_dump(**kwargs)\n    if exclude_timestamps:\n        model_dump[\"item\"].pop(\"created_at\", None)\n        model_dump[\"item\"].pop(\"updated_at\", None)\n        for k in [\"entities\", \"annotations\", \"views\"]:\n            for model in model_dump[k].values():\n                if model is None:\n                    continue\n                elif isinstance(model, list):  # Only one level deep.\n                    for item in model:\n                        item.pop(\"created_at\", None)\n                        item.pop(\"updated_at\", None)\n                else:\n                    model.pop(\"created_at\", None)\n                    model.pop(\"updated_at\", None)\n    return model_dump\n</code></pre>"},{"location":"api_reference/app/models/dataset_items/#pixano.app.models.dataset_items.DatasetItemModel.to_dataset_item","title":"<code>to_dataset_item(dataset)</code>","text":"<p>Create a DatasetItem from a model.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset containing the model.</p> required <p>Returns:</p> Type Description <code>DatasetItem</code> <p>The created dataset item.</p> Source code in <code>pixano/app/models/dataset_items.py</code> <pre><code>def to_dataset_item(self, dataset: Dataset) -&gt; DatasetItem:\n    \"\"\"Create a [DatasetItem][pixano.datasets.DatasetItem] from a model.\n\n    Args:\n        dataset: The dataset containing the model.\n\n    Returns:\n        The created dataset item.\n    \"\"\"\n    schema_dict = {}\n\n    item = self.item\n    schema_dict.update(item.to_row(dataset).model_dump())\n\n    for group in [self.annotations, self.entities, self.views]:\n        for key, value in group.items():\n            if isinstance(value, list):\n                schema_dict[key] = [v.to_row(dataset) for v in value]\n            elif value is None:\n                schema_dict[key] = None\n            else:\n                schema_dict[key] = value.to_row(dataset)\n\n    return DatasetItem.from_dataset_schema(dataset.schema, exclude_embeddings=True).model_validate(schema_dict)\n</code></pre>"},{"location":"api_reference/app/models/dataset_items/#pixano.app.models.dataset_items.DatasetItemModel.to_dataset_items","title":"<code>to_dataset_items(models, dataset)</code>  <code>staticmethod</code>","text":"<p>Create a list of DatasetItems from a list of models.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list[DatasetItemModel]</code> <p>The models to create the dataset items from.</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset containing the model.</p> required <p>Returns:</p> Type Description <code>list[DatasetItem]</code> <p>The list of created dataset items.</p> Source code in <code>pixano/app/models/dataset_items.py</code> <pre><code>@staticmethod\ndef to_dataset_items(models: list[\"DatasetItemModel\"], dataset: Dataset) -&gt; list[DatasetItem]:\n    \"\"\"Create a list of [DatasetItem][pixano.datasets.DatasetItem]s from a list of models.\n\n    Args:\n        models: The models to create the dataset items from.\n        dataset: The dataset containing the model.\n\n    Returns:\n        The list of created dataset items.\n    \"\"\"\n    return [model.to_dataset_item(dataset) for model in models]\n</code></pre>"},{"location":"api_reference/app/models/datasets/","title":"datasets","text":""},{"location":"api_reference/app/models/datasets/#pixano.app.models.datasets","title":"<code>pixano.app.models.datasets</code>","text":""},{"location":"api_reference/app/models/datasets/#pixano.app.models.datasets.DatasetBrowser","title":"<code>DatasetBrowser(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data for Dataset Browser page.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>dataset id</p> <code>name</code> <code>str</code> <p>dataset name</p> <code>table_data</code> <code>TableData</code> <p>table data</p> <code>pagination</code> <code>PaginationInfo</code> <p>pagination infos</p> <code>semantic_search</code> <code>list[str]</code> <p>list of semantic search available models</p> <code>item_ids</code> <code>list[str]</code> <p>full list of item ids returned</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/models/datasets/#pixano.app.models.datasets.DatasetModel","title":"<code>DatasetModel(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The model of a dataset.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Dataset ID.</p> <code>path</code> <code>Path</code> <p>Path to the dataset.</p> <code>previews_path</code> <code>Path</code> <p>Path to the previews.</p> <code>media_dir</code> <code>Path</code> <p>Path to the media directory.</p> <code>thumbnail</code> <code>Path</code> <p>Path to the thumbnail.</p> <code>dataset_schema</code> <code>DatasetSchema</code> <p>The dataset schema.</p> <code>feature_values</code> <code>DatasetFeaturesValues</code> <p>The feature values of the dataset.</p> <code>info</code> <code>DatasetInfoModel</code> <p>The dataset info.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/models/datasets/#pixano.app.models.datasets.DatasetModel.from_dataset","title":"<code>from_dataset(dataset)</code>  <code>classmethod</code>","text":"<p>Create a dataset model from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The dataset model.</p> Source code in <code>pixano/app/models/datasets.py</code> <pre><code>@classmethod\ndef from_dataset(cls, dataset: Dataset) -&gt; Self:\n    \"\"\"Create a dataset model from a dataset.\n\n    Args:\n        dataset: The dataset.\n\n    Returns:\n        The dataset model.\n    \"\"\"\n    info = DatasetInfoModel.from_dataset_info(dataset.info, dataset.path)\n    return cls(\n        id=dataset.info.id,\n        path=dataset.path,\n        previews_path=dataset.previews_path,\n        media_dir=dataset.media_dir,\n        thumbnail=dataset.thumbnail,\n        dataset_schema=dataset.schema,\n        feature_values=dataset.features_values,\n        info=info,\n    )\n</code></pre>"},{"location":"api_reference/app/models/datasets/#pixano.app.models.datasets.DatasetModel.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Create a dataset model from a JSON object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>JSON object.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>Dataset model.</p> Source code in <code>pixano/app/models/datasets.py</code> <pre><code>@classmethod\ndef from_json(cls, data: dict[str, Any]) -&gt; Self:\n    \"\"\"Create a dataset model from a JSON object.\n\n    Args:\n        data: JSON object.\n\n    Returns:\n        Dataset model.\n    \"\"\"\n    return cls(\n        id=data[\"id\"],\n        path=Path(data[\"path\"]),\n        previews_path=Path(data[\"previews_path\"]),\n        media_dir=Path(data[\"media_dir\"]),\n        thumbnail=Path(data[\"thumbnail\"]),\n        dataset_schema=DatasetSchema.deserialize(data[\"dataset_schema\"]),\n        feature_values=DatasetFeaturesValues(**data[\"feature_values\"]),\n        info=DatasetInfoModel.from_dataset_info(DatasetInfo(**data[\"info\"]), Path(data[\"path\"])),\n    )\n</code></pre>"},{"location":"api_reference/app/models/datasets/#pixano.app.models.datasets.DatasetModel.to_dataset","title":"<code>to_dataset()</code>","text":"<p>Create a dataset from a dataset model.</p> Source code in <code>pixano/app/models/datasets.py</code> <pre><code>def to_dataset(self) -&gt; Dataset:\n    \"\"\"Create a dataset from a dataset model.\"\"\"\n    return Dataset(self.path, self.media_dir)\n</code></pre>"},{"location":"api_reference/app/models/datasets/#pixano.app.models.datasets.PaginationColumn","title":"<code>PaginationColumn(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Column description.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>column name.</p> <code>type</code> <code>str</code> <p>column type.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/models/datasets/#pixano.app.models.datasets.PaginationInfo","title":"<code>PaginationInfo(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pagination info.</p> <p>Attributes:</p> Name Type Description <code>current_page</code> <code>int</code> <p>current page.</p> <code>page_size</code> <code>int</code> <p>number of items per page.</p> <code>total_size</code> <code>int</code> <p>total number of items.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/models/datasets/#pixano.app.models.datasets.TableData","title":"<code>TableData(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Table data.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>list[PaginationColumn]</code> <p>column descriptions.</p> <code>rows</code> <code>list[dict[str, Any]]</code> <p>rows (actual data).</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/models/embeddings/","title":"embeddings","text":""},{"location":"api_reference/app/models/embeddings/#pixano.app.models.embeddings","title":"<code>pixano.app.models.embeddings</code>","text":""},{"location":"api_reference/app/models/embeddings/#pixano.app.models.embeddings.EmbeddingModel","title":"<code>EmbeddingModel(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchemaModel[Embedding]</code></p> <p>Model for the Embedding schema.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/app/models/embeddings/#pixano.app.models.embeddings.EmbeddingModel.to_row","title":"<code>to_row(dataset)</code>","text":"<p>Create an Embedding from the model.</p> Source code in <code>pixano/app/models/embeddings.py</code> <pre><code>def to_row(self, dataset: Dataset) -&gt; Embedding:\n    \"\"\"Create an [Embedding][pixano.features.Embedding] from the model.\"\"\"\n    if not is_embedding(dataset.schema.schemas[self.table_info.name]):\n        raise ValueError(f\"Schema type must be a subclass of {Embedding.__name__}.\")\n    return super().to_row(dataset)\n</code></pre>"},{"location":"api_reference/app/models/entities/","title":"entities","text":""},{"location":"api_reference/app/models/entities/#pixano.app.models.entities","title":"<code>pixano.app.models.entities</code>","text":""},{"location":"api_reference/app/models/entities/#pixano.app.models.entities.EntityModel","title":"<code>EntityModel(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchemaModel[Entity]</code></p> <p>Model for the Entity schema.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/app/models/entities/#pixano.app.models.entities.EntityModel.to_row","title":"<code>to_row(dataset)</code>","text":"<p>Create an Entity from the model.</p> Source code in <code>pixano/app/models/entities.py</code> <pre><code>def to_row(self, dataset: Dataset) -&gt; Entity:\n    \"\"\"Create an [Entity][pixano.features.Entity] from the model.\"\"\"\n    if not is_entity(dataset.schema.schemas[self.table_info.name]):\n        raise ValueError(f\"Schema type must be a subclass of {Entity.__name__}.\")\n    return super().to_row(dataset)\n</code></pre>"},{"location":"api_reference/app/models/item_info/","title":"item_info","text":""},{"location":"api_reference/app/models/item_info/#pixano.app.models.item_info","title":"<code>pixano.app.models.item_info</code>","text":""},{"location":"api_reference/app/models/item_info/#pixano.app.models.item_info.ItemInfoModel","title":"<code>ItemInfoModel(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>ItemModel</code></p> <p>Item information.</p> <p>It contains all the information contained in an ItemModel and additional information about the dataset item such as the number of annotations, embeddings, entities and views.</p> <p>Attributes:</p> Name Type Description <code>infos</code> <p>Information about the dataset item. Structure: {info_name: {sub_info_name: {\"count\": int, ...}, ...}, ...}</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/app/models/items/","title":"items","text":""},{"location":"api_reference/app/models/items/#pixano.app.models.items","title":"<code>pixano.app.models.items</code>","text":""},{"location":"api_reference/app/models/items/#pixano.app.models.items.ItemModel","title":"<code>ItemModel(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchemaModel[Item]</code></p> <p>Model for the Item schema.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/app/models/items/#pixano.app.models.items.ItemModel.to_row","title":"<code>to_row(dataset)</code>","text":"<p>Create an Item from the model.</p> Source code in <code>pixano/app/models/items.py</code> <pre><code>def to_row(self, dataset: Dataset) -&gt; Item:\n    \"\"\"Create an [Item][pixano.features.Item] from the model.\"\"\"\n    if not is_item(dataset.schema.schemas[self.table_info.name]):\n        raise ValueError(f\"Schema type must be a subclass of {Item.__name__}.\")\n    return super().to_row(dataset)\n</code></pre>"},{"location":"api_reference/app/models/sources/","title":"sources","text":""},{"location":"api_reference/app/models/sources/#pixano.app.models.sources","title":"<code>pixano.app.models.sources</code>","text":""},{"location":"api_reference/app/models/sources/#pixano.app.models.sources.SourceModel","title":"<code>SourceModel(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchemaModel[Source]</code></p> <p>Model for the Source schema.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/app/models/sources/#pixano.app.models.sources.SourceModel.from_row","title":"<code>from_row(row, table_info)</code>  <code>classmethod</code>","text":"<p>Create a SourceModel from a Source.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Source</code> <p>The row to create the model from.</p> required <code>table_info</code> <code>TableInfo</code> <p>The table info of the row.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The created model.</p> Source code in <code>pixano/app/models/sources.py</code> <pre><code>@classmethod\ndef from_row(cls, row: Source, table_info: TableInfo) -&gt; Self:\n    \"\"\"Create a SourceModel from a Source.\n\n    Args:\n        row: The row to create the model from.\n        table_info: The table info of the row.\n\n    Returns:\n        The created model.\n    \"\"\"\n    source_model = BaseSchemaModel.from_row(row, table_info)\n    source_model.data[\"metadata\"] = json.loads(source_model.data[\"metadata\"])\n    return cls.model_construct(**source_model.__dict__)  # Avoid validation and casting\n</code></pre>"},{"location":"api_reference/app/models/sources/#pixano.app.models.sources.SourceModel.to_row","title":"<code>to_row(dataset)</code>","text":"<p>Create a Source from the model.</p> Source code in <code>pixano/app/models/sources.py</code> <pre><code>def to_row(self, dataset: Dataset) -&gt; Source:\n    \"\"\"Create a [Source][pixano.features.Source] from the model.\"\"\"\n    schema_dict = self.model_dump()\n    row = Source.model_validate(\n        {\n            \"id\": schema_dict[\"id\"],\n            \"created_at\": schema_dict[\"created_at\"],\n            \"updated_at\": schema_dict[\"updated_at\"],\n            **schema_dict[\"data\"],\n        }\n    )\n    row.dataset = dataset\n    row.table_name = self.table_info.name\n    row.metadata = json.dumps(self.data[\"metadata\"])\n    return row\n</code></pre>"},{"location":"api_reference/app/models/table_info/","title":"table_info","text":""},{"location":"api_reference/app/models/table_info/#pixano.app.models.table_info","title":"<code>pixano.app.models.table_info</code>","text":""},{"location":"api_reference/app/models/table_info/#pixano.app.models.table_info.TableInfo","title":"<code>TableInfo(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The information of a table.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Table name.</p> <code>group</code> <code>str</code> <p>Group of the schema associated with the table.</p> <code>base_schema</code> <code>str</code> <p>Base Pixano schema stored in the registry.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/models/utils/","title":"utils","text":""},{"location":"api_reference/app/models/utils/#pixano.app.models.utils","title":"<code>pixano.app.models.utils</code>","text":""},{"location":"api_reference/app/models/views/","title":"views","text":""},{"location":"api_reference/app/models/views/#pixano.app.models.views","title":"<code>pixano.app.models.views</code>","text":""},{"location":"api_reference/app/models/views/#pixano.app.models.views.ViewModel","title":"<code>ViewModel(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchemaModel[View]</code></p> <p>Model for the View schema.</p> Source code in <code>pixano/app/models/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/app/models/views/#pixano.app.models.views.ViewModel.to_row","title":"<code>to_row(dataset)</code>","text":"<p>Create a View from the model.</p> Source code in <code>pixano/app/models/views.py</code> <pre><code>def to_row(self, dataset: Dataset) -&gt; View:\n    \"\"\"Create a [View][pixano.features.View] from the model.\"\"\"\n    if not is_view(dataset.schema.schemas[self.table_info.name]):\n        raise ValueError(f\"Schema type must be a subclass of {View.__name__}.\")\n    return super().to_row(dataset)\n</code></pre>"},{"location":"api_reference/app/routers/annotations/","title":"annotations","text":""},{"location":"api_reference/app/routers/annotations/#pixano.app.routers.annotations","title":"<code>pixano.app.routers.annotations</code>","text":""},{"location":"api_reference/app/routers/annotations/#pixano.app.routers.annotations.create_annotation","title":"<code>create_annotation(dataset_id, table, id, annotation, settings)</code>  <code>async</code>","text":"<p>Add an annotation in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the annotation.</p> required <code>annotation</code> <code>AnnotationModel</code> <p>Annotation to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>AnnotationModel</code> <p>The annotation added.</p> Source code in <code>pixano/app/routers/annotations.py</code> <pre><code>@router.post(\"/{dataset_id}/{table}/{id}\", response_model=AnnotationModel)\nasync def create_annotation(\n    dataset_id: str,\n    table: str,\n    id: str,\n    annotation: AnnotationModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; AnnotationModel:\n    \"\"\"Add an annotation in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the annotation.\n        annotation: Annotation to add.\n        settings: App settings.\n\n    Returns:\n        The annotation added.\n    \"\"\"\n    return await create_row_handler(dataset_id, SchemaGroup.ANNOTATION, table, id, annotation, settings)\n</code></pre>"},{"location":"api_reference/app/routers/annotations/#pixano.app.routers.annotations.create_annotations","title":"<code>create_annotations(dataset_id, table, annotations, settings)</code>  <code>async</code>","text":"<p>Add annotations in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>annotations</code> <code>list[AnnotationModel]</code> <p>Annotations to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[AnnotationModel]</code> <p>List of annotations added.</p> Source code in <code>pixano/app/routers/annotations.py</code> <pre><code>@router.post(\"/{dataset_id}/{table}\", response_model=list[AnnotationModel])\nasync def create_annotations(\n    dataset_id: str,\n    table: str,\n    annotations: list[AnnotationModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[AnnotationModel]:\n    \"\"\"Add annotations in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        annotations: Annotations to add.\n        settings: App settings.\n\n    Returns:\n        List of annotations added.\n    \"\"\"\n    return await create_rows_handler(dataset_id, SchemaGroup.ANNOTATION, table, annotations, settings)\n</code></pre>"},{"location":"api_reference/app/routers/annotations/#pixano.app.routers.annotations.delete_annotation","title":"<code>delete_annotation(dataset_id, table, id, settings)</code>  <code>async</code>","text":"<p>Delete an annotation from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the annotation to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/annotations.py</code> <pre><code>@router.delete(\"/{dataset_id}/{table}/{id}\")\nasync def delete_annotation(\n    dataset_id: str, table: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; None:\n    \"\"\"Delete an annotation from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the annotation to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_row_handler(dataset_id, SchemaGroup.ANNOTATION, table, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/annotations/#pixano.app.routers.annotations.delete_annotations","title":"<code>delete_annotations(dataset_id, table, ids, settings)</code>  <code>async</code>","text":"<p>Delete annotations from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>ids</code> <code>Annotated[list[str], Query()]</code> <p>IDs of the annotations to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/annotations.py</code> <pre><code>@router.delete(\"/{dataset_id}/{table}\")\nasync def delete_annotations(\n    dataset_id: str,\n    table: str,\n    ids: Annotated[list[str], Query()],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; None:\n    \"\"\"Delete annotations from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        ids: IDs of the annotations to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_rows_handler(dataset_id, SchemaGroup.ANNOTATION, table, ids, settings)\n</code></pre>"},{"location":"api_reference/app/routers/annotations/#pixano.app.routers.annotations.get_annotation","title":"<code>get_annotation(dataset_id, table, id, settings)</code>  <code>async</code>","text":"<p>Get an annotation from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the annotation.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>AnnotationModel</code> <p>The annotation.</p> Source code in <code>pixano/app/routers/annotations.py</code> <pre><code>@router.get(\"/{dataset_id}/{table}/{id}\", response_model=AnnotationModel)\nasync def get_annotation(\n    dataset_id: str, table: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; AnnotationModel:\n    \"\"\"Get an annotation from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the annotation.\n        settings: App settings.\n\n    Returns:\n        The annotation.\n    \"\"\"\n    return await get_row_handler(dataset_id, SchemaGroup.ANNOTATION, table, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/annotations/#pixano.app.routers.annotations.get_annotations","title":"<code>get_annotations(dataset_id, table, settings, ids=Query(None), limit=None, skip=0, where=None, item_ids=Query(None))</code>  <code>async</code>","text":"<p>Get annotations from a table of a dataset.</p> <p>They can be filtered by IDs, item IDs, a where clause or paginated.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>ids</code> <code>list[str] | None</code> <p>IDs of the annotations.</p> <code>Query(None)</code> <code>limit</code> <code>int | None</code> <p>Limit number of annotations.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of annotations.</p> <code>0</code> <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <code>item_ids</code> <code>list[str] | None</code> <p>Item IDs.</p> <code>Query(None)</code> <p>Returns:</p> Type Description <code>list[AnnotationModel]</code> <p>List of annotations.</p> Source code in <code>pixano/app/routers/annotations.py</code> <pre><code>@router.get(\"/{dataset_id}/{table}\", response_model=list[AnnotationModel])\nasync def get_annotations(\n    dataset_id: str,\n    table: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    ids: list[str] | None = Query(None),\n    limit: int | None = None,\n    skip: int = 0,\n    where: str | None = None,\n    item_ids: list[str] | None = Query(None),\n) -&gt; list[AnnotationModel]:\n    \"\"\"Get annotations from a table of a dataset.\n\n    They can be filtered by IDs, item IDs, a where clause or paginated.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        settings: App settings.\n        ids: IDs of the annotations.\n        limit: Limit number of annotations.\n        skip: Skip number of annotations.\n        where: Where clause.\n        item_ids: Item IDs.\n\n    Returns:\n        List of annotations.\n    \"\"\"\n    return await get_rows_handler(\n        dataset_id=dataset_id,\n        group=SchemaGroup.ANNOTATION,\n        table=table,\n        settings=settings,\n        where=where,\n        ids=ids,\n        item_ids=item_ids,\n        limit=limit,\n        skip=skip,\n    )\n</code></pre>"},{"location":"api_reference/app/routers/annotations/#pixano.app.routers.annotations.update_annotation","title":"<code>update_annotation(dataset_id, table, id, annotation, settings)</code>  <code>async</code>","text":"<p>Update an annotation in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the annotation.</p> required <code>annotation</code> <code>AnnotationModel</code> <p>Annotation to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>AnnotationModel</code> <p>The annotation updated.</p> Source code in <code>pixano/app/routers/annotations.py</code> <pre><code>@router.put(\"/{dataset_id}/{table}/{id}\", response_model=AnnotationModel)\nasync def update_annotation(\n    dataset_id: str,\n    table: str,\n    id: str,\n    annotation: AnnotationModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; AnnotationModel:\n    \"\"\"Update an annotation in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the annotation.\n        annotation: Annotation to update.\n        settings: App settings.\n\n    Returns:\n        The annotation updated.\n    \"\"\"\n    return await update_row_handler(dataset_id, SchemaGroup.ANNOTATION, table, id, annotation, settings)\n</code></pre>"},{"location":"api_reference/app/routers/annotations/#pixano.app.routers.annotations.update_annotations","title":"<code>update_annotations(dataset_id, table, annotations, settings)</code>  <code>async</code>","text":"<p>Update annotations in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>annotations</code> <code>list[AnnotationModel]</code> <p>Annotations to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[AnnotationModel]</code> <p>List of annotations updated.</p> Source code in <code>pixano/app/routers/annotations.py</code> <pre><code>@router.put(\"/{dataset_id}/{table}\", response_model=list[AnnotationModel])\nasync def update_annotations(\n    dataset_id: str,\n    table: str,\n    annotations: list[AnnotationModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[AnnotationModel]:\n    \"\"\"Update annotations in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        annotations: Annotations to update.\n        settings: App settings.\n\n    Returns:\n        List of annotations updated.\n    \"\"\"\n    return await update_rows_handler(dataset_id, SchemaGroup.ANNOTATION, table, annotations, settings)\n</code></pre>"},{"location":"api_reference/app/routers/browser/","title":"browser","text":""},{"location":"api_reference/app/routers/browser/#pixano.app.routers.browser","title":"<code>pixano.app.routers.browser</code>","text":""},{"location":"api_reference/app/routers/browser/#pixano.app.routers.browser.get_browser","title":"<code>get_browser(request, id, settings, limit=50, skip=0, query='', embedding_table='', where=None, sortcol=None, order=None)</code>  <code>async</code>","text":"<p>Load dataset items for the explorer page.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>the request object, used to retrieve the current app instance</p> required <code>id</code> <code>str</code> <p>Dataset ID containing the items.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>limit</code> <code>PositiveInt</code> <p>Limit number of items.</p> <code>50</code> <code>skip</code> <code>NonNegativeInt</code> <p>Skip number of items.</p> <code>0</code> <code>query</code> <code>str</code> <p>Text query for semantic search.</p> <code>''</code> <code>embedding_table</code> <code>str</code> <p>Table name for embeddings.</p> <code>''</code> <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <code>sortcol</code> <code>str | None</code> <p>column to order by</p> <code>None</code> <code>order</code> <code>str | None</code> <p>sort order (asc or desc)</p> <code>None</code> <p>Returns:</p> Type Description <code>DatasetBrowser</code> <p>Dataset explorer page.</p> Source code in <code>pixano/app/routers/browser.py</code> <pre><code>@router.get(\"/{id}\", response_model=DatasetBrowser)\nasync def get_browser(\n    request: Request,\n    id: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    limit: PositiveInt = 50,\n    skip: NonNegativeInt = 0,\n    query: str = \"\",\n    embedding_table: str = \"\",\n    where: str | None = None,\n    sortcol: str | None = None,\n    order: str | None = None,\n) -&gt; DatasetBrowser:  # type: ignore\n    \"\"\"Load dataset items for the explorer page.\n\n    Args:\n        request: the request object, used to retrieve the current app instance\n        id: Dataset ID containing the items.\n        settings: App settings.\n        limit: Limit number of items.\n        skip: Skip number of items.\n        query: Text query for semantic search.\n        embedding_table: Table name for embeddings.\n        where: Where clause.\n        sortcol: column to order by\n        order: sort order (asc or desc)\n\n    Returns:\n        Dataset explorer page.\n    \"\"\"\n    # Load dataset\n    dataset = get_dataset(id, settings.library_dir, settings.media_dir)\n\n    semantic_search = embedding_table != \"\"\n    if query != \"\" or embedding_table != \"\":\n        if query == \"\" or embedding_table == \"\":\n            raise HTTPException(\n                status_code=400,\n                detail=\"Both query and model_name should be provided for semantic search.\",\n            )\n\n    # Get page parameters\n    total = dataset.num_rows\n    original_limit = limit\n    limit = min(limit, total - skip)\n\n    # Get tables\n    table_item = SchemaGroup.ITEM.value\n    tables_view = sorted(dataset.schema.groups[SchemaGroup.VIEW])\n\n    # get data (items and views)\n    if semantic_search:\n        try:\n            item_rows, distances, list_ids = dataset.semantic_search(query, embedding_table, limit, skip)\n        except DatasetAccessError as e:\n            raise HTTPException(status_code=400, detail=str(e))\n    else:\n        if where is not None or sortcol is not None:\n            full_item_rows = get_rows(dataset=dataset, table=table_item, where=where, sortcol=sortcol, order=order)\n            item_rows = full_item_rows[skip : skip + limit]\n            list_ids = [item.id for item in full_item_rows]\n        else:\n            item_rows = get_rows(dataset=dataset, table=table_item, limit=limit, skip=skip)\n            list_ids = dataset.get_all_ids()\n\n    item_ids = [item.id for item in item_rows]\n    item_first_media: dict[str, dict] = {}\n    for view in tables_view:\n        try:\n            view_rows = get_rows(dataset=dataset, table=view, item_ids=item_ids)\n        except HTTPException:\n            view_rows = []\n        item_first_media[view] = {}\n        for item_id in item_ids:\n            # store image or first frame\n            item_first_media[view][item_id] = next(filter(lambda v: v.item_ref.id == item_id, view_rows), None)\n\n    # build column headers (PaginationColumn)\n    cols = []\n    for view in tables_view:\n        view_type = \"image\"\n        # NOTE: right now for video we use a frame\n        # (first returned by get_rows for each item. May not be the real first frame)\n        # when we'll have thumbnail clip, use instead:\n        # view_type = \"video\" if isinstance(item_first_media[view][item_ids[0]], SequenceFrame) else \"image\"\n        cols.append(PaginationColumn(name=view, type=view_type))\n    for feat in vars(item_rows[0]).keys():\n        cols.append(PaginationColumn(name=feat, type=type(getattr(item_rows[0], feat)).__name__))\n    if semantic_search:\n        cols.append(PaginationColumn(name=\"distance\", type=\"float\"))\n\n    # build rows\n    rows: list[dict[str, Any]] = []\n    for i, item in enumerate(item_rows):\n        row: dict[str, Any] = {}\n        # VIEWS -&gt; thumbnails previews\n        for view in tables_view:\n            curr_view = item_first_media[view][item.id]\n            if curr_view is not None:\n                if is_image(type(curr_view)):\n                    try:\n                        # Try to open image. If image is found, give an url to access via the get_thumbnail endpoint.\n                        curr_view.open(settings.media_dir, output_type=\"image\")\n                        encoded_url = base64.b64encode(curr_view.url.encode(\"utf-8\")).decode(\"utf-8\")\n                        row_view_url = str(request.url_for(\"get_thumbnail\", b64_image_path=encoded_url))\n                    except ValueError:\n                        # If image not accesible, set view_url to empty string.\n                        row_view_url = \"\"\n                elif is_text(type(curr_view)):\n                    row_view_url = generate_text_image_base64(curr_view.content[:80])\n                row[view] = row_view_url\n\n        # ITEM features\n        for feat in vars(item).keys():\n            row[feat] = getattr(item, feat)\n        # DISTANCE\n        if semantic_search:\n            row[\"distance\"] = distances[i]\n\n        rows.append(row)\n\n    semantic_search_list = sorted(\n        [\n            table_name\n            for table_name in dataset.schema.groups[SchemaGroup.EMBEDDING]\n            if is_view_embedding(dataset.schema.schemas[table_name])\n        ]\n    )\n    # Return dataset items\n    return DatasetBrowser(\n        id=id,\n        name=dataset.info.name,\n        table_data=TableData(columns=cols, rows=rows),\n        pagination=PaginationInfo(current_page=skip, page_size=original_limit, total_size=total),\n        semantic_search=semantic_search_list,\n        item_ids=list_ids,\n    )\n</code></pre>"},{"location":"api_reference/app/routers/browser/#pixano.app.routers.browser.get_items_ids","title":"<code>get_items_ids(id, settings)</code>  <code>async</code>","text":"<p>Get all item ids.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Dataset ID.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of dataset items ids.</p> Source code in <code>pixano/app/routers/browser.py</code> <pre><code>@router.get(\"/item_ids/{id}\", response_model=list[str])\nasync def get_items_ids(\n    id: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[str]:\n    \"\"\"Get all item ids.\n\n    Args:\n        id: Dataset ID.\n        settings: App settings.\n\n    Returns:\n        List of dataset items ids.\n    \"\"\"\n    dataset = get_dataset(id, settings.library_dir, None)\n    assert_table_in_group(dataset, SchemaGroup.ITEM.value, SchemaGroup.ITEM)\n    return dataset.get_all_ids()\n</code></pre>"},{"location":"api_reference/app/routers/dataset_items/","title":"dataset_items","text":""},{"location":"api_reference/app/routers/dataset_items/#pixano.app.routers.dataset_items","title":"<code>pixano.app.routers.dataset_items</code>","text":""},{"location":"api_reference/app/routers/dataset_items/#pixano.app.routers.dataset_items.create_dataset_item","title":"<code>create_dataset_item(dataset_id, id, dataset_item, settings)</code>  <code>async</code>","text":"<p>Add a dataset item.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID to add the item.</p> required <code>id</code> <code>str</code> <p>Dataset item ID to add.</p> required <code>dataset_item</code> <code>DatasetItemModel</code> <p>Dataset item.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>DatasetItemModel</code> <p>The dataset item added.</p> Source code in <code>pixano/app/routers/dataset_items.py</code> <pre><code>@router.post(\"/{dataset_id}/{id}\", response_model=DatasetItemModel)\nasync def create_dataset_item(\n    dataset_id: str,\n    id: str,\n    dataset_item: DatasetItemModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; DatasetItemModel:\n    \"\"\"Add a dataset item.\n\n    Args:\n        dataset_id: Dataset ID to add the item.\n        id: Dataset item ID to add.\n        dataset_item: Dataset item.\n        settings: App settings.\n\n    Returns:\n        The dataset item added.\n    \"\"\"\n    if id != dataset_item.id:\n        raise HTTPException(status_code=400, detail=\"ID in path and body do not match.\")\n\n    return (await create_dataset_items(dataset_id, [dataset_item], settings))[0]\n</code></pre>"},{"location":"api_reference/app/routers/dataset_items/#pixano.app.routers.dataset_items.create_dataset_items","title":"<code>create_dataset_items(dataset_id, dataset_items, settings)</code>  <code>async</code>","text":"<p>Add dataset items in a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID to add the items.</p> required <code>dataset_items</code> <code>list[DatasetItemModel]</code> <p>Dataset items to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[DatasetItemModel]</code> <p>List of dataset items added.</p> Source code in <code>pixano/app/routers/dataset_items.py</code> <pre><code>@router.post(\"/{dataset_id}\", response_model=list[DatasetItemModel])\nasync def create_dataset_items(\n    dataset_id: str,\n    dataset_items: list[DatasetItemModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[DatasetItemModel]:\n    \"\"\"Add dataset items in a dataset.\n\n    Args:\n        dataset_id: Dataset ID to add the items.\n        dataset_items: Dataset items to add.\n        settings: App settings.\n\n    Returns:\n        List of dataset items added.\n    \"\"\"\n    dataset = get_dataset(dataset_id, settings.library_dir, None)\n\n    try:\n        rows = dataset.add_dataset_items(DatasetItemModel.to_dataset_items(dataset_items, dataset))\n    except ValueError:\n        raise HTTPException(status_code=400, detail=\"Invalid dataset items\")\n    return DatasetItemModel.from_dataset_items(rows, dataset.schema)\n</code></pre>"},{"location":"api_reference/app/routers/dataset_items/#pixano.app.routers.dataset_items.delete_dataset_item","title":"<code>delete_dataset_item(dataset_id, id, settings)</code>  <code>async</code>","text":"<p>Delete a dataset item from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the item.</p> required <code>id</code> <code>str</code> <p>ID of the dataset item to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/dataset_items.py</code> <pre><code>@router.delete(\"/{dataset_id}/{id}\")\nasync def delete_dataset_item(dataset_id: str, id: str, settings: Annotated[Settings, Depends(get_settings)]) -&gt; None:\n    \"\"\"Delete a dataset item from a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the item.\n        id: ID of the dataset item to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_dataset_items(dataset_id, ids=[id], settings=settings)\n</code></pre>"},{"location":"api_reference/app/routers/dataset_items/#pixano.app.routers.dataset_items.delete_dataset_items","title":"<code>delete_dataset_items(dataset_id, ids, settings)</code>  <code>async</code>","text":"<p>Delete dataset items from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the items.</p> required <code>ids</code> <code>Annotated[list[str], Query()]</code> <p>IDs of the dataset items to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/dataset_items.py</code> <pre><code>@router.delete(\"/{dataset_id}\")\nasync def delete_dataset_items(\n    dataset_id: str,\n    ids: Annotated[list[str], Query()],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; None:\n    \"\"\"Delete dataset items from a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the items.\n        ids: IDs of the dataset items to delete.\n        settings: App settings.\n    \"\"\"\n    dataset = get_dataset(dataset_id, settings.library_dir, None)\n\n    dataset.delete_dataset_items(ids)\n    return\n</code></pre>"},{"location":"api_reference/app/routers/dataset_items/#pixano.app.routers.dataset_items.get_dataset_item","title":"<code>get_dataset_item(dataset_id, id, settings)</code>  <code>async</code>","text":"<p>Get dataset item from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID.</p> required <code>id</code> <code>str</code> <p>Dataset item ID.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>DatasetItemModel</code> <p>The dataset item.</p> Source code in <code>pixano/app/routers/dataset_items.py</code> <pre><code>@router.get(\"/{dataset_id}/{id}\", response_model=DatasetItemModel)\nasync def get_dataset_item(\n    dataset_id: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; DatasetItemModel:\n    \"\"\"Get dataset item from a dataset.\n\n    Args:\n        dataset_id: Dataset ID.\n        id: Dataset item ID.\n        settings: App settings.\n\n    Returns:\n        The dataset item.\n    \"\"\"\n    return (await get_dataset_items(dataset_id, settings, ids=[id]))[0]\n</code></pre>"},{"location":"api_reference/app/routers/dataset_items/#pixano.app.routers.dataset_items.get_dataset_items","title":"<code>get_dataset_items(dataset_id, settings, ids=Query(None), limit=None, skip=0)</code>  <code>async</code>","text":"<p>Get dataset items from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the items.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>ids</code> <code>list[str] | None</code> <p>IDs of the dataset items.</p> <code>Query(None)</code> <code>limit</code> <code>int | None</code> <p>Limit number of dataset items.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of dataset items.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[DatasetItemModel]</code> <p>List of dataset items.</p> Source code in <code>pixano/app/routers/dataset_items.py</code> <pre><code>@router.get(\"/{dataset_id}\", response_model=list[DatasetItemModel])\nasync def get_dataset_items(\n    dataset_id: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    ids: list[str] | None = Query(None),\n    limit: int | None = None,\n    skip: int = 0,\n) -&gt; list[DatasetItemModel]:\n    \"\"\"Get dataset items from a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the items.\n        settings: App settings.\n        ids: IDs of the dataset items.\n        limit: Limit number of dataset items.\n        skip: Skip number of dataset items.\n\n    Returns:\n        List of dataset items.\n    \"\"\"\n    dataset = get_dataset(dataset_id, settings.library_dir, None)\n\n    try:\n        rows = dataset.get_dataset_items(ids, limit, skip)\n    except DatasetPaginationError as err:\n        raise HTTPException(status_code=400, detail=\"Invalid query parameters. \" + str(err))\n    except DatasetAccessError as err:\n        raise HTTPException(status_code=500, detail=\"Insternal server error. \" + str(err))\n    if rows == []:\n        raise HTTPException(status_code=404, detail=\"Dataset items not found.\")\n\n    return DatasetItemModel.from_dataset_items(rows, dataset.schema)\n</code></pre>"},{"location":"api_reference/app/routers/dataset_items/#pixano.app.routers.dataset_items.update_dataset_item","title":"<code>update_dataset_item(dataset_id, id, dataset_item, settings)</code>  <code>async</code>","text":"<p>Update dataset item in a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the item.</p> required <code>id</code> <code>str</code> <p>Dataset item ID to update.</p> required <code>dataset_item</code> <code>DatasetItemModel</code> <p>Dataset item.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>DatasetItemModel</code> <p>The dataset item updated.</p> Source code in <code>pixano/app/routers/dataset_items.py</code> <pre><code>@router.put(\"/{dataset_id}/{id}\", response_model=DatasetItemModel)\nasync def update_dataset_item(\n    dataset_id: str,\n    id: str,\n    dataset_item: DatasetItemModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; DatasetItemModel:\n    \"\"\"Update dataset item in a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the item.\n        id: Dataset item ID to update.\n        dataset_item: Dataset item.\n        settings: App settings.\n\n    Returns:\n        The dataset item updated.\n    \"\"\"\n    if id != dataset_item.id:\n        raise HTTPException(status_code=400, detail=\"ID in path and body do not match.\")\n\n    return (await update_dataset_items(dataset_id, [dataset_item], settings))[0]\n</code></pre>"},{"location":"api_reference/app/routers/dataset_items/#pixano.app.routers.dataset_items.update_dataset_items","title":"<code>update_dataset_items(dataset_id, dataset_items, settings)</code>  <code>async</code>","text":"<p>Update dataset items in a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the items.</p> required <code>dataset_items</code> <code>list[DatasetItemModel]</code> <p>Dataset items to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[DatasetItemModel]</code> <p>List of dataset items updated.</p> Source code in <code>pixano/app/routers/dataset_items.py</code> <pre><code>@router.put(\"/{dataset_id}\", response_model=list[DatasetItemModel])\nasync def update_dataset_items(\n    dataset_id: str,\n    dataset_items: list[DatasetItemModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[DatasetItemModel]:\n    \"\"\"Update dataset items in a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the items.\n        dataset_items: Dataset items to update.\n        settings: App settings.\n\n    Returns:\n        List of dataset items updated.\n    \"\"\"\n    dataset = get_dataset(dataset_id, settings.library_dir, None)\n\n    try:\n        rows = dataset.update_dataset_items(DatasetItemModel.to_dataset_items(dataset_items, dataset))\n    except ValueError:\n        raise HTTPException(status_code=400, detail=\"Invalid dataset items\")\n    return DatasetItemModel.from_dataset_items(rows, dataset.schema)\n</code></pre>"},{"location":"api_reference/app/routers/datasets/","title":"datasets","text":""},{"location":"api_reference/app/routers/datasets/#pixano.app.routers.datasets","title":"<code>pixano.app.routers.datasets</code>","text":""},{"location":"api_reference/app/routers/datasets/#pixano.app.routers.datasets.get_dataset","title":"<code>get_dataset(id, settings)</code>  <code>async</code>","text":"<p>Load dataset from ID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Dataset ID to load.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>DatasetModel</code> <p>Dataset model.</p> Source code in <code>pixano/app/routers/datasets.py</code> <pre><code>@router.get(\"/{id}\", response_model=DatasetModel)\nasync def get_dataset(\n    id: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; DatasetModel:\n    \"\"\"Load dataset from ID.\n\n    Args:\n        id: Dataset ID to load.\n        settings: App settings.\n\n    Returns:\n        Dataset model.\n    \"\"\"\n    return DatasetModel.from_dataset(get_dataset_utils(id, settings.library_dir, settings.media_dir))\n</code></pre>"},{"location":"api_reference/app/routers/datasets/#pixano.app.routers.datasets.get_dataset_info","title":"<code>get_dataset_info(id, settings)</code>  <code>async</code>","text":"<p>Load a single dataset information.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Dataset ID to load info from.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>DatasetInfoModel</code> <p>The dataset info.</p> Source code in <code>pixano/app/routers/datasets.py</code> <pre><code>@router.get(\"/info/{id}\", response_model=DatasetInfoModel)\nasync def get_dataset_info(\n    id: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; DatasetInfoModel:\n    \"\"\"Load a single dataset information.\n\n    Args:\n        id: Dataset ID to load info from.\n        settings: App settings.\n\n    Returns:\n        The dataset info.\n    \"\"\"\n    try:\n        info, path = DatasetInfo.load_id(id, settings.library_dir, return_path=True)\n    except FileNotFoundError:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Dataset {id} not found in {settings.library_dir.absolute()}.\",\n        )\n\n    return DatasetInfoModel.from_dataset_info(info, path)\n</code></pre>"},{"location":"api_reference/app/routers/datasets/#pixano.app.routers.datasets.get_datasets_info","title":"<code>get_datasets_info(settings)</code>  <code>async</code>","text":"<p>Load a list of dataset information.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[DatasetInfoModel]</code> <p>List of dataset info.</p> Source code in <code>pixano/app/routers/datasets.py</code> <pre><code>@router.get(\"/info\", response_model=list[DatasetInfoModel])\nasync def get_datasets_info(\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[DatasetInfoModel]:\n    \"\"\"Load a list of dataset information.\n\n    Args:\n        settings: App settings.\n\n    Returns:\n        List of dataset info.\n    \"\"\"\n    try:\n        infos_and_paths: list[tuple[DatasetInfo, Path]] = DatasetInfo.load_directory(\n            directory=settings.library_dir, return_path=True\n        )\n    except FileNotFoundError:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"No datasets found in {settings.library_dir.absolute()}.\",\n        )\n\n    if len(infos_and_paths) &gt; 0:\n        return [DatasetInfoModel.from_dataset_info(info, path) for info, path in infos_and_paths]\n    raise HTTPException(\n        status_code=404,\n        detail=f\"No datasets found in {settings.library_dir.absolute()}.\",\n    )\n</code></pre>"},{"location":"api_reference/app/routers/datasets/#pixano.app.routers.datasets.get_table_count","title":"<code>get_table_count(id, table, settings)</code>  <code>async</code>","text":"<p>Get the number of rows in a table.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of rows in the table.</p> Source code in <code>pixano/app/routers/datasets.py</code> <pre><code>@router.get(\"/{id}/{table}/count\", response_model=int)\nasync def get_table_count(\n    id: str,\n    table: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; int:\n    \"\"\"Get the number of rows in a table.\n\n    Args:\n        id: Dataset ID containing the table.\n        table: Table name.\n        settings: App settings.\n\n    Returns:\n        The number of rows in the table.\n    \"\"\"\n    dataset = get_dataset_utils(id, settings.library_dir, settings.media_dir)\n    try:\n        db_table = dataset.open_table(table)\n    except DatasetAccessError as e:\n        raise HTTPException(\n            status_code=404,\n            detail=str(e),\n        )\n    return db_table.count_rows()\n</code></pre>"},{"location":"api_reference/app/routers/embeddings/","title":"embeddings","text":""},{"location":"api_reference/app/routers/embeddings/#pixano.app.routers.embeddings","title":"<code>pixano.app.routers.embeddings</code>","text":""},{"location":"api_reference/app/routers/embeddings/#pixano.app.routers.embeddings.create_embedding","title":"<code>create_embedding(dataset_id, table, id, embedding, settings)</code>  <code>async</code>","text":"<p>Update an embedding in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the embedding.</p> required <code>embedding</code> <code>EmbeddingModel</code> <p>Embedding to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>EmbeddingModel</code> <p>The embedding updated.</p> Source code in <code>pixano/app/routers/embeddings.py</code> <pre><code>@router.post(\"/{dataset_id}/{table}/{id}\", response_model=EmbeddingModel)\nasync def create_embedding(\n    dataset_id: str,\n    table: str,\n    id: str,\n    embedding: EmbeddingModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; EmbeddingModel:\n    \"\"\"Update an embedding in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the embedding.\n        embedding: Embedding to update.\n        settings: App settings.\n\n    Returns:\n        The embedding updated.\n    \"\"\"\n    return await create_row_handler(dataset_id, SchemaGroup.EMBEDDING, table, id, embedding, settings)\n</code></pre>"},{"location":"api_reference/app/routers/embeddings/#pixano.app.routers.embeddings.create_embeddings","title":"<code>create_embeddings(dataset_id, table, embeddings, settings)</code>  <code>async</code>","text":"<p>Add embeddings in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>embeddings</code> <code>list[EmbeddingModel]</code> <p>Embeddings to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[EmbeddingModel]</code> <p>List of embeddings added.</p> Source code in <code>pixano/app/routers/embeddings.py</code> <pre><code>@router.post(\"/{dataset_id}/{table}\", response_model=list[EmbeddingModel])\nasync def create_embeddings(\n    dataset_id: str,\n    table: str,\n    embeddings: list[EmbeddingModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[EmbeddingModel]:\n    \"\"\"Add embeddings in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        embeddings: Embeddings to add.\n        settings: App settings.\n\n    Returns:\n        List of embeddings added.\n    \"\"\"\n    return await create_rows_handler(dataset_id, SchemaGroup.EMBEDDING, table, embeddings, settings)\n</code></pre>"},{"location":"api_reference/app/routers/embeddings/#pixano.app.routers.embeddings.delete_embedding","title":"<code>delete_embedding(dataset_id, table, id, settings)</code>  <code>async</code>","text":"<p>Delete an embedding from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the embedding to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/embeddings.py</code> <pre><code>@router.delete(\"/{dataset_id}/{table}/{id}\")\nasync def delete_embedding(\n    dataset_id: str, table: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; None:\n    \"\"\"Delete an embedding from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the embedding to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_row_handler(dataset_id, SchemaGroup.EMBEDDING, table, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/embeddings/#pixano.app.routers.embeddings.delete_embeddings","title":"<code>delete_embeddings(dataset_id, table, ids, settings)</code>  <code>async</code>","text":"<p>Delete embeddings from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>ids</code> <code>Annotated[list[str], Query()]</code> <p>IDs of the embeddings to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/embeddings.py</code> <pre><code>@router.delete(\"/{dataset_id}/{table}\")\nasync def delete_embeddings(\n    dataset_id: str,\n    table: str,\n    ids: Annotated[list[str], Query()],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; None:\n    \"\"\"Delete embeddings from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        ids: IDs of the embeddings to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_rows_handler(dataset_id, SchemaGroup.EMBEDDING, table, ids, settings)\n</code></pre>"},{"location":"api_reference/app/routers/embeddings/#pixano.app.routers.embeddings.get_embedding","title":"<code>get_embedding(dataset_id, table, id, settings)</code>  <code>async</code>","text":"<p>Get an embedding from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the embedding.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>EmbeddingModel</code> <p>The embedding.</p> Source code in <code>pixano/app/routers/embeddings.py</code> <pre><code>@router.get(\"/{dataset_id}/{table}/{id}\", response_model=EmbeddingModel)\nasync def get_embedding(\n    dataset_id: str, table: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; EmbeddingModel:\n    \"\"\"Get an embedding from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the embedding.\n        settings: App settings.\n\n    Returns:\n        The embedding.\n    \"\"\"\n    return await get_row_handler(dataset_id, SchemaGroup.EMBEDDING, table, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/embeddings/#pixano.app.routers.embeddings.get_embeddings","title":"<code>get_embeddings(dataset_id, table, settings, ids=Query(None), limit=None, skip=0, where=None, item_ids=Query(None))</code>  <code>async</code>","text":"<p>Get embeddings from a table of a dataset.</p> <p>They can be filtered by IDs, item IDs, a where clause or paginated.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>ids</code> <code>list[str] | None</code> <p>IDs of the embeddings.</p> <code>Query(None)</code> <code>limit</code> <code>int | None</code> <p>Limit number of embeddings.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of embeddings.</p> <code>0</code> <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <code>item_ids</code> <code>list[str] | None</code> <p>Item IDs.</p> <code>Query(None)</code> <p>Returns:</p> Type Description <code>list[EmbeddingModel]</code> <p>List of embeddings.</p> Source code in <code>pixano/app/routers/embeddings.py</code> <pre><code>@router.get(\"/{dataset_id}/{table}\", response_model=list[EmbeddingModel])\nasync def get_embeddings(\n    dataset_id: str,\n    table: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    ids: list[str] | None = Query(None),\n    limit: int | None = None,\n    skip: int = 0,\n    where: str | None = None,\n    item_ids: list[str] | None = Query(None),\n) -&gt; list[EmbeddingModel]:\n    \"\"\"Get embeddings from a table of a dataset.\n\n    They can be filtered by IDs, item IDs, a where clause or paginated.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        settings: App settings.\n        ids: IDs of the embeddings.\n        limit: Limit number of embeddings.\n        skip: Skip number of embeddings.\n        where: Where clause.\n        item_ids: Item IDs.\n\n    Returns:\n        List of embeddings.\n    \"\"\"\n    return await get_rows_handler(\n        dataset_id=dataset_id,\n        group=SchemaGroup.EMBEDDING,\n        table=table,\n        settings=settings,\n        where=where,\n        ids=ids,\n        item_ids=item_ids,\n        limit=limit,\n        skip=skip,\n    )\n</code></pre>"},{"location":"api_reference/app/routers/embeddings/#pixano.app.routers.embeddings.update_embedding","title":"<code>update_embedding(dataset_id, table, id, embedding, settings)</code>  <code>async</code>","text":"<p>Update an embedding in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the embedding.</p> required <code>embedding</code> <code>EmbeddingModel</code> <p>Embedding to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>EmbeddingModel</code> <p>The embedding updated.</p> Source code in <code>pixano/app/routers/embeddings.py</code> <pre><code>@router.put(\"/{dataset_id}/{table}/{id}\", response_model=EmbeddingModel)\nasync def update_embedding(\n    dataset_id: str,\n    table: str,\n    id: str,\n    embedding: EmbeddingModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; EmbeddingModel:\n    \"\"\"Update an embedding in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the embedding.\n        embedding: Embedding to update.\n        settings: App settings.\n\n    Returns:\n        The embedding updated.\n    \"\"\"\n    return await update_row_handler(dataset_id, SchemaGroup.EMBEDDING, table, id, embedding, settings)\n</code></pre>"},{"location":"api_reference/app/routers/embeddings/#pixano.app.routers.embeddings.update_embeddings","title":"<code>update_embeddings(dataset_id, table, embeddings, settings)</code>  <code>async</code>","text":"<p>Update embeddings in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>embeddings</code> <code>list[EmbeddingModel]</code> <p>Embeddings to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[EmbeddingModel]</code> <p>List of embeddings updated.</p> Source code in <code>pixano/app/routers/embeddings.py</code> <pre><code>@router.put(\"/{dataset_id}/{table}\", response_model=list[EmbeddingModel])\nasync def update_embeddings(\n    dataset_id: str,\n    table: str,\n    embeddings: list[EmbeddingModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[EmbeddingModel]:\n    \"\"\"Update embeddings in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        embeddings: Embeddings to update.\n        settings: App settings.\n\n    Returns:\n        List of embeddings updated.\n    \"\"\"\n    return await update_rows_handler(dataset_id, SchemaGroup.EMBEDDING, table, embeddings, settings)\n</code></pre>"},{"location":"api_reference/app/routers/entities/","title":"entities","text":""},{"location":"api_reference/app/routers/entities/#pixano.app.routers.entities","title":"<code>pixano.app.routers.entities</code>","text":""},{"location":"api_reference/app/routers/entities/#pixano.app.routers.entities.create_entities","title":"<code>create_entities(dataset_id, table, entities, settings)</code>  <code>async</code>","text":"<p>Add entities in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>entities</code> <code>list[EntityModel]</code> <p>Entities to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[EntityModel]</code> <p>List of entities added.</p> Source code in <code>pixano/app/routers/entities.py</code> <pre><code>@router.post(\"/{dataset_id}/{table}\", response_model=list[EntityModel])\nasync def create_entities(\n    dataset_id: str,\n    table: str,\n    entities: list[EntityModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[EntityModel]:\n    \"\"\"Add entities in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        entities: Entities to add.\n        settings: App settings.\n\n    Returns:\n        List of entities added.\n    \"\"\"\n    return await create_rows_handler(dataset_id, SchemaGroup.ENTITY, table, entities, settings)\n</code></pre>"},{"location":"api_reference/app/routers/entities/#pixano.app.routers.entities.create_entity","title":"<code>create_entity(dataset_id, table, id, entity, settings)</code>  <code>async</code>","text":"<p>Add an entity in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the entity.</p> required <code>entity</code> <code>EntityModel</code> <p>Entity to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>EntityModel</code> <p>The entity added.</p> Source code in <code>pixano/app/routers/entities.py</code> <pre><code>@router.post(\"/{dataset_id}/{table}/{id}\", response_model=EntityModel)\nasync def create_entity(\n    dataset_id: str,\n    table: str,\n    id: str,\n    entity: EntityModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; EntityModel:\n    \"\"\"Add an entity in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the entity.\n        entity: Entity to add.\n        settings: App settings.\n\n    Returns:\n        The entity added.\n    \"\"\"\n    return await create_row_handler(dataset_id, SchemaGroup.ENTITY, table, id, entity, settings)\n</code></pre>"},{"location":"api_reference/app/routers/entities/#pixano.app.routers.entities.delete_entities","title":"<code>delete_entities(dataset_id, table, ids, settings)</code>  <code>async</code>","text":"<p>Delete entities from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>ids</code> <code>Annotated[list[str], Query()]</code> <p>IDs of the entities to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/entities.py</code> <pre><code>@router.delete(\"/{dataset_id}/{table}\")\nasync def delete_entities(\n    dataset_id: str,\n    table: str,\n    ids: Annotated[list[str], Query()],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; None:\n    \"\"\"Delete entities from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        ids: IDs of the entities to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_rows_handler(dataset_id, SchemaGroup.ENTITY, table, ids, settings)\n</code></pre>"},{"location":"api_reference/app/routers/entities/#pixano.app.routers.entities.delete_entity","title":"<code>delete_entity(dataset_id, table, id, settings)</code>  <code>async</code>","text":"<p>Delete an entity from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the entity to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/entities.py</code> <pre><code>@router.delete(\"/{dataset_id}/{table}/{id}\")\nasync def delete_entity(\n    dataset_id: str, table: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; None:\n    \"\"\"Delete an entity from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the entity to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_row_handler(dataset_id, SchemaGroup.ENTITY, table, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/entities/#pixano.app.routers.entities.get_entities","title":"<code>get_entities(dataset_id, table, settings, ids=Query(None), limit=None, skip=0, where=None, item_ids=Query(None))</code>  <code>async</code>","text":"<p>Get entities from a table of a dataset.</p> <p>They can be filtered by IDs, item IDs, a where clause or paginated.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>ids</code> <code>list[str] | None</code> <p>IDs of the views.</p> <code>Query(None)</code> <code>limit</code> <code>int | None</code> <p>Limit number of views.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of views.</p> <code>0</code> <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <code>item_ids</code> <code>list[str] | None</code> <p>Item IDs.</p> <code>Query(None)</code> <p>Returns:</p> Type Description <code>list[EntityModel]</code> <p>List of views.</p> Source code in <code>pixano/app/routers/entities.py</code> <pre><code>@router.get(\"/{dataset_id}/{table}\", response_model=list[EntityModel])\nasync def get_entities(\n    dataset_id: str,\n    table: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    ids: list[str] | None = Query(None),\n    limit: int | None = None,\n    skip: int = 0,\n    where: str | None = None,\n    item_ids: list[str] | None = Query(None),\n) -&gt; list[EntityModel]:\n    \"\"\"Get entities from a table of a dataset.\n\n    They can be filtered by IDs, item IDs, a where clause or paginated.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        settings: App settings.\n        ids: IDs of the views.\n        limit: Limit number of views.\n        skip: Skip number of views.\n        where: Where clause.\n        item_ids: Item IDs.\n\n    Returns:\n        List of views.\n    \"\"\"\n    return await get_rows_handler(\n        dataset_id=dataset_id,\n        group=SchemaGroup.ENTITY,\n        table=table,\n        settings=settings,\n        where=where,\n        ids=ids,\n        item_ids=item_ids,\n        limit=limit,\n        skip=skip,\n    )\n</code></pre>"},{"location":"api_reference/app/routers/entities/#pixano.app.routers.entities.get_entity","title":"<code>get_entity(dataset_id, table, id, settings)</code>  <code>async</code>","text":"<p>Get an entity from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the entity.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>EntityModel</code> <p>The entity.</p> Source code in <code>pixano/app/routers/entities.py</code> <pre><code>@router.get(\"/{dataset_id}/{table}/{id}\", response_model=EntityModel)\nasync def get_entity(\n    dataset_id: str, table: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; EntityModel:\n    \"\"\"Get an entity from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the entity.\n        settings: App settings.\n\n    Returns:\n        The entity.\n    \"\"\"\n    return await get_row_handler(dataset_id, SchemaGroup.ENTITY, table, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/entities/#pixano.app.routers.entities.update_entities","title":"<code>update_entities(dataset_id, table, entities, settings)</code>  <code>async</code>","text":"<p>Update entities in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>entities</code> <code>list[EntityModel]</code> <p>Entities to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[EntityModel]</code> <p>List of entities updated.</p> Source code in <code>pixano/app/routers/entities.py</code> <pre><code>@router.put(\"/{dataset_id}/{table}\", response_model=list[EntityModel])\nasync def update_entities(\n    dataset_id: str,\n    table: str,\n    entities: list[EntityModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[EntityModel]:\n    \"\"\"Update entities in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        entities: Entities to update.\n        settings: App settings.\n\n    Returns:\n        List of entities updated.\n    \"\"\"\n    return await update_rows_handler(dataset_id, SchemaGroup.ENTITY, table, entities, settings)\n</code></pre>"},{"location":"api_reference/app/routers/entities/#pixano.app.routers.entities.update_entity","title":"<code>update_entity(dataset_id, table, id, entity, settings)</code>  <code>async</code>","text":"<p>Update an entity in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the entity.</p> required <code>entity</code> <code>EntityModel</code> <p>Entity to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>EntityModel</code> <p>The entity updated.</p> Source code in <code>pixano/app/routers/entities.py</code> <pre><code>@router.put(\"/{dataset_id}/{table}/{id}\", response_model=EntityModel)\nasync def update_entity(\n    dataset_id: str,\n    table: str,\n    id: str,\n    entity: EntityModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; EntityModel:\n    \"\"\"Update an entity in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the entity.\n        entity: Entity to update.\n        settings: App settings.\n\n    Returns:\n        The entity updated.\n    \"\"\"\n    return await update_row_handler(dataset_id, SchemaGroup.ENTITY, table, id, entity, settings)\n</code></pre>"},{"location":"api_reference/app/routers/items/","title":"items","text":""},{"location":"api_reference/app/routers/items/#pixano.app.routers.items","title":"<code>pixano.app.routers.items</code>","text":""},{"location":"api_reference/app/routers/items/#pixano.app.routers.items.create_item","title":"<code>create_item(dataset_id, id, item, settings)</code>  <code>async</code>","text":"<p>Add an item in the <code>'item'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>id</code> <code>str</code> <p>ID of the item.</p> required <code>item</code> <code>ItemModel</code> <p>Item to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>ItemModel</code> <p>The item added.</p> Source code in <code>pixano/app/routers/items.py</code> <pre><code>@router.post(\"/{dataset_id}/{id}\", response_model=ItemModel)\nasync def create_item(\n    dataset_id: str,\n    id: str,\n    item: ItemModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; ItemModel:\n    \"\"\"Add an item in the `'item'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        id: ID of the item.\n        item: Item to add.\n        settings: App settings.\n\n    Returns:\n        The item added.\n    \"\"\"\n    return await create_row_handler(dataset_id, SchemaGroup.ITEM, SchemaGroup.ITEM.value, id, item, settings)\n</code></pre>"},{"location":"api_reference/app/routers/items/#pixano.app.routers.items.create_items","title":"<code>create_items(dataset_id, items, settings)</code>  <code>async</code>","text":"<p>Add items in the <code>'item'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>items</code> <code>list[ItemModel]</code> <p>Items to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[ItemModel]</code> <p>List of items added.</p> Source code in <code>pixano/app/routers/items.py</code> <pre><code>@router.post(\"/{dataset_id}\", response_model=list[ItemModel])\nasync def create_items(\n    dataset_id: str,\n    items: list[ItemModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[ItemModel]:\n    \"\"\"Add items in the `'item'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        items: Items to add.\n        settings: App settings.\n\n    Returns:\n        List of items added.\n    \"\"\"\n    return await create_rows_handler(dataset_id, SchemaGroup.ITEM, SchemaGroup.ITEM.value, items, settings)\n</code></pre>"},{"location":"api_reference/app/routers/items/#pixano.app.routers.items.delete_item","title":"<code>delete_item(dataset_id, id, settings)</code>  <code>async</code>","text":"<p>Delete an item from the <code>'item'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>id</code> <code>str</code> <p>ID of the item to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/items.py</code> <pre><code>@router.delete(\"/{dataset_id}/{id}\")\nasync def delete_item(dataset_id: str, id: str, settings: Annotated[Settings, Depends(get_settings)]) -&gt; None:\n    \"\"\"Delete an item from the `'item'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        id: ID of the item to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_row_handler(dataset_id, SchemaGroup.ITEM, SchemaGroup.ITEM.value, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/items/#pixano.app.routers.items.delete_items","title":"<code>delete_items(dataset_id, ids, settings)</code>  <code>async</code>","text":"<p>Delete items from the <code>'item'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>ids</code> <code>Annotated[list[str], Query()]</code> <p>IDs of the items to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/items.py</code> <pre><code>@router.delete(\"/{dataset_id}\")\nasync def delete_items(\n    dataset_id: str,\n    ids: Annotated[list[str], Query()],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; None:\n    \"\"\"Delete items from the `'item'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        ids: IDs of the items to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_rows_handler(dataset_id, SchemaGroup.ITEM, SchemaGroup.ITEM.value, ids, settings)\n</code></pre>"},{"location":"api_reference/app/routers/items/#pixano.app.routers.items.get_item","title":"<code>get_item(dataset_id, id, settings)</code>  <code>async</code>","text":"<p>Get an item from the <code>'item'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>id</code> <code>str</code> <p>ID of the item.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>ItemModel</code> <p>The item.</p> Source code in <code>pixano/app/routers/items.py</code> <pre><code>@router.get(\"/{dataset_id}/{id}\", response_model=ItemModel)\nasync def get_item(dataset_id: str, id: str, settings: Annotated[Settings, Depends(get_settings)]) -&gt; ItemModel:\n    \"\"\"Get an item from the `'item'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        id: ID of the item.\n        settings: App settings.\n\n    Returns:\n        The item.\n    \"\"\"\n    return await get_row_handler(dataset_id, SchemaGroup.ITEM, SchemaGroup.ITEM.value, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/items/#pixano.app.routers.items.get_items","title":"<code>get_items(dataset_id, settings, ids=Query(None), limit=None, skip=0, where=None)</code>  <code>async</code>","text":"<p>Get sources from the <code>'item'</code> table of a dataset.</p> <p>They can be filtered by IDs, a where clause or paginated.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>ids</code> <code>list[str] | None</code> <p>IDs of the sources.</p> <code>Query(None)</code> <code>limit</code> <code>int | None</code> <p>Limit number of sources.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of sources.</p> <code>0</code> <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ItemModel]</code> <p>List of sources.</p> Source code in <code>pixano/app/routers/items.py</code> <pre><code>@router.get(\"/{dataset_id}\", response_model=list[ItemModel])\nasync def get_items(\n    dataset_id: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    ids: list[str] | None = Query(None),\n    limit: int | None = None,\n    skip: int = 0,\n    where: str | None = None,\n) -&gt; list[ItemModel]:\n    \"\"\"Get sources from the `'item'` table of a dataset.\n\n    They can be filtered by IDs, a where clause or paginated.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        settings: App settings.\n        ids: IDs of the sources.\n        limit: Limit number of sources.\n        skip: Skip number of sources.\n        where: Where clause.\n\n    Returns:\n        List of sources.\n    \"\"\"\n    return await get_rows_handler(\n        dataset_id=dataset_id,\n        group=SchemaGroup.ITEM,\n        table=SchemaGroup.ITEM.value,\n        settings=settings,\n        where=where,\n        ids=ids,\n        item_ids=None,\n        limit=limit,\n        skip=skip,\n    )\n</code></pre>"},{"location":"api_reference/app/routers/items/#pixano.app.routers.items.update_item","title":"<code>update_item(dataset_id, id, item, settings)</code>  <code>async</code>","text":"<p>Update an item in the <code>'item'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>id</code> <code>str</code> <p>ID of the item.</p> required <code>item</code> <code>ItemModel</code> <p>Item to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>ItemModel</code> <p>The item updated.</p> Source code in <code>pixano/app/routers/items.py</code> <pre><code>@router.put(\"/{dataset_id}/{id}\", response_model=ItemModel)\nasync def update_item(\n    dataset_id: str,\n    id: str,\n    item: ItemModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; ItemModel:\n    \"\"\"Update an item in the `'item'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        id: ID of the item.\n        item: Item to update.\n        settings: App settings.\n\n    Returns:\n        The item updated.\n    \"\"\"\n    return await update_row_handler(dataset_id, SchemaGroup.ITEM, SchemaGroup.ITEM.value, id, item, settings)\n</code></pre>"},{"location":"api_reference/app/routers/items/#pixano.app.routers.items.update_items","title":"<code>update_items(dataset_id, items, settings)</code>  <code>async</code>","text":"<p>Update items in the <code>'item'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>items</code> <code>list[ItemModel]</code> <p>Items to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[ItemModel]</code> <p>List of items updated.</p> Source code in <code>pixano/app/routers/items.py</code> <pre><code>@router.put(\"/{dataset_id}\", response_model=list[ItemModel])\nasync def update_items(\n    dataset_id: str,\n    items: list[ItemModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[ItemModel]:\n    \"\"\"Update items in the `'item'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        items: Items to update.\n        settings: App settings.\n\n    Returns:\n        List of items updated.\n    \"\"\"\n    return await update_rows_handler(dataset_id, SchemaGroup.ITEM, SchemaGroup.ITEM.value, items, settings)\n</code></pre>"},{"location":"api_reference/app/routers/items_info/","title":"items_info","text":""},{"location":"api_reference/app/routers/items_info/#pixano.app.routers.items_info","title":"<code>pixano.app.routers.items_info</code>","text":""},{"location":"api_reference/app/routers/items_info/#pixano.app.routers.items_info.get_item_info","title":"<code>get_item_info(dataset_id, id, settings)</code>  <code>async</code>","text":"<p>Get an item info.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID.</p> required <code>id</code> <code>str</code> <p>ID.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>ItemInfoModel</code> <p>The item info.</p> Source code in <code>pixano/app/routers/items_info.py</code> <pre><code>@router.get(\"/{dataset_id}/{id}\", response_model=ItemInfoModel)\nasync def get_item_info(\n    dataset_id: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; ItemInfoModel:\n    \"\"\"Get an item info.\n\n    Args:\n        dataset_id: Dataset ID.\n        id: ID.\n        settings: App settings.\n\n    Returns:\n        The item info.\n    \"\"\"\n    items_info = await get_items_info(dataset_id=dataset_id, settings=settings, ids=[id], limit=None, skip=0)\n\n    return items_info[0]\n</code></pre>"},{"location":"api_reference/app/routers/items_info/#pixano.app.routers.items_info.get_items_info","title":"<code>get_items_info(dataset_id, settings, where=None, ids=Query(None), limit=None, skip=0)</code>  <code>async</code>","text":"<p>Get items info.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>where</code> <code>str | None</code> <p>Where clause for the item table.</p> <code>None</code> <code>ids</code> <code>list[str] | None</code> <p>IDs.</p> <code>Query(None)</code> <code>limit</code> <code>int | None</code> <p>Limit number of items.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of items.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[ItemInfoModel]</code> <p>List of items info.</p> Source code in <code>pixano/app/routers/items_info.py</code> <pre><code>@router.get(\"/{dataset_id}\", response_model=list[ItemInfoModel])\nasync def get_items_info(\n    dataset_id: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    where: str | None = None,\n    ids: list[str] | None = Query(None),\n    limit: int | None = None,\n    skip: int = 0,\n) -&gt; list[ItemInfoModel]:\n    \"\"\"Get items info.\n\n    Args:\n        dataset_id: Dataset ID.\n        settings: App settings.\n        where: Where clause for the item table.\n        ids: IDs.\n        limit: Limit number of items.\n        skip: Skip number of items.\n\n    Returns:\n        List of items info.\n    \"\"\"\n    dataset = get_dataset(dataset_id, settings.library_dir, None)\n    assert_table_in_group(dataset, SchemaGroup.ITEM.value, SchemaGroup.ITEM)\n\n    try:\n        item_rows = get_rows(dataset, SchemaGroup.ITEM.value, where, ids, None, limit, skip)\n    except DatasetPaginationError as err:\n        raise HTTPException(status_code=400, detail=str(err))\n    except DatasetAccessError as err:\n        raise HTTPException(status_code=500, detail=str(err))\n\n    items_models = get_models_from_rows(SchemaGroup.ITEM.value, ItemModel, item_rows)\n    item_models_identified = {item.id: item for item in items_models}\n\n    set_ids = {item.id for item in items_models}\n    infos = {\n        id: {\n            group.value: {table: {\"count\": 0} for table in tables}\n            for group, tables in dataset.schema.groups.items()\n            if group.value not in [SchemaGroup.EMBEDDING.value, SchemaGroup.ITEM.value]\n        }\n        for id in set_ids\n    }\n\n    for table_name, table in dataset.open_tables().items():\n        group_name = dataset.schema.get_table_group(table_name).value\n        if group_name in [SchemaGroup.EMBEDDING.value, SchemaGroup.ITEM.value]:\n            continue\n        sql_ids = to_sql_list(set_ids)\n        df: pd.DataFrame = (\n            TableQueryBuilder(table).select([\"item_ref.id\"]).where(f\"item_ref.id in {sql_ids}\").to_pandas()\n        )\n        for id, count in df[\"item_ref.id\"].value_counts().to_dict().items():\n            infos[id][group_name][table_name] = {\"count\": count}\n\n    items_info = [ItemInfoModel(info=info, **item_models_identified[id].model_dump()) for id, info in infos.items()]\n\n    return items_info\n</code></pre>"},{"location":"api_reference/app/routers/models/","title":"models","text":""},{"location":"api_reference/app/routers/models/#pixano.app.routers.models","title":"<code>pixano.app.routers.models</code>","text":""},{"location":"api_reference/app/routers/models/#pixano.app.routers.models.get_model","title":"<code>get_model(model_name, settings)</code>  <code>async</code>","text":"<p>Get model file by name.</p> <p>The model file is expected to be in the models directory with the extension <code>.onnx</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>FileResponse</code> <p>Model file.</p> Source code in <code>pixano/app/routers/models.py</code> <pre><code>@router.get(\"/{model_name}\", response_class=FileResponse)\nasync def get_model(\n    model_name: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; FileResponse:\n    \"\"\"Get model file by name.\n\n    The model file is expected to be in the models directory with the extension `.onnx`.\n\n    Args:\n        model_name: Model name.\n        settings: App settings.\n\n    Returns:\n        Model file.\n    \"\"\"\n    if settings.models_dir is None:\n        raise HTTPException(status_code=500, detail=\"Models directory not set\")\n    model_path = settings.models_dir / f\"{model_name}.onnx\"\n    if not model_path.exists():\n        raise HTTPException(status_code=404, detail=f\"Model {model_name} not found in {settings.models_dir}\")\n    return FileResponse(model_path)\n</code></pre>"},{"location":"api_reference/app/routers/models/#pixano.app.routers.models.get_models","title":"<code>get_models(settings)</code>  <code>async</code>","text":"<p>Get all models in the models directory with the extension <code>.onnx</code>.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of models.</p> Source code in <code>pixano/app/routers/models.py</code> <pre><code>@router.get(\"\", response_model=list[str])\nasync def get_models(\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[str]:\n    \"\"\"Get all models in the models directory with the extension `.onnx`.\n\n    Args:\n        settings: App settings.\n\n    Returns:\n        List of models.\n    \"\"\"\n    if settings.models_dir is None:\n        raise HTTPException(status_code=500, detail=\"Models directory not set\")\n    models = [file.stem for file in settings.models_dir.glob(\"*.onnx\")]\n    return models\n</code></pre>"},{"location":"api_reference/app/routers/sources/","title":"sources","text":""},{"location":"api_reference/app/routers/sources/#pixano.app.routers.sources","title":"<code>pixano.app.routers.sources</code>","text":""},{"location":"api_reference/app/routers/sources/#pixano.app.routers.sources.create_source","title":"<code>create_source(dataset_id, id, source, settings)</code>  <code>async</code>","text":"<p>Add a source in the <code>'source'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>id</code> <code>str</code> <p>ID of the source.</p> required <code>source</code> <code>SourceModel</code> <p>Source to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>SourceModel</code> <p>The source added.</p> Source code in <code>pixano/app/routers/sources.py</code> <pre><code>@router.post(\"/{dataset_id}/{id}\", response_model=SourceModel)\nasync def create_source(\n    dataset_id: str,\n    id: str,\n    source: SourceModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; SourceModel:\n    \"\"\"Add a source in the `'source'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        id: ID of the source.\n        source: Source to add.\n        settings: App settings.\n\n    Returns:\n        The source added.\n    \"\"\"\n    return await create_row_handler(dataset_id, SchemaGroup.SOURCE, SchemaGroup.SOURCE.value, id, source, settings)\n</code></pre>"},{"location":"api_reference/app/routers/sources/#pixano.app.routers.sources.create_sources","title":"<code>create_sources(dataset_id, sources, settings)</code>  <code>async</code>","text":"<p>Add sources in the <code>'source'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>sources</code> <code>list[SourceModel]</code> <p>Sources to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[SourceModel]</code> <p>List of sources added.</p> Source code in <code>pixano/app/routers/sources.py</code> <pre><code>@router.post(\"/{dataset_id}\", response_model=list[SourceModel])\nasync def create_sources(\n    dataset_id: str,\n    sources: list[SourceModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[SourceModel]:\n    \"\"\"Add sources in the `'source'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        sources: Sources to add.\n        settings: App settings.\n\n    Returns:\n        List of sources added.\n    \"\"\"\n    return await create_rows_handler(dataset_id, SchemaGroup.SOURCE, SchemaGroup.SOURCE.value, sources, settings)\n</code></pre>"},{"location":"api_reference/app/routers/sources/#pixano.app.routers.sources.delete_source","title":"<code>delete_source(dataset_id, id, settings)</code>  <code>async</code>","text":"<p>Delete a source from the <code>'source'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>id</code> <code>str</code> <p>ID of the source to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/sources.py</code> <pre><code>@router.delete(\"/{dataset_id}/{id}\")\nasync def delete_source(dataset_id: str, id: str, settings: Annotated[Settings, Depends(get_settings)]) -&gt; None:\n    \"\"\"Delete a source from the `'source'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        id: ID of the source to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_row_handler(dataset_id, SchemaGroup.SOURCE, SchemaGroup.SOURCE.value, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/sources/#pixano.app.routers.sources.delete_sources","title":"<code>delete_sources(dataset_id, ids, settings)</code>  <code>async</code>","text":"<p>Delete sources from the <code>'source'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>ids</code> <code>Annotated[list[str], Query()]</code> <p>IDs of the sources to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/sources.py</code> <pre><code>@router.delete(\"/{dataset_id}\")\nasync def delete_sources(\n    dataset_id: str,\n    ids: Annotated[list[str], Query()],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; None:\n    \"\"\"Delete sources from the `'source'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        ids: IDs of the sources to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_rows_handler(dataset_id, SchemaGroup.SOURCE, SchemaGroup.SOURCE.value, ids, settings)\n</code></pre>"},{"location":"api_reference/app/routers/sources/#pixano.app.routers.sources.get_source","title":"<code>get_source(dataset_id, id, settings)</code>  <code>async</code>","text":"<p>Get a source from the <code>'source'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>id</code> <code>str</code> <p>ID of the source.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>SourceModel</code> <p>The source.</p> Source code in <code>pixano/app/routers/sources.py</code> <pre><code>@router.get(\"/{dataset_id}/{id}\", response_model=SourceModel)\nasync def get_source(dataset_id: str, id: str, settings: Annotated[Settings, Depends(get_settings)]) -&gt; SourceModel:\n    \"\"\"Get a source from the `'source'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        id: ID of the source.\n        settings: App settings.\n\n    Returns:\n        The source.\n    \"\"\"\n    return await get_row_handler(dataset_id, SchemaGroup.SOURCE, SchemaGroup.SOURCE.value, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/sources/#pixano.app.routers.sources.get_sources","title":"<code>get_sources(dataset_id, settings, ids=Query(None), limit=None, skip=0, where=None)</code>  <code>async</code>","text":"<p>Get sources from the <code>'source'</code> table of a dataset.</p> <p>They can be filtered by IDs, a where clause or paginated.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>ids</code> <code>list[str] | None</code> <p>IDs of the sources.</p> <code>Query(None)</code> <code>limit</code> <code>int | None</code> <p>Limit number of sources.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of sources.</p> <code>0</code> <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[SourceModel]</code> <p>List of sources.</p> Source code in <code>pixano/app/routers/sources.py</code> <pre><code>@router.get(\"/{dataset_id}\", response_model=list[SourceModel])\nasync def get_sources(\n    dataset_id: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    ids: list[str] | None = Query(None),\n    limit: int | None = None,\n    skip: int = 0,\n    where: str | None = None,\n) -&gt; list[SourceModel]:\n    \"\"\"Get sources from the `'source'` table of a dataset.\n\n    They can be filtered by IDs, a where clause or paginated.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        settings: App settings.\n        ids: IDs of the sources.\n        limit: Limit number of sources.\n        skip: Skip number of sources.\n        where: Where clause.\n\n    Returns:\n        List of sources.\n    \"\"\"\n    return await get_rows_handler(\n        dataset_id=dataset_id,\n        group=SchemaGroup.SOURCE,\n        table=SchemaGroup.SOURCE.value,\n        settings=settings,\n        where=where,\n        ids=ids,\n        item_ids=None,\n        limit=limit,\n        skip=skip,\n    )\n</code></pre>"},{"location":"api_reference/app/routers/sources/#pixano.app.routers.sources.update_source","title":"<code>update_source(dataset_id, id, source, settings)</code>  <code>async</code>","text":"<p>Update a source in the <code>'source'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>id</code> <code>str</code> <p>ID of the source.</p> required <code>source</code> <code>SourceModel</code> <p>Source to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>SourceModel</code> <p>The source updated.</p> Source code in <code>pixano/app/routers/sources.py</code> <pre><code>@router.put(\"/{dataset_id}/{id}\", response_model=SourceModel)\nasync def update_source(\n    dataset_id: str,\n    id: str,\n    source: SourceModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; SourceModel:\n    \"\"\"Update a source in the `'source'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        id: ID of the source.\n        source: Source to update.\n        settings: App settings.\n\n    Returns:\n        The source updated.\n    \"\"\"\n    return await update_row_handler(dataset_id, SchemaGroup.SOURCE, SchemaGroup.SOURCE.value, id, source, settings)\n</code></pre>"},{"location":"api_reference/app/routers/sources/#pixano.app.routers.sources.update_sources","title":"<code>update_sources(dataset_id, sources, settings)</code>  <code>async</code>","text":"<p>Update sources in the <code>'source'</code> table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>sources</code> <code>list[SourceModel]</code> <p>Sources to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[SourceModel]</code> <p>List of sources updated.</p> Source code in <code>pixano/app/routers/sources.py</code> <pre><code>@router.put(\"/{dataset_id}\", response_model=list[SourceModel])\nasync def update_sources(\n    dataset_id: str,\n    sources: list[SourceModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[SourceModel]:\n    \"\"\"Update sources in the `'source'` table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        sources: Sources to update.\n        settings: App settings.\n\n    Returns:\n        List of sources updated.\n    \"\"\"\n    return await update_rows_handler(dataset_id, SchemaGroup.SOURCE, SchemaGroup.SOURCE.value, sources, settings)\n</code></pre>"},{"location":"api_reference/app/routers/thumbnail/","title":"thumbnail","text":""},{"location":"api_reference/app/routers/thumbnail/#pixano.app.routers.thumbnail","title":"<code>pixano.app.routers.thumbnail</code>","text":""},{"location":"api_reference/app/routers/thumbnail/#pixano.app.routers.thumbnail.get_thumbnail","title":"<code>get_thumbnail(b64_image_path, settings, max_size=128)</code>  <code>async</code>","text":"<p>Generates a thumbnail for image found in media dir.</p> <p>Parameters:</p> Name Type Description Default <code>b64_image_path</code> <code>str</code> <p>image identifier in the media directory</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>max_size</code> <code>PositiveInt</code> <p>maximal resolution of the image</p> <code>128</code> <p>Returns:</p> Type Description <code>StreamingResponse</code> <p>StreamingResponse</p> Source code in <code>pixano/app/routers/thumbnail.py</code> <pre><code>@router.get(\"/{b64_image_path}\", name=\"get_thumbnail\")\nasync def get_thumbnail(\n    b64_image_path: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    max_size: PositiveInt = 128,\n) -&gt; StreamingResponse:\n    \"\"\"Generates a thumbnail for image found in media dir.\n\n    Args:\n        b64_image_path: image identifier in the media directory\n        settings: App settings.\n        max_size: maximal resolution of the image\n\n    Returns:\n        StreamingResponse\n    \"\"\"\n    try:\n        image_path = base64.b64decode(b64_image_path).decode(\"utf-8\")\n    except Exception:\n        raise HTTPException(status_code=400, detail=\"Unable to decode the image path.\")\n\n    try:\n        image = Image.open(settings.media_dir / Path(image_path))  # ou depuis la DB, autre source\u2026\n    except Exception:\n        raise HTTPException(status_code=404, detail=\"Requested image cannot be found.\")\n\n    media_type = image.get_format_mimetype()\n    image_format = image.format\n    if max_size &gt; max(image.width, image.height):\n        max_size = max(image.width, image.height)\n    image.thumbnail((max_size, max_size))  # g\u00e9n\u00e8re le thumbnail\n\n    buf = BytesIO()\n    image.save(buf, format=image_format)\n    buf.seek(0)\n    return StreamingResponse(buf, media_type=media_type)\n</code></pre>"},{"location":"api_reference/app/routers/utils/","title":"utils","text":""},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils","title":"<code>pixano.app.routers.utils</code>","text":""},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.assert_table_in_group","title":"<code>assert_table_in_group(dataset, table, group)</code>","text":"<p>Assert that a table belongs to a group.</p> <p>If the table does not belong to the group, raise a 404 error.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>group</code> <code>SchemaGroup</code> <p>Group.</p> required Source code in <code>pixano/app/routers/utils.py</code> <pre><code>def assert_table_in_group(dataset: Dataset, table: str, group: SchemaGroup) -&gt; None:\n    \"\"\"Assert that a table belongs to a group.\n\n    If the table does not belong to the group, raise a 404 error.\n\n    Args:\n        dataset: Dataset.\n        table: Table name.\n        group: Group.\n    \"\"\"\n    if table in [SchemaGroup.ITEM.value, SchemaGroup.SOURCE.value]:\n        return\n    elif table not in dataset.schema.groups[group]:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Table {table} is not in the {group.value} group table.\",\n        )\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.create_row_handler","title":"<code>create_row_handler(dataset_id, group, table, id, row, settings)</code>  <code>async</code>","text":"<p>Add a row to a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>group</code> <code>SchemaGroup</code> <p>Schema group of the schema of the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the row.</p> required <code>row</code> <code>BaseSchemaModel</code> <p>Row to add.</p> required <code>settings</code> <code>Settings</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>BaseSchemaModel</code> <p>The added row.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>async def create_row_handler(\n    dataset_id: str,\n    group: SchemaGroup,\n    table: str,\n    id: str,\n    row: BaseSchemaModel,\n    settings: Settings,\n) -&gt; BaseSchemaModel:\n    \"\"\"Add a row to a table.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        group: Schema group of the schema of the table.\n        table: Table name.\n        id: ID of the row.\n        row: Row to add.\n        settings: App settings.\n\n    Returns:\n        The added row.\n    \"\"\"\n    if id != row.id:\n        raise HTTPException(status_code=400, detail=\"ID in path and body do not match.\")\n    return (await create_rows_handler(dataset_id=dataset_id, group=group, table=table, rows=[row], settings=settings))[\n        0\n    ]\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.create_rows","title":"<code>create_rows(dataset, table, models)</code>","text":"<p>Add rows to a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>models</code> <code>list[BaseSchemaModel]</code> <p>Models of the rows to add.</p> required <p>Returns:</p> Type Description <code>list[BaseSchema]</code> <p>The added rows.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>def create_rows(\n    dataset: Dataset,\n    table: str,\n    models: list[BaseSchemaModel],\n) -&gt; list[BaseSchema]:\n    \"\"\"Add rows to a table.\n\n    Args:\n        dataset: Dataset containing the table.\n        table: Table name.\n        models: Models of the rows to add.\n\n    Returns:\n        The added rows.\n    \"\"\"\n    try:\n        rows: list[BaseSchema] = BaseSchemaModel.to_rows(models, dataset)\n    except Exception as err:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Invalid data.\\n\" + str(err),\n        )\n\n    try:\n        created_rows = dataset.add_data(table, rows)\n    except DatasetIntegrityError as err:\n        raise HTTPException(status_code=400, detail=\"Dataset integrity compromised.\\n\" + str(err))\n    except ValueError as err:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Invalid data.\\n\" + str(err),\n        )\n\n    return created_rows\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.create_rows_handler","title":"<code>create_rows_handler(dataset_id, group, table, rows, settings)</code>  <code>async</code>","text":"<p>Add rows to a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>group</code> <code>SchemaGroup</code> <p>Schema group of the schema of the table.</p> required <code>table</code> <code>str</code> <p>Table name containing the rows.</p> required <code>rows</code> <code>list[BaseSchemaModel]</code> <p>Rows to add.</p> required <code>settings</code> <code>Settings</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[BaseSchemaModel]</code> <p>List of updated rows.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>async def create_rows_handler(\n    dataset_id: str,\n    group: SchemaGroup,\n    table: str,\n    rows: list[BaseSchemaModel],\n    settings: Settings,\n) -&gt; list[BaseSchemaModel]:\n    \"\"\"Add rows to a table.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        group: Schema group of the schema of the table.\n        table: Table name containing the rows.\n        rows: Rows to add.\n        settings: App settings.\n\n    Returns:\n        List of updated rows.\n    \"\"\"\n    group = validate_group(group)\n    dataset = get_dataset(dataset_id, settings.library_dir, settings.media_dir)\n    assert_table_in_group(dataset, table, group)\n    rows_rows = create_rows(dataset, table, rows)\n    rows_models = get_models_from_rows(table, _SCHEMA_GROUP_TO_SCHEMA_MODEL_DICT[group], rows_rows)\n    return rows_models\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.delete_row_handler","title":"<code>delete_row_handler(dataset_id, group, table, id, settings)</code>  <code>async</code>","text":"<p>Delete a row from a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>group</code> <code>SchemaGroup</code> <p>Schema group of the schema of the table.</p> required <code>table</code> <code>str</code> <p>Table name containing the row.</p> required <code>id</code> <code>str</code> <p>ID of the row to delete.</p> required <code>settings</code> <code>Settings</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/utils.py</code> <pre><code>async def delete_row_handler(dataset_id: str, group: SchemaGroup, table: str, id: str, settings: Settings) -&gt; None:\n    \"\"\"Delete a row from a table.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        group: Schema group of the schema of the table.\n        table: Table name containing the row.\n        id: ID of the row to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_rows_handler(dataset_id, group, table, ids=[id], settings=settings)\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.delete_rows","title":"<code>delete_rows(dataset, table, ids)</code>","text":"<p>Delete rows from a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>ids</code> <code>list[str]</code> <p>IDs of the rows to delete.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>IDs not found.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>def delete_rows(\n    dataset: Dataset,\n    table: str,\n    ids: list[str],\n) -&gt; list[str]:\n    \"\"\"Delete rows from a table.\n\n    Args:\n        dataset: Dataset containing the table.\n        table: Table name.\n        ids: IDs of the rows to delete.\n\n    Returns:\n        IDs not found.\n    \"\"\"\n    try:\n        ids_not_found = dataset.delete_data(table, ids)\n    except ValueError:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Invalid query parameters. ids and item_ids cannot be set at the same time\",\n        )\n    return ids_not_found\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.delete_rows_handler","title":"<code>delete_rows_handler(dataset_id, group, table, ids, settings)</code>  <code>async</code>","text":"<p>Delete rows from a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>group</code> <code>SchemaGroup</code> <p>Schema group of the schema of the table.</p> required <code>table</code> <code>str</code> <p>Table name containing the rows.</p> required <code>ids</code> <code>list[str]</code> <p>IDs of the rows to delete.</p> required <code>settings</code> <code>Settings</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/utils.py</code> <pre><code>async def delete_rows_handler(\n    dataset_id: str,\n    group: SchemaGroup,\n    table: str,\n    ids: list[str],\n    settings: Settings,\n) -&gt; None:\n    \"\"\"Delete rows from a table.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        group: Schema group of the schema of the table.\n        table: Table name containing the rows.\n        ids: IDs of the rows to delete.\n        settings: App settings.\n    \"\"\"\n    group = validate_group(group)\n    dataset = get_dataset(dataset_id, settings.library_dir, settings.media_dir)\n    assert_table_in_group(dataset, table, group)\n    delete_rows(dataset, table, ids)\n    return None\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.get_dataset","title":"<code>get_dataset(dataset_id, dir, media_dir=None)</code>","text":"<p>Get a dataset.</p> <p>If the dataset is not found, raise a 404 error.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID.</p> required <code>dir</code> <code>Path</code> <p>Directory containing the dataset.</p> required <code>media_dir</code> <code>Path | None</code> <p>Directory containing the media files.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The dataset.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>def get_dataset(dataset_id: str, dir: Path, media_dir: Path | None = None) -&gt; Dataset:\n    \"\"\"Get a dataset.\n\n    If the dataset is not found, raise a 404 error.\n\n    Args:\n        dataset_id: Dataset ID.\n        dir: Directory containing the dataset.\n        media_dir: Directory containing the media files.\n\n    Returns:\n        The dataset.\n    \"\"\"\n    try:\n        dataset = Dataset.find(dataset_id, dir, media_dir)\n    except FileNotFoundError:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Dataset {dataset_id} not found in {dir.absolute()}.\",\n        )\n    return dataset\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.get_model_from_row","title":"<code>get_model_from_row(table, model_type, row)</code>","text":"<p>Get a model from a row.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table name containing the row.</p> required <code>model_type</code> <code>type[T]</code> <p>Model type to create.</p> required <code>row</code> <code>BaseSchema</code> <p>Row.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The model.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>def get_model_from_row(table: str, model_type: type[T], row: BaseSchema) -&gt; T:\n    \"\"\"Get a model from a row.\n\n    Args:\n        table: Table name containing the row.\n        model_type: Model type to create.\n        row: Row.\n\n    Returns:\n        The model.\n    \"\"\"\n    try:\n        is_group = issubclass(model_type, BaseSchemaModel)\n        if not is_group:\n            raise HTTPException(status_code=500, detail=\"Model type is not a subclass of BaseModelSchema.\")\n    except TypeError:\n        raise HTTPException(status_code=500, detail=\"Model type is not a subclass of BaseModelSchema.\")\n    if issubclass(model_type, AnnotationModel):\n        group = SchemaGroup.ANNOTATION\n    elif issubclass(model_type, EmbeddingModel):\n        group = SchemaGroup.EMBEDDING\n    elif issubclass(model_type, EntityModel):\n        group = SchemaGroup.ENTITY\n    elif issubclass(model_type, ItemModel):\n        group = SchemaGroup.ITEM\n    elif issubclass(model_type, SourceModel):\n        group = SchemaGroup.SOURCE\n    elif issubclass(model_type, ViewModel):\n        group = SchemaGroup.VIEW\n    else:\n        raise HTTPException(\n            status_code=500,\n            detail=\"Model type not correct.\",\n        )\n\n    pixano_schema_type = get_super_type_from_dict(type(row), _PIXANO_SCHEMA_REGISTRY)\n\n    if pixano_schema_type is None:\n        raise HTTPException(\n            status_code=500,\n            detail=\"Schema type not found in registry.\",\n        )\n    table_info = TableInfo(name=table, group=group.value, base_schema=pixano_schema_type.__name__)\n    model = model_type.from_row(row, table_info)\n    return model\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.get_models_from_rows","title":"<code>get_models_from_rows(table, model_type, rows)</code>","text":"<p>Get models from rows.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table name containing the rows.</p> required <code>model_type</code> <code>type[T]</code> <p>Model type to create.</p> required <code>rows</code> <code>list[BaseSchema]</code> <p>Rows.</p> required <p>Returns:</p> Type Description <code>list[T]</code> <p>List of models.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>def get_models_from_rows(\n    table: str,\n    model_type: type[T],\n    rows: list[BaseSchema],\n) -&gt; list[T]:\n    \"\"\"Get models from rows.\n\n    Args:\n        table: Table name containing the rows.\n        model_type: Model type to create.\n        rows: Rows.\n\n    Returns:\n        List of models.\n    \"\"\"\n    return [get_model_from_row(table, model_type, row) for row in rows]\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.get_row_handler","title":"<code>get_row_handler(dataset_id, group, table, id, settings)</code>  <code>async</code>","text":"<p>Get a row model.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>group</code> <code>SchemaGroup</code> <p>Schema group of the schema of the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the row.</p> required <code>settings</code> <code>Settings</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>BaseSchemaModel</code> <p>The model.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>async def get_row_handler(\n    dataset_id: str, group: SchemaGroup, table: str, id: str, settings: Settings\n) -&gt; BaseSchemaModel:\n    \"\"\"Get a row model.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        group: Schema group of the schema of the table.\n        table: Table name.\n        id: ID of the row.\n        settings: App settings.\n\n    Returns:\n        The model.\n    \"\"\"\n    return (await get_rows_handler(dataset_id, group, table, settings, ids=[id], item_ids=None, limit=None, skip=0))[0]\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.get_rows","title":"<code>get_rows(dataset, table, where=None, ids=None, item_ids=None, limit=None, skip=0, sortcol=None, order=None)</code>","text":"<p>Get rows from a table.</p> <p>The rows can be filtered by a where clause, IDs, item IDs or a limit and a skip.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <code>ids</code> <code>list[str] | None</code> <p>IDs of the rows.</p> <code>None</code> <code>item_ids</code> <code>list[str] | None</code> <p>Item IDs of the rows.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Limit number of rows.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of rows.</p> <code>0</code> <code>sortcol</code> <code>str | None</code> <p>column to order by</p> <code>None</code> <code>order</code> <code>str | None</code> <p>sort order (asc or desc)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[BaseSchema]</code> <p>List of rows.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>def get_rows(\n    dataset: Dataset,\n    table: str,\n    where: str | None = None,\n    ids: list[str] | None = None,\n    item_ids: list[str] | None = None,\n    limit: int | None = None,\n    skip: int = 0,\n    sortcol: str | None = None,\n    order: str | None = None,\n) -&gt; list[BaseSchema]:\n    \"\"\"Get rows from a table.\n\n    The rows can be filtered by a where clause, IDs, item IDs or a limit and a skip.\n\n    Args:\n        dataset: Dataset containing the table.\n        table: Table name.\n        where: Where clause.\n        ids: IDs of the rows.\n        item_ids: Item IDs of the rows.\n        limit: Limit number of rows.\n        skip: Skip number of rows.\n        sortcol: column to order by\n        order: sort order (asc or desc)\n\n    Returns:\n        List of rows.\n    \"\"\"\n    try:\n        rows = dataset.get_data(\n            table_name=table,\n            where=where,\n            ids=ids,\n            limit=limit,\n            skip=skip,\n            item_ids=item_ids,\n            sortcol=sortcol,\n            order=order,\n        )\n    except DatasetPaginationError as err:\n        raise HTTPException(status_code=400, detail=\"Invalid query parameters. \" + str(err))\n    except DatasetAccessError as err:\n        raise HTTPException(status_code=500, detail=\"Internal server error. \" + str(err))\n\n    if rows == [] or rows is None:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"No rows found for {dataset.info.id}/{table}.\",\n        )\n    return rows\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.get_rows_handler","title":"<code>get_rows_handler(dataset_id, group, table, settings, where=None, ids=None, item_ids=None, limit=None, skip=0)</code>  <code>async</code>","text":"<p>Get row models.</p> <p>Rows can be filtered by a where clause, IDs, item IDs or a limit and a skip.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>group</code> <code>SchemaGroup</code> <p>Schema group of the schema of the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>settings</code> <code>Settings</code> <p>App settings.</p> required <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <code>ids</code> <code>list[str] | None</code> <p>IDs of the rows.</p> <code>None</code> <code>item_ids</code> <code>list[str] | None</code> <p>Item IDs of the rows.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Limit number of rows.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of rows.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[BaseSchemaModel]</code> <p>List of models.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>async def get_rows_handler(\n    dataset_id: str,\n    group: SchemaGroup,\n    table: str,\n    settings: Settings,\n    where: str | None = None,\n    ids: list[str] | None = None,\n    item_ids: list[str] | None = None,\n    limit: int | None = None,\n    skip: int = 0,\n) -&gt; list[BaseSchemaModel]:\n    \"\"\"Get row models.\n\n    Rows can be filtered by a where clause, IDs, item IDs or a limit and a skip.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        group: Schema group of the schema of the table.\n        table: Table name.\n        settings: App settings.\n        where: Where clause.\n        ids: IDs of the rows.\n        item_ids: Item IDs of the rows.\n        limit: Limit number of rows.\n        skip: Skip number of rows.\n\n    Returns:\n        List of models.\n    \"\"\"\n    group = validate_group(group)\n    dataset = get_dataset(dataset_id, settings.library_dir, settings.media_dir)\n    assert_table_in_group(dataset, table, group)\n    rows = get_rows(dataset=dataset, table=table, where=where, ids=ids, item_ids=item_ids, limit=limit, skip=skip)\n    models = get_models_from_rows(table, _SCHEMA_GROUP_TO_SCHEMA_MODEL_DICT[group], rows)\n    return models\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.update_row_handler","title":"<code>update_row_handler(dataset_id, group, table, id, row, settings)</code>  <code>async</code>","text":"<p>Update a row in a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>group</code> <code>SchemaGroup</code> <p>Schema group of the schema of the table.</p> required <code>table</code> <code>str</code> <p>Table name containing the row.</p> required <code>id</code> <code>str</code> <p>ID of the row.</p> required <code>row</code> <code>BaseSchemaModel</code> <p>Row to update.</p> required <code>settings</code> <code>Settings</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>BaseSchemaModel</code> <p>The updated row.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>async def update_row_handler(\n    dataset_id: str,\n    group: SchemaGroup,\n    table: str,\n    id: str,\n    row: BaseSchemaModel,\n    settings: Settings,\n) -&gt; BaseSchemaModel:\n    \"\"\"Update a row in a table.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        group: Schema group of the schema of the table.\n        table: Table name containing the row.\n        id: ID of the row.\n        row: Row to update.\n        settings: App settings.\n\n    Returns:\n        The updated row.\n    \"\"\"\n    if id != row.id:\n        raise HTTPException(status_code=400, detail=\"ID in path and body do not match.\")\n    return (await update_rows_handler(dataset_id=dataset_id, group=group, table=table, rows=[row], settings=settings))[\n        0\n    ]\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.update_rows","title":"<code>update_rows(dataset, table, models)</code>","text":"<p>Update rows in a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>models</code> <code>list[BaseSchemaModel]</code> <p>Models of the rows to update.</p> required <p>Returns:</p> Type Description <code>list[BaseSchema]</code> <p>The updated rows.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>def update_rows(\n    dataset: Dataset,\n    table: str,\n    models: list[BaseSchemaModel],\n) -&gt; list[BaseSchema]:\n    \"\"\"Update rows in a table.\n\n    Args:\n        dataset: Dataset containing the table.\n        table: Table name.\n        models: Models of the rows to update.\n\n    Returns:\n        The updated rows.\n    \"\"\"\n    try:\n        rows: list[BaseSchema] = BaseSchemaModel.to_rows(models, dataset)\n    except Exception as err:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Invalid data.\\n\" + str(err),\n        )\n\n    try:\n        updated_rows = dataset.update_data(table, rows)\n    except DatasetIntegrityError as err:\n        raise HTTPException(status_code=400, detail=\"Dataset integrity compromised.\\n\" + str(err))\n    except ValueError as err:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Invalid data.\\n\" + str(err),\n        )\n\n    # TODO: return updated rows instead of input rows\n    # TODO: check if rows are updated or created which change HTTP status code\n    return updated_rows\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.update_rows_handler","title":"<code>update_rows_handler(dataset_id, group, table, rows, settings)</code>  <code>async</code>","text":"<p>Update rows in a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>group</code> <code>SchemaGroup</code> <p>Schema group of the schema of the table.</p> required <code>table</code> <code>str</code> <p>Table name containing the rows.</p> required <code>rows</code> <code>list[BaseSchemaModel]</code> <p>Rows to update.</p> required <code>settings</code> <code>Settings</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[BaseSchemaModel]</code> <p>List of updated rows.</p> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>async def update_rows_handler(\n    dataset_id: str,\n    group: SchemaGroup,\n    table: str,\n    rows: list[BaseSchemaModel],\n    settings: Settings,\n) -&gt; list[BaseSchemaModel]:\n    \"\"\"Update rows in a table.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        group: Schema group of the schema of the table.\n        table: Table name containing the rows.\n        rows: Rows to update.\n        settings: App settings.\n\n    Returns:\n        List of updated rows.\n    \"\"\"\n    group = validate_group(group)\n    dataset = get_dataset(dataset_id, settings.library_dir, settings.media_dir)\n    assert_table_in_group(dataset, table, group)\n    row_rows = update_rows(dataset, table, rows)\n    row_models = get_models_from_rows(table, _SCHEMA_GROUP_TO_SCHEMA_MODEL_DICT[group], row_rows)\n    return row_models\n</code></pre>"},{"location":"api_reference/app/routers/utils/#pixano.app.routers.utils.validate_group","title":"<code>validate_group(group, valid_groups=set(SchemaGroup))</code>","text":"<p>Assert that a group is valid.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str | SchemaGroup</code> <p>Group.</p> required <code>valid_groups</code> <code>set[SchemaGroup]</code> <p>The valid groups.</p> <code>set(SchemaGroup)</code> Source code in <code>pixano/app/routers/utils.py</code> <pre><code>def validate_group(\n    group: str | SchemaGroup,\n    valid_groups: set[SchemaGroup] = set(SchemaGroup),\n) -&gt; SchemaGroup:\n    \"\"\"Assert that a group is valid.\n\n    Args:\n        group: Group.\n        valid_groups: The valid groups.\n    \"\"\"\n    try:\n        group = SchemaGroup(group)\n    except ValueError:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Group {group} is not a SchemaGroup.\",\n        )\n    if group not in valid_groups:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Group {group.value} is not valid.\",\n        )\n    return group\n</code></pre>"},{"location":"api_reference/app/routers/views/","title":"views","text":""},{"location":"api_reference/app/routers/views/#pixano.app.routers.views","title":"<code>pixano.app.routers.views</code>","text":""},{"location":"api_reference/app/routers/views/#pixano.app.routers.views.create_view","title":"<code>create_view(dataset_id, table, id, view, settings)</code>  <code>async</code>","text":"<p>Add an view in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the view.</p> required <code>view</code> <code>ViewModel</code> <p>View to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>ViewModel</code> <p>The view added.</p> Source code in <code>pixano/app/routers/views.py</code> <pre><code>@router.post(\"/{dataset_id}/{table}/{id}\", response_model=ViewModel)\nasync def create_view(\n    dataset_id: str,\n    table: str,\n    id: str,\n    view: ViewModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; ViewModel:\n    \"\"\"Add an view in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the view.\n        view: View to add.\n        settings: App settings.\n\n    Returns:\n        The view added.\n    \"\"\"\n    return await create_row_handler(dataset_id, SchemaGroup.VIEW, table, id, view, settings)\n</code></pre>"},{"location":"api_reference/app/routers/views/#pixano.app.routers.views.create_views","title":"<code>create_views(dataset_id, table, views, settings)</code>  <code>async</code>","text":"<p>Add views in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>views</code> <code>list[ViewModel]</code> <p>Views to add.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[ViewModel]</code> <p>List of views added.</p> Source code in <code>pixano/app/routers/views.py</code> <pre><code>@router.post(\"/{dataset_id}/{table}\", response_model=list[ViewModel])\nasync def create_views(\n    dataset_id: str,\n    table: str,\n    views: list[ViewModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[ViewModel]:\n    \"\"\"Add views in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        views: Views to add.\n        settings: App settings.\n\n    Returns:\n        List of views added.\n    \"\"\"\n    return await create_rows_handler(dataset_id, SchemaGroup.VIEW, table, views, settings)\n</code></pre>"},{"location":"api_reference/app/routers/views/#pixano.app.routers.views.delete_view","title":"<code>delete_view(dataset_id, table, id, settings)</code>  <code>async</code>","text":"<p>Delete an view from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the view to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/views.py</code> <pre><code>@router.delete(\"/{dataset_id}/{table}/{id}\")\nasync def delete_view(\n    dataset_id: str, table: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; None:\n    \"\"\"Delete an view from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the view to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_row_handler(dataset_id, SchemaGroup.VIEW, table, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/views/#pixano.app.routers.views.delete_views","title":"<code>delete_views(dataset_id, table, ids, settings)</code>  <code>async</code>","text":"<p>Delete views from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>ids</code> <code>Annotated[list[str], Query()]</code> <p>IDs of the views to delete.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required Source code in <code>pixano/app/routers/views.py</code> <pre><code>@router.delete(\"/{dataset_id}/{table}\")\nasync def delete_views(\n    dataset_id: str,\n    table: str,\n    ids: Annotated[list[str], Query()],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; None:\n    \"\"\"Delete views from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        ids: IDs of the views to delete.\n        settings: App settings.\n    \"\"\"\n    return await delete_rows_handler(dataset_id, SchemaGroup.VIEW, table, ids, settings)\n</code></pre>"},{"location":"api_reference/app/routers/views/#pixano.app.routers.views.get_view","title":"<code>get_view(dataset_id, table, id, settings)</code>  <code>async</code>","text":"<p>Get a view from a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the view.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>ViewModel</code> <p>The view.</p> Source code in <code>pixano/app/routers/views.py</code> <pre><code>@router.get(\"/{dataset_id}/{table}/{id}\", response_model=ViewModel)\nasync def get_view(\n    dataset_id: str, table: str, id: str, settings: Annotated[Settings, Depends(get_settings)]\n) -&gt; ViewModel:\n    \"\"\"Get a view from a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the view.\n        settings: App settings.\n\n    Returns:\n        The view.\n    \"\"\"\n    return await get_row_handler(dataset_id, SchemaGroup.VIEW, table, id, settings)\n</code></pre>"},{"location":"api_reference/app/routers/views/#pixano.app.routers.views.get_views","title":"<code>get_views(dataset_id, table, settings, ids=Query(None), limit=None, skip=0, where=None, item_ids=Query(None))</code>  <code>async</code>","text":"<p>Get views from a table of a dataset.</p> <p>They can be filtered by IDs, item IDs, a where clause or paginated.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>ids</code> <code>list[str] | None</code> <p>IDs of the views.</p> <code>Query(None)</code> <code>limit</code> <code>int | None</code> <p>Limit number of views.</p> <code>None</code> <code>skip</code> <code>int</code> <p>Skip number of views.</p> <code>0</code> <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <code>item_ids</code> <code>list[str] | None</code> <p>Item IDs.</p> <code>Query(None)</code> <p>Returns:</p> Type Description <code>list[ViewModel]</code> <p>List of views.</p> Source code in <code>pixano/app/routers/views.py</code> <pre><code>@router.get(\"/{dataset_id}/{table}\", response_model=list[ViewModel])\nasync def get_views(\n    dataset_id: str,\n    table: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n    ids: list[str] | None = Query(None),\n    limit: int | None = None,\n    skip: int = 0,\n    where: str | None = None,\n    item_ids: list[str] | None = Query(None),\n) -&gt; list[ViewModel]:\n    \"\"\"Get views from a table of a dataset.\n\n    They can be filtered by IDs, item IDs, a where clause or paginated.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        settings: App settings.\n        ids: IDs of the views.\n        limit: Limit number of views.\n        skip: Skip number of views.\n        where: Where clause.\n        item_ids: Item IDs.\n\n    Returns:\n        List of views.\n    \"\"\"\n    return await get_rows_handler(\n        dataset_id=dataset_id,\n        group=SchemaGroup.VIEW,\n        table=table,\n        settings=settings,\n        where=where,\n        ids=ids,\n        item_ids=item_ids,\n        limit=limit,\n        skip=skip,\n    )\n</code></pre>"},{"location":"api_reference/app/routers/views/#pixano.app.routers.views.update_view","title":"<code>update_view(dataset_id, table, id, view, settings)</code>  <code>async</code>","text":"<p>Update an view in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>id</code> <code>str</code> <p>ID of the view.</p> required <code>view</code> <code>ViewModel</code> <p>View to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>ViewModel</code> <p>The view updated.</p> Source code in <code>pixano/app/routers/views.py</code> <pre><code>@router.put(\"/{dataset_id}/{table}/{id}\", response_model=ViewModel)\nasync def update_view(\n    dataset_id: str,\n    table: str,\n    id: str,\n    view: ViewModel,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; ViewModel:\n    \"\"\"Update an view in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        id: ID of the view.\n        view: View to update.\n        settings: App settings.\n\n    Returns:\n        The view updated.\n    \"\"\"\n    return await update_row_handler(dataset_id, SchemaGroup.VIEW, table, id, view, settings)\n</code></pre>"},{"location":"api_reference/app/routers/views/#pixano.app.routers.views.update_views","title":"<code>update_views(dataset_id, table, views, settings)</code>  <code>async</code>","text":"<p>Update views in a table of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset ID containing the table.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>views</code> <code>list[ViewModel]</code> <p>Views to update.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <p>Returns:</p> Type Description <code>list[ViewModel]</code> <p>List of views updated.</p> Source code in <code>pixano/app/routers/views.py</code> <pre><code>@router.put(\"/{dataset_id}/{table}\", response_model=list[ViewModel])\nasync def update_views(\n    dataset_id: str,\n    table: str,\n    views: list[ViewModel],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[ViewModel]:\n    \"\"\"Update views in a table of a dataset.\n\n    Args:\n        dataset_id: Dataset ID containing the table.\n        table: Table name.\n        views: Views to update.\n        settings: App settings.\n\n    Returns:\n        List of views updated.\n    \"\"\"\n    return await update_rows_handler(dataset_id, SchemaGroup.VIEW, table, views, settings)\n</code></pre>"},{"location":"api_reference/app/routers/inference/conditional_generation/","title":"conditional_generation","text":""},{"location":"api_reference/app/routers/inference/conditional_generation/#pixano.app.routers.inference.conditional_generation","title":"<code>pixano.app.routers.inference.conditional_generation</code>","text":""},{"location":"api_reference/app/routers/inference/conditional_generation/#pixano.app.routers.inference.conditional_generation.call_text_image_conditional_generation","title":"<code>call_text_image_conditional_generation(dataset_id, conversation, messages, model, settings, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, temperature=DEFAULT_TEMPERATURE, role_system=DEFAULT_ROLE_SYSTEM, role_user=DEFAULT_ROLE_USER, role_assistant=DEFAULT_ROLE_ASSISTANT)</code>  <code>async</code>","text":"<p>Call a text image conditional generation model for a conversation.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>Annotated[str, Body(embed=True)]</code> <p>The ID of the dataset to use.</p> required <code>conversation</code> <code>Annotated[EntityModel, Body(embed=True)]</code> <p>The conversation to use.</p> required <code>messages</code> <code>Annotated[list[AnnotationModel], Body(embed=True)]</code> <p>The messages to use.</p> required <code>model</code> <code>Annotated[str, Body(embed=True)]</code> <p>The name of the model to use.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>max_new_tokens</code> <code>Annotated[int, Body(embed=True)]</code> <p>The maximum number of tokens to generate.</p> <code>DEFAULT_MAX_NEW_TOKENS</code> <code>temperature</code> <code>Annotated[float, Body(embed=True)]</code> <p>The temperature to use.</p> <code>DEFAULT_TEMPERATURE</code> <code>role_system</code> <code>Annotated[str, Body(embed=True)]</code> <p>The role of the system.</p> <code>DEFAULT_ROLE_SYSTEM</code> <code>role_user</code> <code>Annotated[str, Body(embed=True)]</code> <p>The role of the user.</p> <code>DEFAULT_ROLE_USER</code> <code>role_assistant</code> <code>Annotated[str, Body(embed=True)]</code> <p>The role of the assistant.</p> <code>DEFAULT_ROLE_ASSISTANT</code> <p>Returns: The generated message model.</p> Source code in <code>pixano/app/routers/inference/conditional_generation.py</code> <pre><code>@router.post(\n    \"/text-image\",\n    response_model=AnnotationModel,\n)\nasync def call_text_image_conditional_generation(\n    dataset_id: Annotated[str, Body(embed=True)],\n    conversation: Annotated[EntityModel, Body(embed=True)],\n    messages: Annotated[list[AnnotationModel], Body(embed=True)],\n    model: Annotated[str, Body(embed=True)],\n    settings: Annotated[Settings, Depends(get_settings)],\n    max_new_tokens: Annotated[int, Body(embed=True)] = DEFAULT_MAX_NEW_TOKENS,\n    temperature: Annotated[float, Body(embed=True)] = DEFAULT_TEMPERATURE,\n    role_system: Annotated[str, Body(embed=True)] = DEFAULT_ROLE_SYSTEM,\n    role_user: Annotated[str, Body(embed=True)] = DEFAULT_ROLE_USER,\n    role_assistant: Annotated[str, Body(embed=True)] = DEFAULT_ROLE_ASSISTANT,\n) -&gt; AnnotationModel:\n    \"\"\"Call a text image conditional generation model for a conversation.\n\n    Args:\n        dataset_id: The ID of the dataset to use.\n        conversation: The conversation to use.\n        messages: The messages to use.\n        model: The name of the model to use.\n        settings: App settings.\n        max_new_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use.\n        role_system: The role of the system.\n        role_user: The role of the user.\n        role_assistant: The role of the assistant.\n\n    Returns: The generated message model.\n    \"\"\"\n    dataset = get_dataset(dataset_id=dataset_id, dir=settings.library_dir, media_dir=settings.media_dir)\n    client = get_client_from_settings(settings=settings)\n\n    if not is_conversation(dataset.schema.schemas[conversation.table_info.name]):\n        raise HTTPException(status_code=400, detail=\"Conversation must be a conversation.\")\n\n    conversation_row: Conversation = conversation.to_row(dataset)\n\n    messages_in_one_table = len({m.table_info.name for m in messages}) == 1\n    if not messages_in_one_table:\n        raise HTTPException(status_code=400, detail=\"Only one table for messages is allowed.\")\n    elif not is_message(dataset.schema.schemas[messages[0].table_info.name]):\n        raise HTTPException(status_code=400, detail=\"Messages must be a message.\")\n\n    messages_rows: list[Message] = []\n    for m in messages:\n        m_row: Message = m.to_row(dataset)\n        messages_rows.append(m_row)\n\n    source = get_model_source(dataset=dataset, model=model)\n\n    try:\n        infered_message = await text_image_conditional_generation(\n            client=client,\n            source=source,\n            dataset=dataset,\n            media_dir=settings.media_dir,\n            messages=messages_rows,\n            conversation=conversation_row,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            role_system=role_system,\n            role_user=role_user,\n            role_assistant=role_assistant,\n        )\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e)) from e\n\n    message_model = AnnotationModel.from_row(row=infered_message, table_info=messages[0].table_info)\n\n    return message_model\n</code></pre>"},{"location":"api_reference/app/routers/inference/mask_generation/","title":"mask_generation","text":""},{"location":"api_reference/app/routers/inference/mask_generation/#pixano.app.routers.inference.mask_generation","title":"<code>pixano.app.routers.inference.mask_generation</code>","text":""},{"location":"api_reference/app/routers/inference/mask_generation/#pixano.app.routers.inference.mask_generation.ImageMaskGenerationOutput","title":"<code>ImageMaskGenerationOutput(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Image mask generation output.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/routers/inference/mask_generation/#pixano.app.routers.inference.mask_generation.VideoMaskGenerationOutput","title":"<code>VideoMaskGenerationOutput(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Video masks generation output.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/routers/inference/mask_generation/#pixano.app.routers.inference.mask_generation.call_image_mask_generation","title":"<code>call_image_mask_generation(dataset_id, image, model, mask_table_name, settings, entity=None, bbox=None, points=None, labels=None)</code>  <code>async</code>","text":"<p>Perform image mask generation on an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>Annotated[str, Body(embed=True)]</code> <p>The ID of the dataset to use.</p> required <code>image</code> <code>Annotated[ViewModel, Body(embed=True)]</code> <p>The image to use for detection.</p> required <code>entity</code> <code>Annotated[EntityModel | None, Body(embed=True)]</code> <p>The entity to use for detection.</p> <code>None</code> <code>model</code> <code>Annotated[str, Body(embed=True)]</code> <p>The name of the model to use.</p> required <code>mask_table_name</code> <code>Annotated[str, Body(embed=True)]</code> <p>The name of the table to use for masks in dataset.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>bbox</code> <code>Annotated[BBox | None, Body(embed=True)]</code> <p>Input bounding box or None.</p> <code>None</code> <code>points</code> <code>Annotated[list[list[int]] | None, Body(embed=True)]</code> <p>Input points or None.</p> <code>None</code> <code>labels</code> <code>Annotated[list[int] | None, Body(embed=True)]</code> <p>Labels for input points, or None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageMaskGenerationOutput</code> <p>The generated mask.</p> Source code in <code>pixano/app/routers/inference/mask_generation.py</code> <pre><code>@router.post(\n    \"/image\",\n    response_model=ImageMaskGenerationOutput,\n)\nasync def call_image_mask_generation(\n    dataset_id: Annotated[str, Body(embed=True)],\n    image: Annotated[ViewModel, Body(embed=True)],\n    model: Annotated[str, Body(embed=True)],\n    mask_table_name: Annotated[str, Body(embed=True)],\n    settings: Annotated[Settings, Depends(get_settings)],\n    entity: Annotated[EntityModel | None, Body(embed=True)] = None,\n    bbox: Annotated[BBox | None, Body(embed=True)] = None,\n    points: Annotated[list[list[int]] | None, Body(embed=True)] = None,\n    labels: Annotated[list[int] | None, Body(embed=True)] = None,\n) -&gt; ImageMaskGenerationOutput:\n    \"\"\"Perform image mask generation on an image.\n\n    Args:\n        dataset_id: The ID of the dataset to use.\n        image: The image to use for detection.\n        entity: The entity to use for detection.\n        model: The name of the model to use.\n        mask_table_name: The name of the table to use for masks in dataset.\n        settings: App settings.\n        bbox: Input bounding box or None.\n        points: Input points or None.\n        labels: Labels for input points, or None.\n\n    Returns:\n        The generated mask.\n    \"\"\"\n    global store_last_image_id\n\n    dataset = get_dataset(dataset_id=dataset_id, dir=settings.library_dir, media_dir=settings.media_dir)\n    client = get_client_from_settings(settings=settings)\n\n    if not is_image(dataset.schema.schemas[image.table_info.name]):\n        raise HTTPException(status_code=400, detail=\"Image must be an image.\")\n\n    image_row: Image = image.to_row(dataset)\n    entity_row: Entity | None = entity.to_row(dataset) if entity else None\n    source = get_model_source(dataset=dataset, model=model)\n\n    try:\n        mask_output: tuple[\n            CompressedRLE, float, NDArrayFloat | None, list[NDArrayFloat] | None\n        ] = await image_mask_generation(\n            client=client,\n            source=source,\n            media_dir=settings.media_dir,\n            image=image_row,\n            entity=entity_row,\n            image_embedding=None,\n            high_resolution_features=None,\n            reset_predictor=store_last_image_id != image_row.id,\n            bbox=bbox,\n            points=points,\n            labels=labels,\n        )\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e)) from e\n\n    # store image_id to check against for next generation\n    store_last_image_id = image_row.id\n\n    output: ImageMaskGenerationOutput = ImageMaskGenerationOutput(\n        mask=AnnotationModel.from_row(\n            row=mask_output[0],  # right now we don't use other ouputs (score, ...)\n            table_info=TableInfo(name=mask_table_name, group=\"annotations\", base_schema=\"CompressedRLE\"),\n        )\n    )\n    return output\n</code></pre>"},{"location":"api_reference/app/routers/inference/mask_generation/#pixano.app.routers.inference.mask_generation.call_video_mask_generation","title":"<code>call_video_mask_generation(dataset_id, video, model, settings, bbox=None, points=None, labels=None)</code>  <code>async</code>","text":"<p>Perform video mask generation on a video.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>Annotated[str, Body(embed=True)]</code> <p>The ID of the dataset to use.</p> required <code>video</code> <code>Annotated[list[ViewModel], Body(embed=True)]</code> <p>The video as a list of SequenceFrame (or a subset).</p> required <code>model</code> <code>Annotated[str, Body(embed=True)]</code> <p>The name of the model to use.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>bbox</code> <code>Annotated[BBox | None, Body(embed=True)]</code> <p>Input bounding box or None.</p> <code>None</code> <code>points</code> <code>Annotated[list[list[int]] | None, Body(embed=True)]</code> <p>Input points or None.</p> <code>None</code> <code>labels</code> <code>Annotated[list[int] | None, Body(embed=True)]</code> <p>Labels for input points, or None.</p> <code>None</code> <p>Returns:</p> Type Description <code>VideoMaskGenerationOutput</code> <p>The generated masks.</p> Source code in <code>pixano/app/routers/inference/mask_generation.py</code> <pre><code>@router.post(\n    \"/video\",\n    response_model=VideoMaskGenerationOutput,\n)\nasync def call_video_mask_generation(\n    dataset_id: Annotated[str, Body(embed=True)],\n    video: Annotated[list[ViewModel], Body(embed=True)],\n    model: Annotated[str, Body(embed=True)],\n    settings: Annotated[Settings, Depends(get_settings)],\n    bbox: Annotated[BBox | None, Body(embed=True)] = None,\n    points: Annotated[list[list[int]] | None, Body(embed=True)] = None,\n    labels: Annotated[list[int] | None, Body(embed=True)] = None,\n) -&gt; VideoMaskGenerationOutput:\n    \"\"\"Perform video mask generation on a video.\n\n    Args:\n        dataset_id: The ID of the dataset to use.\n        video: The video as a list of SequenceFrame (or a subset).\n        model: The name of the model to use.\n        settings: App settings.\n        bbox: Input bounding box or None.\n        points: Input points or None.\n        labels: Labels for input points, or None.\n\n    Returns:\n        The generated masks.\n    \"\"\"\n    dataset = get_dataset(dataset_id=dataset_id, dir=settings.library_dir, media_dir=settings.media_dir)\n    client = get_client_from_settings(settings=settings)\n\n    if not is_sequence_frame(dataset.schema.schemas[video[0].table_info.name]):\n        raise HTTPException(status_code=400, detail=\"Video must be a list of SequenceFrame.\")\n\n    video_row: list[SequenceFrame] = [vm.to_row(dataset) for vm in video]\n    source = get_model_source(dataset=dataset, model=model)\n\n    try:\n        mask_output: tuple[list[CompressedRLE], list[int], list[int]] = await video_mask_generation(\n            client=client,\n            source=source,\n            media_dir=settings.media_dir,\n            video=video_row,\n            bbox=bbox,\n            points=points,\n            labels=labels,\n        )\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e)) from e\n\n    output: VideoMaskGenerationOutput = VideoMaskGenerationOutput(\n        masks=[\n            AnnotationModel.from_row(\n                row=m,  # only use mask (don't send obj_ids &amp; frame_idx =&gt; viewRef has the frame_index info)\n                table_info=TableInfo(name=\"mask_table_name\", group=\"annotations\", base_schema=\"CompressedRLE\"),\n            )\n            for m in mask_output[0]\n        ]\n    )\n    return output\n</code></pre>"},{"location":"api_reference/app/routers/inference/models/","title":"models","text":""},{"location":"api_reference/app/routers/inference/models/#pixano.app.routers.inference.models","title":"<code>pixano.app.routers.inference.models</code>","text":""},{"location":"api_reference/app/routers/inference/models/#pixano.app.routers.inference.models.delete_model","title":"<code>delete_model(model_name, settings)</code>  <code>async</code>","text":"<p>Delete a model from pixano inference client.</p> Source code in <code>pixano/app/routers/inference/models.py</code> <pre><code>@router.delete(\"/delete/{model_name}\")\nasync def delete_model(\n    model_name: str,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; None:\n    \"\"\"Delete a model from pixano inference client.\"\"\"\n    client = get_client_from_settings(settings)\n    return await client.delete_model(model_name=model_name)\n</code></pre>"},{"location":"api_reference/app/routers/inference/models/#pixano.app.routers.inference.models.instantiate_model","title":"<code>instantiate_model(config, provider, settings)</code>  <code>async</code>","text":"<p>Instantiate a model from pixano inference client.</p> Source code in <code>pixano/app/routers/inference/models.py</code> <pre><code>@router.post(\"/instantiate\")\nasync def instantiate_model(\n    config: Annotated[ModelConfig, Body(embed=True)],\n    provider: Annotated[str, Body(embed=True)],\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; None:\n    \"\"\"Instantiate a model from pixano inference client.\"\"\"\n    client = get_client_from_settings(settings)\n    return await client.instantiate_model(provider=provider, config=config)\n</code></pre>"},{"location":"api_reference/app/routers/inference/models/#pixano.app.routers.inference.models.list_models","title":"<code>list_models(settings, task=None)</code>  <code>async</code>","text":"<p>List all models from pixano inference client.</p> Source code in <code>pixano/app/routers/inference/models.py</code> <pre><code>@router.get(\"/list\", response_model=list[ModelInfo])\nasync def list_models(\n    settings: Annotated[Settings, Depends(get_settings)], task: str | None = None\n) -&gt; list[ModelInfo]:\n    \"\"\"List all models from pixano inference client.\"\"\"\n    client = get_client_from_settings(settings)\n    models = await client.list_models()\n    if task is None:\n        return models\n    return [m for m in models if task == m.task]\n</code></pre>"},{"location":"api_reference/app/routers/inference/utils/","title":"utils","text":""},{"location":"api_reference/app/routers/inference/utils/#pixano.app.routers.inference.utils","title":"<code>pixano.app.routers.inference.utils</code>","text":""},{"location":"api_reference/app/routers/inference/utils/#pixano.app.routers.inference.utils.get_client_from_settings","title":"<code>get_client_from_settings(settings)</code>","text":"<p>Get the Pixano inference client from the settings.</p> Source code in <code>pixano/app/routers/inference/utils.py</code> <pre><code>def get_client_from_settings(settings: Settings) -&gt; PixanoInferenceClient:\n    \"\"\"Get the Pixano inference client from the settings.\"\"\"\n    client = settings.pixano_inference_client\n    if client is None:\n        raise HTTPException(status_code=500, detail=\"PixanoInferenceClient not set in settings\")\n    return client\n</code></pre>"},{"location":"api_reference/app/routers/inference/utils/#pixano.app.routers.inference.utils.get_model_source","title":"<code>get_model_source(dataset, model)</code>","text":"<p>Get the model's source from a given Dataset and Model.</p> <p>If it exists in the database already it returns that one otherwise creates a new instance.</p> Source code in <code>pixano/app/routers/inference/utils.py</code> <pre><code>def get_model_source(dataset: Dataset, model: str):\n    \"\"\"Get the model's source from a given Dataset and Model.\n\n    If it exists in the database already it returns that one otherwise creates a new instance.\n    \"\"\"\n    sources: list[Source] = dataset.get_data(table_name=\"source\", limit=2, where=f\"name='{model}' AND kind='model'\")\n    # TODO: enforce check consistency for source to have a unique name instead of checking here.\n    if len(sources) &gt; 1:\n        raise HTTPException(status_code=400, detail=\"Only one source for model is allowed.\")\n    elif len(sources) == 0:\n        source = Source(id=shortuuid.uuid(), name=model, kind=\"model\")\n        dataset.add_data(\"source\", data=[source])\n    else:\n        source = sources[0]\n    return source\n</code></pre>"},{"location":"api_reference/app/routers/inference/zero_shot_detection/","title":"zero_shot_detection","text":""},{"location":"api_reference/app/routers/inference/zero_shot_detection/#pixano.app.routers.inference.zero_shot_detection","title":"<code>pixano.app.routers.inference.zero_shot_detection</code>","text":""},{"location":"api_reference/app/routers/inference/zero_shot_detection/#pixano.app.routers.inference.zero_shot_detection.ZeroShotOutput","title":"<code>ZeroShotOutput(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Zero shot output.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/app/routers/inference/zero_shot_detection/#pixano.app.routers.inference.zero_shot_detection.call_image_zero_shot_detection","title":"<code>call_image_zero_shot_detection(dataset_id, image, entity, classes, model, box_table_name, class_table_name, settings, box_threshold=0.3, text_threshold=0.2)</code>  <code>async</code>","text":"<p>Perform zero shot detection on an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>Annotated[str, Body(embed=True)]</code> <p>The ID of the dataset to use.</p> required <code>image</code> <code>Annotated[ViewModel, Body(embed=True)]</code> <p>The image to use for detection.</p> required <code>entity</code> <code>Annotated[EntityModel, Body(embed=True)]</code> <p>The entity to use for detection.</p> required <code>classes</code> <code>Annotated[list[str] | str, Body(embed=True)]</code> <p>Labels to detect.</p> required <code>model</code> <code>Annotated[str, Body(embed=True)]</code> <p>The name of the model to use.</p> required <code>box_table_name</code> <code>Annotated[str, Body(embed=True)]</code> <p>The name of the table to use for boxes in dataset.</p> required <code>class_table_name</code> <code>Annotated[str, Body(embed=True)]</code> <p>The name of the table to use for classifications in dataset.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>App settings.</p> required <code>box_threshold</code> <code>Annotated[float, Body(embed=True)]</code> <p>Box threshold for detection in the image.</p> <code>0.3</code> <code>text_threshold</code> <code>Annotated[float, Body(embed=True)]</code> <p>Text threshold for detection in the image.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>list[ZeroShotOutput]</code> <p>The predicted bboxes and classifications.</p> Source code in <code>pixano/app/routers/inference/zero_shot_detection.py</code> <pre><code>@router.post(\n    \"/image\",\n    response_model=list[ZeroShotOutput],\n)\nasync def call_image_zero_shot_detection(\n    dataset_id: Annotated[str, Body(embed=True)],\n    image: Annotated[ViewModel, Body(embed=True)],\n    entity: Annotated[EntityModel, Body(embed=True)],\n    classes: Annotated[list[str] | str, Body(embed=True)],\n    model: Annotated[str, Body(embed=True)],\n    box_table_name: Annotated[str, Body(embed=True)],\n    class_table_name: Annotated[str, Body(embed=True)],\n    settings: Annotated[Settings, Depends(get_settings)],\n    box_threshold: Annotated[float, Body(embed=True)] = 0.3,\n    text_threshold: Annotated[float, Body(embed=True)] = 0.2,\n) -&gt; list[ZeroShotOutput]:\n    \"\"\"Perform zero shot detection on an image.\n\n    Args:\n        dataset_id: The ID of the dataset to use.\n        image: The image to use for detection.\n        entity: The entity to use for detection.\n        classes: Labels to detect.\n        model: The name of the model to use.\n        box_table_name: The name of the table to use for boxes in dataset.\n        class_table_name: The name of the table to use for classifications in dataset.\n        settings: App settings.\n        box_threshold: Box threshold for detection in the image.\n        text_threshold: Text threshold for detection in the image.\n\n    Returns:\n        The predicted bboxes and classifications.\n    \"\"\"\n    dataset = get_dataset(dataset_id=dataset_id, dir=settings.library_dir, media_dir=settings.media_dir)\n    client = get_client_from_settings(settings=settings)\n\n    if not is_image(dataset.schema.schemas[image.table_info.name]):\n        raise HTTPException(status_code=400, detail=\"Image must be an image.\")\n\n    image_row: Image = image.to_row(dataset)\n    entity_row: Entity = entity.to_row(dataset)\n    source = get_model_source(dataset=dataset, model=model)\n\n    try:\n        bboxes_and_classifications: list[tuple[BBox, Classification]] = await image_zero_shot_detection(\n            client=client,\n            source=source,\n            media_dir=settings.media_dir,\n            image=image_row,\n            entity=entity_row,\n            classes=classes,\n            box_threshold=box_threshold,\n            text_threshold=text_threshold,\n        )\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e)) from e\n\n    output: list[ZeroShotOutput] = []\n    for bbox, classification in bboxes_and_classifications:\n        bbox_model = AnnotationModel.from_row(\n            row=bbox, table_info=TableInfo(name=box_table_name, group=\"annotations\", base_schema=\"BBox\")\n        )\n        classification_model = AnnotationModel.from_row(\n            row=classification,\n            table_info=TableInfo(name=class_table_name, group=\"annotations\", base_schema=\"Classification\"),\n        )\n        output.append(ZeroShotOutput(bbox=bbox_model, classification=classification_model))\n    return output\n</code></pre>"},{"location":"api_reference/datasets/dataset/","title":"dataset","text":""},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset","title":"<code>pixano.datasets.dataset</code>","text":""},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset","title":"<code>Dataset(path, media_dir=None)</code>","text":"<p>The Pixano Dataset.</p> <p>It is a collection of tables that can be queried and manipulated with LanceDB.</p> <p>The tables are defined by the DatasetSchema which allows the dataset to return the data in the form of LanceModel instances.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>Path</code> <p>Path to the dataset.</p> <code>info</code> <code>DatasetInfo</code> <p>Dataset info.</p> <code>schema</code> <code>DatasetSchema</code> <p>Dataset schema.</p> <code>features_values</code> <code>DatasetFeaturesValues</code> <p>Dataset features values.</p> <code>stats</code> <code>list[DatasetStatistic]</code> <p>Dataset statistics.</p> <code>thumbnail</code> <code>Path</code> <p>Dataset thumbnail base 64 URL.</p> <code>media_dir</code> <code>Path</code> <p>Path to the media directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the dataset.</p> required <code>media_dir</code> <code>Path | None</code> <p>Path to the media directory.</p> <code>None</code> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def __init__(self, path: Path, media_dir: Path | None = None):\n    \"\"\"Initialize the dataset.\n\n    Args:\n        path: Path to the dataset.\n        media_dir: Path to the media directory.\n    \"\"\"\n    self.path = path\n\n    self._info_file = self.path / self._INFO_FILE\n    self._schema_file = self.path / self._SCHEMA_FILE\n    self._features_values_file = self.path / self._FEATURES_VALUES_FILE\n    self._stat_file = self.path / self._STAT_FILE\n    self._thumb_file = self.path / self._THUMB_FILE\n    self._db_path = self.path / self._DB_PATH\n\n    self.info = DatasetInfo.from_json(self._info_file)\n    self.features_values = DatasetFeaturesValues.from_json(self._features_values_file)\n    self.stats = DatasetStatistic.from_json(self._stat_file) if self._stat_file.is_file() else []\n    self.media_dir = media_dir or self.path / \"media\"\n    self.thumbnail = self._thumb_file\n    self.previews_path = self.path / self._PREVIEWS_PATH\n\n    self._db_connection = self._connect()\n\n    self._reload_schema()\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.id","title":"<code>id</code>  <code>property</code>","text":"<p>Return the dataset ID.</p>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.num_rows","title":"<code>num_rows</code>  <code>property</code>","text":"<p>Return the number of rows in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of rows.</p>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.add_constraint","title":"<code>add_constraint(table, field_name, values, restricted=True)</code>","text":"<p>Add or replace a constraint.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>TableName</code> <p>Table name (as in DatasetItem schema)</p> required <code>field_name</code> <code>str</code> <p>Name of the field to constrain.</p> required <code>values</code> <code>List[Union[int, float, str, bool]]</code> <p>List of allowed values.</p> required <code>restricted</code> <code>bool</code> <p>True if no other values are allowed.</p> <code>True</code> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def add_constraint(\n    self,\n    table: TableName,\n    field_name: str,\n    values: List[Union[int, float, str, bool]],\n    restricted: bool = True,\n):\n    \"\"\"Add or replace a constraint.\n\n    Args:\n        table: Table name (as in DatasetItem schema)\n        field_name: Name of the field to constrain.\n        values: List of allowed values.\n        restricted: True if no other values are allowed.\n    \"\"\"\n    # get kind (\"item\", \"views\", \"entities\", \"annotations\") from schema (table name are unique)\n    kinds = [group.value for group, tables in self.schema.groups.items() if table in tables]\n    if len(kinds) != 1:\n        raise ValueError(f\"Table {table} do not exist in schema\")\n\n    constraint_dict: ConstraintDict = getattr(self.features_values, kinds[0])\n\n    # Ensure the list exists for the given table\n    if table not in constraint_dict:\n        constraint_dict[table] = []\n\n    # Check if the field already has a constraint \u2192 update it if so\n    for constraint in constraint_dict[table]:\n        if constraint.name == field_name:\n            constraint.restricted = restricted\n            constraint.values = values\n            return\n\n    # Otherwise, add a new constraint\n    constraint_dict[table].append(Constraint(name=field_name, restricted=restricted, values=values))\n\n    # Save json\n    self.features_values.to_json(self._features_values_file)\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.add_data","title":"<code>add_data(table_name, data, ignore_integrity_checks=None, raise_or_warn='raise')</code>","text":"<p>Add data to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name.</p> required <code>data</code> <code>list[BaseSchema]</code> <p>Data to add.</p> required <code>ignore_integrity_checks</code> <code>list[IntegrityCheck] | None</code> <p>List of integrity checks to ignore.</p> <code>None</code> <code>raise_or_warn</code> <code>Literal['raise', 'warn', 'none']</code> <p>Whether to raise or warn on integrity errors. Can be 'raise', 'warn' or 'none'.</p> <code>'raise'</code> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def add_data(\n    self,\n    table_name: str,\n    data: list[BaseSchema],\n    ignore_integrity_checks: list[IntegrityCheck] | None = None,\n    raise_or_warn: Literal[\"raise\", \"warn\", \"none\"] = \"raise\",\n) -&gt; list[BaseSchema]:\n    \"\"\"Add data to a table.\n\n    Args:\n        table_name: Table name.\n        data: Data to add.\n        ignore_integrity_checks: List of integrity checks to ignore.\n        raise_or_warn: Whether to raise or warn on integrity errors. Can be 'raise', 'warn' or 'none'.\n    \"\"\"\n    if not all((isinstance(item, type(data[0])) for item in data)) or not set(\n        type(data[0]).model_fields.keys()\n    ) == set(\n        self.schema.schemas[table_name].model_fields.keys()\n        if table_name != SchemaGroup.SOURCE.value\n        else Source.model_fields.keys()\n    ):\n        raise DatasetAccessError(\n            \"All data must be instances of the table type \"\n            f\"{self.schema.schemas[table_name] if table_name != SchemaGroup.SOURCE.value else Source}.\"\n        )\n    _validate_raise_or_warn(raise_or_warn)\n\n    table = self.open_table(table_name)\n    if raise_or_warn != \"none\":\n        handle_integrity_errors(\n            check_table_integrity(table_name, self, data, False, ignore_integrity_checks), raise_or_warn\n        )\n    for d in data:\n        d.created_at = datetime.now()\n        d.updated_at = d.created_at\n    table.add(data)\n\n    return data\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.add_dataset_items","title":"<code>add_dataset_items(dataset_items)</code>","text":"<pre><code>add_dataset_items(dataset_items: DatasetItem) -&gt; DatasetItem\n</code></pre><pre><code>add_dataset_items(dataset_items: list[DatasetItem]) -&gt; list[DatasetItem]\n</code></pre> <p>Add dataset items to the dataset.</p> Warn <p>Does not test for integrity of the data.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_items</code> <code>list[DatasetItem] | DatasetItem</code> <p>Dataset items to add.</p> required Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def add_dataset_items(self, dataset_items: list[DatasetItem] | DatasetItem) -&gt; list[DatasetItem] | DatasetItem:\n    \"\"\"Add dataset items to the dataset.\n\n    Warn:\n        Does not test for integrity of the data.\n\n    Args:\n        dataset_items: Dataset items to add.\n    \"\"\"\n    batch = True\n    if isinstance(dataset_items, DatasetItem):\n        dataset_items = [dataset_items]\n        batch = False\n    fields = self.dataset_item_model.model_fields.keys()\n    if not all(\n        isinstance(item, DatasetItem) and set(fields) == set(item.model_fields.keys()) for item in dataset_items\n    ):\n        raise DatasetAccessError(\"All data must be instances of the same DatasetItem.\")\n\n    schemas_data = [item.to_schemas_data(self.schema) for item in dataset_items]\n    tables_data: dict[str, Any] = {}\n    for table_name in self.schema.schemas.keys():\n        for item in schemas_data:\n            if table_name not in tables_data:\n                tables_data[table_name] = []\n            if table_name not in item:\n                continue\n            if isinstance(item[table_name], list):\n                tables_data[table_name].extend(item[table_name])\n            elif item[table_name] is not None:\n                tables_data[table_name].append(item[table_name])\n    for table_name, table_data in tables_data.items():\n        if table_data != []:\n            self.add_data(\n                table_name=table_name,\n                data=table_data,\n                ignore_integrity_checks=[],\n                raise_or_warn=\"none\",\n            )\n    return dataset_items if batch else dataset_items[0]\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.compute_view_embeddings","title":"<code>compute_view_embeddings(table_name, data)</code>","text":"<p>Compute the view embeddings via the     Embedding Function stored in the table metadata.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name containing the view embeddings.</p> required <code>data</code> <code>list[dict]</code> <p>Data to compute. Dictionary representing a view embedding without the vector field.</p> required Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def compute_view_embeddings(self, table_name: str, data: list[dict]) -&gt; None:\n    \"\"\"Compute the [view embeddings][pixano.features.ViewEmbedding] via the\n        [Embedding Function][lancedb.embeddings.base.EmbeddingFunction] stored in the table metadata.\n\n    Args:\n        table_name: Table name containing the view embeddings.\n        data: Data to compute. Dictionary representing a view embedding without the vector field.\n    \"\"\"\n    table_schema = self.schema.schemas[table_name]\n    if not issubclass(table_schema, ViewEmbedding):\n        raise DatasetAccessError(f\"Table {table_name} is not a view embedding table\")\n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise DatasetAccessError(\"Data must be a list of dictionaries\")\n    # TODO: improve how to handle shape, this works but feels hacky\n    for item in data:\n        if \"shape\" not in item:\n            item[\"shape\"] = []\n    table = self.open_table(table_name)\n    data = pa.Table.from_pylist(\n        data, schema=table_schema.to_arrow_schema(remove_vector=True, remove_metadata=True)\n    )\n    table.add(data)\n    return None\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.create_table","title":"<code>create_table(name, schema, relation_item, data=None, mode='create', exist_ok=False, on_bad_vectors='error', fill_value=0.0)</code>","text":"<p>Add a table to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name.</p> required <code>schema</code> <code>type[BaseSchema]</code> <p>Table schema.</p> required <code>relation_item</code> <code>SchemaRelation</code> <p>Relation with the <code>'item'</code> table (table to item).</p> required <code>data</code> <code>DATA | None</code> <p>Table data.</p> <code>None</code> <code>mode</code> <code>str</code> <p>Table mode ('create', 'overwrite').</p> <code>'create'</code> <code>exist_ok</code> <code>bool</code> <p>If True, do not raise an error if the table already exists.</p> <code>False</code> <code>on_bad_vectors</code> <code>str</code> <p>Raise an error, drop or fill bad vectors (\"error\", \"drop\", \"fill\").</p> <code>'error'</code> <code>fill_value</code> <code>float</code> <p>Value to fill bad vectors.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>LanceTable</code> <p>The table created.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def create_table(\n    self,\n    name: str,\n    schema: type[BaseSchema],\n    relation_item: SchemaRelation,\n    data: DATA | None = None,\n    mode: str = \"create\",\n    exist_ok: bool = False,\n    on_bad_vectors: str = \"error\",\n    fill_value: float = 0.0,\n) -&gt; LanceTable:\n    \"\"\"Add a table to the dataset.\n\n    Args:\n        name: Table name.\n        schema: Table schema.\n        relation_item: Relation with the `'item'` table (table to item).\n        data: Table data.\n        mode: Table mode ('create', 'overwrite').\n        exist_ok: If True, do not raise an error if the table already exists.\n        on_bad_vectors: Raise an error, drop or fill bad vectors (\"error\", \"drop\", \"fill\").\n        fill_value: Value to fill bad vectors.\n\n    Returns:\n        The table created.\n    \"\"\"\n    table = self._db_connection.create_table(\n        name=name,\n        schema=schema,\n        data=data,\n        mode=mode,\n        exist_ok=exist_ok,\n        on_bad_vectors=on_bad_vectors,\n        fill_value=fill_value,\n        embedding_functions=None,\n    )\n    self.schema.add_schema(name, schema, relation_item, exist_ok or mode == \"overwrite\")\n    self.schema.to_json(self._schema_file)\n    self._reload_schema()\n    return table\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.delete_data","title":"<code>delete_data(table_name, ids)</code>","text":"<p>Delete data from a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name.</p> required <code>ids</code> <code>list[str]</code> <p>Ids to delete.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>The list of ids not found.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def delete_data(self, table_name: str, ids: list[str]) -&gt; list[str]:\n    \"\"\"Delete data from a table.\n\n    Args:\n        table_name: Table name.\n        ids: Ids to delete.\n\n    Returns:\n        The list of ids not found.\n    \"\"\"\n    if not isinstance(ids, list) or not all(isinstance(i, str) for i in ids):\n        raise DatasetAccessError(\"ids must be a list of strings\")\n\n    set_ids = set(ids)\n\n    table = self.open_table(table_name)\n    sql_ids = to_sql_list(set_ids)\n\n    ids_found = {\n        row[\"id\"] for row in TableQueryBuilder(table).select([\"id\"]).where(f\"id in {to_sql_list(ids)}\").to_list()\n    }\n    ids_not_found = [id for id in set_ids if id not in ids_found]\n\n    table.delete(where=f\"id in {sql_ids}\")\n\n    return ids_not_found\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.delete_dataset_items","title":"<code>delete_dataset_items(ids)</code>","text":"<p>Delete dataset items.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[str]</code> <p>Ids to delete.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>The list of ids not found.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def delete_dataset_items(self, ids: list[str]) -&gt; list[str]:\n    \"\"\"Delete dataset items.\n\n    Args:\n        ids: Ids to delete.\n\n    Returns:\n        The list of ids not found.\n    \"\"\"\n    sql_ids = to_sql_list(ids)\n\n    ids_not_found = []\n    for table_name in self.schema.schemas.keys():\n        if table_name == SchemaGroup.ITEM.value:\n            ids_not_found = self.delete_data(table_name, ids)\n        else:\n            table = self.open_table(table_name)\n            table_ids = (\n                table.search()\n                .select([\"id\"])\n                .where(f\"item_ref.id in {sql_ids}\")\n                .limit(None)\n                .to_arrow()[\"id\"]\n                .to_pylist()\n            )\n            if table_ids == []:\n                continue\n            table_sql_ids = to_sql_list(table_ids)\n            table.delete(where=f\"id in {table_sql_ids}\")\n    return ids_not_found\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.find","title":"<code>find(id, directory, media_dir=None)</code>  <code>staticmethod</code>","text":"<p>Find a Dataset in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Dataset ID to find.</p> required <code>directory</code> <code>Path</code> <p>Directory to search in.</p> required <code>media_dir</code> <code>Path | None</code> <p>Media directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>'Dataset'</code> <p>The found dataset.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>@staticmethod\ndef find(\n    id: str,\n    directory: Path,\n    media_dir: Path | None = None,\n) -&gt; \"Dataset\":\n    \"\"\"Find a Dataset in a directory.\n\n    Args:\n        id: Dataset ID to find.\n        directory: Directory to search in.\n        media_dir: Media directory.\n\n    Returns:\n        The found dataset.\n    \"\"\"\n    # Browse directory\n    for json_fp in directory.glob(\"*/info.json\"):\n        info = DatasetInfo.from_json(json_fp)\n        if info.id == id:\n            # Return dataset\n            return Dataset(json_fp.parent, media_dir)\n    raise FileNotFoundError(f\"Dataset {id} not found in {directory}\")\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.find_ids_in_table","title":"<code>find_ids_in_table(table_name, ids)</code>","text":"<p>Search ids in a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name.</p> required <code>ids</code> <code>set[str]</code> <p>Ids to find.</p> required <p>Returns:</p> Type Description <code>dict[str, bool]</code> <p>Dictionary of ids found. Keys are the ids and values are <code>True</code> if the id is found, <code>False</code> otherwise.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def find_ids_in_table(self, table_name: str, ids: set[str]) -&gt; dict[str, bool]:\n    \"\"\"Search ids in a table.\n\n    Args:\n        table_name: Table name.\n        ids: Ids to find.\n\n    Returns:\n        Dictionary of ids found. Keys are the ids and values are `True` if the id is found, `False` otherwise.\n    \"\"\"\n    if len(ids) == 0:\n        return {}\n    table = self.open_table(table_name)\n    ids_found = list(TableQueryBuilder(table).select([\"id\"]).where(f\"id in {to_sql_list(ids)}\").to_polars()[\"id\"])\n    return {id: id in ids_found for id in ids}\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.get_all_ids","title":"<code>get_all_ids(table_name=SchemaGroup.ITEM.value, sortcol=None, order=None)</code>","text":"<p>Get all the ids from a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>table to look for ids.</p> <code>ITEM.value</code> <code>sortcol</code> <code>str | None</code> <p>column to order by</p> <code>None</code> <code>order</code> <code>str | None</code> <p>sort order (asc or desc)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list of the ids.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def get_all_ids(\n    self,\n    table_name: str = SchemaGroup.ITEM.value,\n    sortcol: str | None = None,\n    order: str | None = None,\n) -&gt; list[str]:\n    \"\"\"Get all the ids from a table.\n\n    Args:\n        table_name: table to look for ids.\n        sortcol: column to order by\n        order: sort order (asc or desc)\n\n    Returns:\n        list of the ids.\n    \"\"\"\n    query = TableQueryBuilder(self.open_table(table_name)).select([\"id\"])\n    if sortcol is not None and order is not None:\n        query = query.order_by(order_by=sortcol, descending=order == \"desc\")\n    return [row[\"id\"] for row in query.to_list()]\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.get_data","title":"<code>get_data(table_name, ids=None, limit=None, skip=0, where=None, item_ids=None, sortcol=None, order=None)</code>","text":"<pre><code>get_data(table_name: str, ids: list[str] | None = None, limit: int | None = None, skip: int = 0, where: str | None = None, item_ids: list[str] | None = None, sortcol: str | None = None, order: str | None = None) -&gt; list[BaseSchema]\n</code></pre><pre><code>get_data(table_name: str, ids: str, limit: int | None = None, skip: int = 0, where: str | None = None, item_ids: None = None, sortcol: str | None = None, order: str | None = None) -&gt; BaseSchema | None\n</code></pre> <p>Read data from a table.</p> <p>Data can be filtered by ids, item ids, where clause, or limit and skip.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name.</p> required <code>where</code> <code>str | None</code> <p>Where clause.</p> <code>None</code> <code>ids</code> <code>list[str] | str | None</code> <p>ids to read.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Amount of items to read. If not set, will default to table size.</p> <code>None</code> <code>skip</code> <code>int</code> <p>The number of data to skip.</p> <code>0</code> <code>item_ids</code> <code>list[str] | None</code> <p>Item ids to read.</p> <code>None</code> <code>sortcol</code> <code>str | None</code> <p>column to order by</p> <code>None</code> <code>order</code> <code>str | None</code> <p>sort order (asc or desc)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[BaseSchema] | BaseSchema | None</code> <p>List of values.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def get_data(\n    self,\n    table_name: str,\n    ids: list[str] | str | None = None,\n    limit: int | None = None,\n    skip: int = 0,\n    where: str | None = None,\n    item_ids: list[str] | None = None,\n    sortcol: str | None = None,\n    order: str | None = None,\n) -&gt; list[BaseSchema] | BaseSchema | None:\n    \"\"\"Read data from a table.\n\n    Data can be filtered by ids, item ids, where clause, or limit and skip.\n\n    Args:\n        table_name: Table name.\n        where: Where clause.\n        ids: ids to read.\n        limit: Amount of items to read. If not set, will default to table size.\n        skip: The number of data to skip.\n        item_ids: Item ids to read.\n        sortcol: column to order by\n        order: sort order (asc or desc)\n\n    Returns:\n        List of values.\n    \"\"\"\n    if table_name == SchemaGroup.ITEM.value:\n        if item_ids is not None:\n            if ids is None:\n                ids = item_ids\n            else:\n                raise DatasetAccessError(\"ids and item_ids cannot be set at the same time\")\n            item_ids = None\n\n    return_list = not isinstance(ids, str)\n    ids = [ids] if isinstance(ids, str) else ids\n\n    _validate_ids_item_ids_and_limit_and_skip(ids, limit, skip, item_ids)\n\n    if item_ids is not None:\n        sql_item_ids = to_sql_list(item_ids)\n    table = self.open_table(table_name)\n\n    if ids is None and item_ids is None and limit is None:\n        limit = table.count_rows()\n\n    if ids is None:\n        if item_ids is None:\n            if where is not None:\n                query = TableQueryBuilder(table).where(where).limit(limit).offset(skip)\n            else:\n                query = TableQueryBuilder(table).limit(limit).offset(skip)\n        else:\n            sql_item_ids = to_sql_list(item_ids)\n            if where is not None:\n                where += f\" AND item_ref.id IN {sql_item_ids}\"\n            else:\n                where = f\"item_ref.id IN {sql_item_ids}\"\n            query = TableQueryBuilder(table).where(where).limit(limit).offset(skip)\n        if sortcol is not None and order is not None:\n            query = query.order_by(sortcol, order == \"desc\")\n    else:\n        sql_ids = to_sql_list(ids)\n        if where is not None:\n            where += f\" AND id IN {sql_ids}\"\n        else:\n            where = f\"id IN {sql_ids}\"\n        query = TableQueryBuilder(table).where(where)\n\n    schema = self.schema.schemas[table_name] if table_name != SchemaGroup.SOURCE.value else Source\n\n    query_models: list[BaseSchema] = query.to_pydantic(schema)\n    for model in query_models:\n        model.dataset = self  # type: ignore[attr-defined]\n        model.table_name = table_name\n\n    return query_models if return_list else (query_models[0] if query_models != [] else None)\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.get_dataset_items","title":"<code>get_dataset_items(ids=None, limit=None, skip=0)</code>","text":"<pre><code>get_dataset_items(ids: list[str] | None = None, limit: int | None = None, skip: int = 0) -&gt; list[DatasetItem]\n</code></pre><pre><code>get_dataset_items(ids: str, limit: int | None = None, skip: int = 0) -&gt; DatasetItem | None\n</code></pre> <p>Read dataset items.</p> <p>Filter dataset items by ids, or limit and skip.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[str] | str | None</code> <p>Item ids to read.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Amount of items to read.</p> <code>None</code> <code>skip</code> <code>int</code> <p>The number of data to skip..</p> <code>0</code> <p>Returns:</p> Type Description <code>list[DatasetItem] | DatasetItem | None</code> <p>List of dataset items.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def get_dataset_items(\n    self,\n    ids: list[str] | str | None = None,\n    limit: int | None = None,\n    skip: int = 0,\n) -&gt; list[DatasetItem] | DatasetItem | None:\n    \"\"\"Read dataset items.\n\n    Filter dataset items by ids, or limit and skip.\n\n    Args:\n        ids: Item ids to read.\n        limit: Amount of items to read.\n        skip: The number of data to skip..\n\n    Returns:\n        List of dataset items.\n    \"\"\"\n    return_list = not isinstance(ids, str)\n    ids = [ids] if isinstance(ids, str) else ids\n\n    _validate_ids_and_limit_and_skip(ids, limit, skip)\n\n    items = self.get_data(table_name=SchemaGroup.ITEM.value, where=None, ids=ids, limit=limit, skip=skip)\n    if items == []:\n        return [] if return_list else None\n    item_ids: list[str] = [item.id for item in items]\n    sql_ids = to_sql_list(item_ids)\n\n    # Load tables\n    ds_tables = self.open_tables(exclude_embeddings=True)\n\n    # Load items data from the tables\n    data_dict: dict[str, dict[str, BaseSchema | list[BaseSchema]]] = {item.id: item.model_dump() for item in items}\n    for table_name, table in ds_tables.items():\n        if table_name == SchemaGroup.ITEM.value:\n            continue\n        is_collection = self.schema.relations[SchemaGroup.ITEM.value][table_name] == SchemaRelation.ONE_TO_MANY\n        table_schema = self.schema.schemas[table_name]\n\n        rows = TableQueryBuilder(table).where(f\"item_ref.id in {sql_ids}\").to_pydantic(table_schema)\n\n        for row in rows:\n            row.dataset = self\n            row.table_name = table_name\n            item_id = row.item_ref.id\n            if is_collection:\n                if table_name not in data_dict[item_id]:\n                    data_dict[item_id][table_name] = []\n                data_dict[item_id][table_name].append(row)\n            else:\n                data_dict[item_id][table_name] = row\n\n    dataset_items = [self.dataset_item_model(**data_dict[item_id]) for item_id in item_ids]  # type: ignore[arg-type]\n\n    return dataset_items if return_list else (dataset_items[0] if dataset_items != [] else None)\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.list","title":"<code>list(directory)</code>  <code>staticmethod</code>","text":"<p>List the datasets information in directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Directory to search in.</p> required <p>Returns:</p> Type Description <code>list[DatasetInfo]</code> <p>List of dataset infos.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>@staticmethod\ndef list(directory: Path) -&gt; list[DatasetInfo]:\n    \"\"\"List the datasets information in directory.\n\n    Args:\n        directory: Directory to search in.\n\n    Returns:\n        List of dataset infos.\n    \"\"\"\n    dataset_infos = []\n    for json_fp in directory.glob(\"*/info.json\"):\n        dataset_infos.append(DatasetInfo.from_json(json_fp))\n    return dataset_infos\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.open_table","title":"<code>open_table(name)</code>","text":"<p>Open a dataset table with LanceDB.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table to open.</p> required <p>Returns:</p> Type Description <code>LanceTable</code> <p>Dataset table.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def open_table(self, name: str) -&gt; LanceTable:\n    \"\"\"Open a dataset table with LanceDB.\n\n    Args:\n        name: Name of the table to open.\n\n    Returns:\n        Dataset table.\n    \"\"\"\n    if name not in self.schema.schemas.keys() and name != SchemaGroup.SOURCE.value:\n        raise DatasetAccessError(f\"Table {name} not found in dataset\")\n\n    table = self._db_connection.open_table(name)\n    if name == SchemaGroup.SOURCE.value:\n        return table\n\n    schema_table = self.schema.schemas[name]\n    if is_view_embedding(schema_table):\n        schema_table = cast(type[ViewEmbedding], schema_table)\n        try:\n            schema_table.get_embedding_fn_from_table(self, name, table.schema.metadata)\n        except TypeError:  # no embedding function\n            pass\n    return table\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.open_tables","title":"<code>open_tables(names=None, exclude_embeddings=True)</code>","text":"<p>Open the dataset tables with LanceDB.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>list[str] | None</code> <p>Table names to open. If None, open all tables.</p> <code>None</code> <code>exclude_embeddings</code> <code>bool</code> <p>Whether to exclude embedding tables from the list.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, LanceTable]</code> <p>Dataset tables.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def open_tables(self, names: list[str] | None = None, exclude_embeddings: bool = True) -&gt; dict[str, LanceTable]:\n    \"\"\"Open the dataset tables with LanceDB.\n\n    Args:\n        names: Table names to open. If None, open all tables.\n        exclude_embeddings: Whether to exclude embedding tables from the list.\n\n    Returns:\n        Dataset tables.\n    \"\"\"\n    tables: dict[str, LanceTable] = defaultdict(dict)\n\n    for name in names if names is not None else self.schema.schemas.keys():\n        if exclude_embeddings and name in self.schema.groups[SchemaGroup.EMBEDDING]:\n            continue\n        tables[name] = self.open_table(name)\n\n    return tables\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.resolve_ref","title":"<code>resolve_ref(ref)</code>","text":"<pre><code>resolve_ref(ref: ItemRef) -&gt; Item\n</code></pre><pre><code>resolve_ref(ref: ViewRef) -&gt; View\n</code></pre><pre><code>resolve_ref(ref: EmbeddingRef) -&gt; Embedding\n</code></pre><pre><code>resolve_ref(ref: EntityRef) -&gt; Entity\n</code></pre><pre><code>resolve_ref(ref: AnnotationRef) -&gt; Annotation\n</code></pre><pre><code>resolve_ref(ref: SourceRef) -&gt; Source\n</code></pre><pre><code>resolve_ref(ref: SchemaRef) -&gt; BaseSchema\n</code></pre> <p>Resolve a SchemaRef.</p> <p>It fetches the data from the table referenced.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>SchemaRef | ItemRef | ViewRef | EmbeddingRef | EntityRef | AnnotationRef | SourceRef</code> <p>Reference to resolve.</p> required <p>Returns:</p> Type Description <code>BaseSchema | Item | View | Embedding | Entity | Annotation | Source</code> <p>The resolved reference.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def resolve_ref(\n    self, ref: SchemaRef | ItemRef | ViewRef | EmbeddingRef | EntityRef | AnnotationRef | SourceRef\n) -&gt; BaseSchema | Item | View | Embedding | Entity | Annotation | Source:\n    \"\"\"Resolve a [SchemaRef][pixano.features.SchemaRef].\n\n    It fetches the data from the table referenced.\n\n    Args:\n        ref: Reference to resolve.\n\n    Returns:\n        The resolved reference.\n    \"\"\"\n    if ref.id == \"\" or ref.name == \"\":\n        raise DatasetAccessError(\"Reference should have a name and an id.\")\n    return self.get_data(ref.name, ids=[ref.id])[0]\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.semantic_search","title":"<code>semantic_search(query, table_name, limit, skip=0)</code>","text":"<p>Perform a semantic search.</p> <p>It searches for the closest items to the query in the table embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Text query for semantic search.</p> required <code>table_name</code> <code>str</code> <p>Table name for embeddings.</p> required <code>limit</code> <code>int</code> <p>Limit number of items.</p> required <code>skip</code> <code>int</code> <p>Skip number of items</p> <code>0</code> <p>Returns:</p> Type Description <code>tuple[list[BaseSchema], list[float], list[str]]</code> <p>Tuple of items, distances, and full sorted list of ids.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def semantic_search(\n    self, query: str, table_name: str, limit: int, skip: int = 0\n) -&gt; tuple[list[BaseSchema], list[float], list[str]]:\n    \"\"\"Perform a semantic search.\n\n    It searches for the closest items to the query in the table embeddings.\n\n    Args:\n        query: Text query for semantic search.\n        table_name: Table name for embeddings.\n        limit: Limit number of items.\n        skip: Skip number of items\n\n    Returns:\n        Tuple of items, distances, and full sorted list of ids.\n    \"\"\"\n    if not isinstance(query, str):\n        raise DatasetAccessError(\"query must be a string.\")\n    elif not isinstance(table_name, str):\n        raise DatasetAccessError(\"table_name must be a string.\")\n    elif not isinstance(limit, int) or limit &lt; 1:\n        raise DatasetAccessError(\"limit must be a strictly positive integer.\")\n    elif not isinstance(skip, int) or skip &lt; 0:\n        raise DatasetAccessError(\"skip must be a positive integer.\")\n    elif table_name not in self.schema.schemas:\n        raise DatasetAccessError(f\"Table {table_name} not found in dataset {self.id}.\")\n    elif table_name not in self.schema.groups[SchemaGroup.EMBEDDING] or not is_view_embedding(\n        self.schema.schemas[table_name]\n    ):\n        raise DatasetAccessError(f\"Table {table_name} is not a view embedding table.\")\n\n    table = self.open_table(table_name)\n    semantic_results: pl.DataFrame = (\n        table.search(query).select([\"item_ref.id\"]).limit(1e9).to_polars()\n    )  # TODO: change high limit if lancedb supports it\n    item_results = semantic_results.group_by(\"item_ref.id\").agg(pl.min(\"_distance\")).sort(\"_distance\")\n    full_item_ids = item_results[\"item_ref.id\"].to_list()\n    item_ids = full_item_ids[skip : skip + limit]\n\n    item_rows = self.get_data(\"item\", ids=item_ids)\n    item_rows = sorted(item_rows, key=lambda x: item_ids.index(x.id))\n    distances = [\n        item_results.row(by_predicate=(pl.col(\"item_ref.id\") == item.id), named=True)[\"_distance\"]\n        for item in item_rows\n    ]\n    return item_rows, distances, full_item_ids\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.update_data","title":"<code>update_data(table_name, data, return_separately=False, ignore_integrity_checks=None, raise_or_warn='raise')</code>","text":"<pre><code>update_data(table_name: str, data: list[BaseSchema], return_separately: Literal[False] = False, ignore_integrity_checks: list[IntegrityCheck] | None = None, raise_or_warn: Literal['raise', 'warn', 'none'] = 'raise') -&gt; list[BaseSchema]\n</code></pre><pre><code>update_data(table_name: str, data: list[BaseSchema], return_separately: Literal[True], ignore_integrity_checks: list[IntegrityCheck] | None = None, raise_or_warn: Literal['raise', 'warn', 'none'] = 'raise') -&gt; tuple[list[BaseSchema], list[BaseSchema]]\n</code></pre> <p>Update data in a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name.</p> required <code>data</code> <code>list[BaseSchema]</code> <p>Data to update.</p> required <code>return_separately</code> <code>bool</code> <p>Whether to return separately added and updated data.</p> <code>False</code> <code>ignore_integrity_checks</code> <code>list[IntegrityCheck] | None</code> <p>List of integrity checks to ignore.</p> <code>None</code> <code>raise_or_warn</code> <code>Literal['raise', 'warn', 'none']</code> <p>Whether to raise or warn on integrity errors. Can be 'raise', 'warn' or 'none'.</p> <code>'raise'</code> <p>Returns:</p> Type Description <code>list[BaseSchema] | tuple[list[BaseSchema], list[BaseSchema]]</code> <p>If <code>return_separately</code> is <code>True</code>, returns a tuple of updated and added data. Otherwise, returns the updated</p> <code>list[BaseSchema] | tuple[list[BaseSchema], list[BaseSchema]]</code> <p>data.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def update_data(\n    self,\n    table_name: str,\n    data: list[BaseSchema],\n    return_separately: bool = False,\n    ignore_integrity_checks: list[IntegrityCheck] | None = None,\n    raise_or_warn: Literal[\"raise\", \"warn\", \"none\"] = \"raise\",\n) -&gt; list[BaseSchema] | tuple[list[BaseSchema], list[BaseSchema]]:\n    \"\"\"Update data in a table.\n\n    Args:\n        table_name: Table name.\n        data: Data to update.\n        return_separately: Whether to return separately added and updated data.\n        ignore_integrity_checks: List of integrity checks to ignore.\n        raise_or_warn: Whether to raise or warn on integrity errors. Can be 'raise', 'warn' or 'none'.\n\n    Returns:\n        If `return_separately` is `True`, returns a tuple of updated and added data. Otherwise, returns the updated\n        data.\n    \"\"\"\n    if not all((isinstance(item, type(data[0])) for item in data)) or not set(\n        type(data[0]).model_fields.keys()\n    ) == set(\n        self.schema.schemas[table_name].model_fields.keys()\n        if table_name != SchemaGroup.SOURCE.value\n        else Source.model_fields.keys()\n    ):\n        raise DatasetAccessError(\n            \"All data must be instances of the table type \"\n            f\"{self.schema.schemas[table_name] if table_name != SchemaGroup.SOURCE.value else Source}.\"\n        )\n    _validate_raise_or_warn(raise_or_warn)\n\n    table = self.open_table(table_name)\n    if raise_or_warn != \"none\":\n        handle_integrity_errors(\n            check_table_integrity(table_name, self, data, True, ignore_integrity_checks), raise_or_warn\n        )\n    set_ids = {item.id for item in data}\n    ids_found: dict[str, datetime] = {}\n\n    ids_found = {\n        row[\"id\"]: row[\"created_at\"]\n        for row in TableQueryBuilder(table)\n        .select([\"id\", \"created_at\"])\n        .where(f\"id in {to_sql_list(set_ids)}\")\n        .to_list()\n    }\n\n    for d in data:\n        d.updated_at = datetime.now()\n        if d.id not in ids_found:\n            d.created_at = d.updated_at\n    table.merge_insert(\"id\").when_matched_update_all().when_not_matched_insert_all().execute(data)\n\n    if not return_separately:\n        return data\n\n    updated_data, added_data = [], []\n    for d in data:\n        if d.id not in ids_found:\n            added_data.append(d)\n        else:\n            updated_data.append(d)\n\n    return updated_data, added_data\n</code></pre>"},{"location":"api_reference/datasets/dataset/#pixano.datasets.dataset.Dataset.update_dataset_items","title":"<code>update_dataset_items(dataset_items, return_separately=False)</code>","text":"<pre><code>update_dataset_items(dataset_items: list[DatasetItem], return_separately: Literal[False] = False) -&gt; list[DatasetItem]\n</code></pre><pre><code>update_dataset_items(dataset_items: list[DatasetItem], return_separately: Literal[True]) -&gt; tuple[list[DatasetItem], list[DatasetItem]]\n</code></pre> <p>Update dataset items.</p> Warn <p>Does not test for integrity of the data.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_items</code> <code>list[DatasetItem]</code> <p>Dataset items to update.</p> required <code>return_separately</code> <code>bool</code> <p>Whether to return separately added and updated dataset items.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[DatasetItem] | tuple[list[DatasetItem], list[DatasetItem]]</code> <p>If <code>return_separately</code> is <code>True</code>, returns a tuple of updated and added dataset items. Otherwise, returns</p> <code>list[DatasetItem] | tuple[list[DatasetItem], list[DatasetItem]]</code> <p>the updated dataset items.</p> Source code in <code>pixano/datasets/dataset.py</code> <pre><code>def update_dataset_items(\n    self,\n    dataset_items: list[DatasetItem],\n    return_separately: bool = False,\n) -&gt; list[DatasetItem] | tuple[list[DatasetItem], list[DatasetItem]]:\n    \"\"\"Update dataset items.\n\n    Warn:\n        Does not test for integrity of the data.\n\n    Args:\n        dataset_items: Dataset items to update.\n        return_separately: Whether to return separately added and updated dataset items.\n\n    Returns:\n        If `return_separately` is `True`, returns a tuple of updated and added dataset items. Otherwise, returns\n        the updated dataset items.\n    \"\"\"\n    fields = self.dataset_item_model.model_fields.keys()\n    if not all(\n        isinstance(item, DatasetItem) and set(fields) == set(item.model_fields.keys()) for item in dataset_items\n    ):\n        raise DatasetAccessError(\"All data must be instances of the same DatasetItem.\")\n\n    schemas_data = [item.to_schemas_data(self.schema) for item in dataset_items]\n    updated_ids = set()\n    tables_data: dict[str, Any] = {}\n    for table_name in self.schema.schemas.keys():\n        for item in schemas_data:\n            if table_name not in tables_data:\n                tables_data[table_name] = []\n            if table_name not in item:\n                continue\n            if isinstance(item[table_name], list):\n                tables_data[table_name].extend(item[table_name])\n            elif item[table_name] is not None:\n                tables_data[table_name].append(item[table_name])\n    for table_name, table_data in tables_data.items():\n        if table_data != []:\n            updated, _ = self.update_data(\n                table_name,\n                table_data,\n                return_separately=True,\n                ignore_integrity_checks=[],\n                raise_or_warn=\"none\",\n            )\n            for row in updated:\n                updated_ids.add(row.item_ref.id if table_name != SchemaGroup.ITEM.value else row.id)\n\n    dataset_items = self.get_dataset_items([item.id for item in dataset_items])\n\n    if not return_separately:\n        return dataset_items\n\n    updated_items, added_items = [], []\n    for item in dataset_items:\n        if item.id not in updated_ids:\n            added_items.append(item)\n        else:\n            updated_items.append(item)\n    return updated_items, added_items\n</code></pre>"},{"location":"api_reference/datasets/dataset_features_values/","title":"dataset_features_values","text":""},{"location":"api_reference/datasets/dataset_features_values/#pixano.datasets.dataset_features_values","title":"<code>pixano.datasets.dataset_features_values</code>","text":""},{"location":"api_reference/datasets/dataset_features_values/#pixano.datasets.dataset_features_values.ConstraintDict","title":"<code>ConstraintDict = Dict[TableName, List[Constraint]]</code>  <code>module-attribute</code>","text":"<p>Dict of Constraint, by table.</p> <p>Keys are table names (<code>TableName</code>), and values are list of constraints (<code>Constraint</code>).</p>"},{"location":"api_reference/datasets/dataset_features_values/#pixano.datasets.dataset_features_values.Constraint","title":"<code>Constraint(name, restricted, values)</code>  <code>dataclass</code>","text":"<p>Constraint values.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the field.</p> <code>restricted</code> <code>bool</code> <p>whether allowed values are restricted to given values, or user may enter new values.</p> <code>values</code> <code>List[Union[int, float, str, bool]]</code> <p>list of allowed values.</p>"},{"location":"api_reference/datasets/dataset_features_values/#pixano.datasets.dataset_features_values.DatasetFeaturesValues","title":"<code>DatasetFeaturesValues(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Constraints for the dataset features values.</p> <p>Attributes:</p> Name Type Description <code>item</code> <code>ConstraintDict</code> <p>Constraints for the dataset item table.</p> <code>views</code> <code>ConstraintDict</code> <p>Constraints for the dataset view tables.</p> <code>entities</code> <code>ConstraintDict</code> <p>Constraints for the dataset entity tables.</p> <code>annotations</code> <code>ConstraintDict</code> <p>Constraints for the dataset annotation tables.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/datasets/dataset_features_values/#pixano.datasets.dataset_features_values.DatasetFeaturesValues.from_json","title":"<code>from_json(json_fp)</code>  <code>staticmethod</code>","text":"<p>Load DatasetFeaturesValues from json file.</p> Source code in <code>pixano/datasets/dataset_features_values.py</code> <pre><code>@staticmethod\ndef from_json(json_fp: Path) -&gt; \"DatasetFeaturesValues\":\n    \"\"\"Load DatasetFeaturesValues from json file.\"\"\"\n    fv_json = json.loads(json_fp.read_text(encoding=\"utf-8\"))\n    fv = DatasetFeaturesValues.model_validate(fv_json)\n\n    return fv\n</code></pre>"},{"location":"api_reference/datasets/dataset_features_values/#pixano.datasets.dataset_features_values.DatasetFeaturesValues.to_json","title":"<code>to_json(json_fp)</code>","text":"<p>Save DatasetFeaturesValues to json file.</p> Source code in <code>pixano/datasets/dataset_features_values.py</code> <pre><code>def to_json(self, json_fp: Path) -&gt; None:\n    \"\"\"Save DatasetFeaturesValues to json file.\"\"\"\n    json_fp.write_text(json.dumps(self.model_dump(), indent=4), encoding=\"utf-8\")\n</code></pre>"},{"location":"api_reference/datasets/dataset_info/","title":"dataset_info","text":""},{"location":"api_reference/datasets/dataset_info/#pixano.datasets.dataset_info","title":"<code>pixano.datasets.dataset_info</code>","text":""},{"location":"api_reference/datasets/dataset_info/#pixano.datasets.dataset_info.DatasetInfo","title":"<code>DatasetInfo(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information of a dataset.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Dataset ID. Must be unique.</p> <code>name</code> <code>str</code> <p>Dataset name.</p> <code>description</code> <code>str</code> <p>Dataset description.</p> <code>estimated_size</code> <code>str</code> <p>Dataset estimated size.</p> <code>preview</code> <code>str</code> <p>Path to a preview thumbnail.</p> <code>workspace</code> <code>WorkspaceType</code> <p>Workspace type.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/datasets/dataset_info/#pixano.datasets.dataset_info.DatasetInfo.from_json","title":"<code>from_json(json_fp)</code>  <code>staticmethod</code>","text":"<p>Read DatasetInfo from JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>json_fp</code> <code>Path</code> <p>JSON file path.</p> required <p>Returns:</p> Type Description <code>'DatasetInfo'</code> <p>the dataset info object.</p> Source code in <code>pixano/datasets/dataset_info.py</code> <pre><code>@staticmethod\ndef from_json(\n    json_fp: Path,\n) -&gt; \"DatasetInfo\":\n    \"\"\"Read DatasetInfo from JSON file.\n\n    Args:\n        json_fp: JSON file path.\n\n    Returns:\n        the dataset info object.\n    \"\"\"\n    info_json = json.loads(json_fp.read_text(encoding=\"utf-8\"))\n\n    info_json[\"workspace\"] = (\n        WorkspaceType(info_json[\"workspace\"]) if \"workspace\" in info_json else WorkspaceType.UNDEFINED\n    )\n    info = DatasetInfo.model_validate(info_json)\n\n    return info\n</code></pre>"},{"location":"api_reference/datasets/dataset_info/#pixano.datasets.dataset_info.DatasetInfo.load_directory","title":"<code>load_directory(directory, return_path=False)</code>  <code>staticmethod</code>","text":"<pre><code>load_directory(directory: Path, return_path: Literal[False] = False) -&gt; list['DatasetInfo']\n</code></pre><pre><code>load_directory(directory: Path, return_path: Literal[True]) -&gt; list[tuple['DatasetInfo', Path]]\n</code></pre> <p>Load list of DatasetInfo from directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Directory to load.</p> required <code>return_path</code> <code>bool</code> <p>Return the paths of the datasets.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[tuple['DatasetInfo', Path]] | list['DatasetInfo']</code> <p>The list of DatasetInfo and the paths of the datasets.</p> Source code in <code>pixano/datasets/dataset_info.py</code> <pre><code>@staticmethod\ndef load_directory(\n    directory: Path,\n    return_path: bool = False,\n) -&gt; list[tuple[\"DatasetInfo\", Path]] | list[\"DatasetInfo\"]:\n    \"\"\"Load list of DatasetInfo from directory.\n\n    Args:\n        directory: Directory to load.\n        return_path: Return the paths of the datasets.\n\n    Returns:\n        The list of DatasetInfo and the paths of the datasets.\n    \"\"\"\n    library: list[DatasetInfo] | list[tuple[DatasetInfo, Path]] = []\n\n    # Browse directory\n    for json_fp in sorted(directory.glob(\"*/info.json\")):\n        info: DatasetInfo = DatasetInfo.from_json(json_fp)\n        try:\n            image = Image.open_url(\n                str(json_fp.parent.resolve() / \"previews/dataset_preview.jpg\"), Path(\"/\"), \"image\"\n            )  # TODO choose correct preview name / path / extension\n            thumb = get_image_thumbnail(image, (350, 150))\n            info.preview = image_to_base64(thumb, \"JPEG\")\n        except Exception:  # TODO: specify exception URL and Value\n            info.preview = \"\"\n        if return_path:\n            library.append((info, json_fp.parent))  #  type: ignore[arg-type]\n        else:\n            library.append(info)\n\n    if library == []:\n        raise FileNotFoundError(f\"No dataset found in {directory}.\")\n\n    return library\n</code></pre>"},{"location":"api_reference/datasets/dataset_info/#pixano.datasets.dataset_info.DatasetInfo.load_id","title":"<code>load_id(id, directory, return_path=False)</code>  <code>staticmethod</code>","text":"<pre><code>load_id(id: str, directory: Path, return_path: Literal[False] = False) -&gt; 'DatasetInfo'\n</code></pre><pre><code>load_id(id: str, directory: Path, return_path: Literal[True] = True) -&gt; tuple['DatasetInfo', Path]\n</code></pre> <p>Load a specific DatasetInfo from directory.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The ID of the dataset to load.</p> required <code>directory</code> <code>Path</code> <p>Directory to load.</p> required <code>return_path</code> <code>bool</code> <p>Return the path of the dataset.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple['DatasetInfo', Path] | 'DatasetInfo'</code> <p>The DatasetInfo.</p> Source code in <code>pixano/datasets/dataset_info.py</code> <pre><code>@staticmethod\ndef load_id(id: str, directory: Path, return_path: bool = False) -&gt; tuple[\"DatasetInfo\", Path] | \"DatasetInfo\":\n    \"\"\"Load a specific DatasetInfo from directory.\n\n    Args:\n        id: The ID of the dataset to load.\n        directory: Directory to load.\n        return_path: Return the path of the dataset.\n\n    Returns:\n        The DatasetInfo.\n    \"\"\"\n    for json_fp in directory.glob(\"*/info.json\"):\n        info = DatasetInfo.from_json(json_fp)\n        if info.id == id:\n            try:\n                info.preview = Image.open_url(\n                    str(json_fp.parent / \"previews/dataset_preview.jpg\"),\n                    json_fp.parent / \"media\",\n                )  # TODO choose correct preview name / path / extension\n            except ValueError:\n                info.preview = \"\"\n            return (info, json_fp.parent) if return_path else info\n    raise FileNotFoundError(f\"No dataset found with ID {id}\")\n</code></pre>"},{"location":"api_reference/datasets/dataset_info/#pixano.datasets.dataset_info.DatasetInfo.serialize_workspace","title":"<code>serialize_workspace(workspace)</code>","text":"<p>Dump workspace as string value, not enum.</p> Source code in <code>pixano/datasets/dataset_info.py</code> <pre><code>@field_serializer(\"workspace\")\ndef serialize_workspace(self, workspace: WorkspaceType):\n    \"\"\"Dump workspace as string value, not enum.\"\"\"\n    return workspace.value\n</code></pre>"},{"location":"api_reference/datasets/dataset_info/#pixano.datasets.dataset_info.DatasetInfo.to_json","title":"<code>to_json(json_fp)</code>","text":"<p>Writes the DatasetInfo object to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>json_fp</code> <code>Path</code> <p>The path to the file where the DatasetInfo object will be written.</p> required Source code in <code>pixano/datasets/dataset_info.py</code> <pre><code>def to_json(self, json_fp: Path) -&gt; None:\n    \"\"\"Writes the DatasetInfo object to a JSON file.\n\n    Args:\n        json_fp: The path to the file where the DatasetInfo object\n            will be written.\n    \"\"\"\n    model_dumped = self.model_dump()\n    json_fp.write_text(json.dumps(model_dumped, indent=4), encoding=\"utf-8\")\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/","title":"dataset_schema","text":""},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema","title":"<code>pixano.datasets.dataset_schema</code>","text":""},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetItem","title":"<code>DatasetItem(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataset Item.</p> <p>It is a Pydantic model that represents an item in a dataset.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The unique identifier of the item.</p> <code>split</code> <code>str</code> <p>The split of the item.</p> <code>created_at</code> <code>datetime</code> <p>The creation date of the item.</p> <code>updated_at</code> <code>datetime</code> <p>The last modification date of the item.</p> <p>Raises <code>ValidationError</code> if the input data cannot be validated to form a valid model.</p> <p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p> <p>Parameters:</p> Name Type Description Default <code>created_at</code> <code>datetime | None</code> <p>The creation date of the object.</p> <code>None</code> <code>updated_at</code> <code>datetime | None</code> <p>The last modification date of the object.</p> <code>None</code> <code>data</code> <code>Any</code> <p>The data of the object validated by Pydantic.</p> <code>{}</code> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetItem.from_dataset_schema","title":"<code>from_dataset_schema(dataset_schema, exclude_embeddings=True)</code>  <code>staticmethod</code>","text":"<p>Create a dataset item model based on the schema.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_schema</code> <code>DatasetSchema</code> <p>The dataset schema.</p> required <code>exclude_embeddings</code> <code>bool</code> <p>Exclude embeddings from the dataset item model to reduce the size.</p> <code>True</code> <p>Returns:</p> Type Description <code>type[DatasetItem]</code> <p>The dataset item model</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>@staticmethod\ndef from_dataset_schema(dataset_schema: DatasetSchema, exclude_embeddings: bool = True) -&gt; type[\"DatasetItem\"]:\n    \"\"\"Create a dataset item model based on the schema.\n\n    Args:\n        dataset_schema: The dataset schema.\n        exclude_embeddings: Exclude embeddings from the dataset item model to reduce the size.\n\n    Returns:\n        The dataset item model\n    \"\"\"\n    item_type = dataset_schema.schemas[SchemaGroup.ITEM.value]\n    fields: dict[str, Any] = {}\n\n    if dataset_schema.relations != {} and SchemaGroup.ITEM.value in dataset_schema.relations:\n        for schema, relation in dataset_schema.relations[SchemaGroup.ITEM.value].items():\n            if exclude_embeddings and schema in dataset_schema.groups[SchemaGroup.EMBEDDING]:\n                continue\n            # Add default value in case an item does not have a specific view or entity.\n            schema_type = dataset_schema.schemas[schema]\n            if relation == SchemaRelation.ONE_TO_MANY:\n                fields[schema] = (list[schema_type], [])  # type: ignore[valid-type]\n            else:\n                fields[schema] = (schema_type | None, None)\n\n    for field_name, field in item_type.model_fields.items():\n        # No default value as all items metadata should be retrieved.\n        fields[field_name] = (field.annotation, ...)\n\n    CustomDatasetItem = create_model(\n        \"DatasetItem\",\n        **fields,\n        __base__=DatasetItem,\n    )\n    return CustomDatasetItem\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetItem.from_schemas_data","title":"<code>from_schemas_data(schemas_data)</code>  <code>staticmethod</code>","text":"<p>Create a DatasetItem from schemas data.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>DatasetItem</code> <p>The DatasetItem class.</p> required <code>schemas_data</code> <code>dict[str, BaseSchema | list[BaseSchema] | None]</code> <p>Schemas data.</p> required <p>Returns:</p> Type Description <code>DatasetItem</code> <p>The created DatasetItem.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>@staticmethod\ndef from_schemas_data(\n    cls: \"DatasetItem\", schemas_data: dict[str, BaseSchema | list[BaseSchema] | None]\n) -&gt; \"DatasetItem\":\n    \"\"\"Create a DatasetItem from schemas data.\n\n    Args:\n        cls: The DatasetItem class.\n        schemas_data: Schemas data.\n\n    Returns:\n        The created DatasetItem.\n    \"\"\"\n    if SchemaGroup.ITEM.value not in schemas_data:\n        raise ValueError(\"Item schema data not found.\")\n\n    schemas_data.update(schemas_data.pop(SchemaGroup.ITEM.value).model_dump())  # type: ignore[union-attr]\n    return cls(**schemas_data)\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetItem.get_sub_dataset_item","title":"<code>get_sub_dataset_item(selected_fields)</code>  <code>classmethod</code>","text":"<p>Create a new dataset item based on the selected fields of the original dataset item.</p> Note <p>The id and split fields are always included in the sub dataset item.</p> Note <p>The sub dataset item does not have the methods and config of the original dataset item.</p> <p>Parameters:</p> Name Type Description Default <code>selected_fields</code> <code>list[str]</code> <p>The selected fields.</p> required <p>Returns:</p> Type Description <code>type[Self]</code> <p>The sub dataset item.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>@classmethod\ndef get_sub_dataset_item(cls, selected_fields: list[str]) -&gt; type[Self]:\n    \"\"\"Create a new dataset item based on the selected fields of the original dataset\n    item.\n\n    Note:\n        The id and split fields are always included in the sub dataset item.\n\n    Note:\n        The sub dataset item does not have the methods and config of the original\n        dataset item.\n\n    Args:\n        selected_fields: The selected fields.\n\n    Returns:\n        The sub dataset item.\n    \"\"\"\n    fields = {}\n    for field_name, field in cls.model_fields.items():\n        if field_name in selected_fields or field_name in [\"id\", \"split\"]:\n            if isinstance(field.annotation, GenericAlias):\n                origin = field.annotation.__origin__\n                args = field.annotation.__args__\n\n                # Check if field is list or tuple\n                if origin is tuple:\n                    fields[field_name] = (origin[args[0], ...], field.default)  # type: ignore[index]\n                else:\n                    fields[field_name] = (field.annotation, field.default)\n            else:\n                fields[field_name] = (field.annotation, field.default)\n\n    SubDatasetItem: type[DatasetItem] = create_model(\n        cls.__name__,\n        **fields,\n        __base__=DatasetItem,\n    )\n\n    return SubDatasetItem\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetItem.model_copy","title":"<code>model_copy(*, dataset, deep=False)</code>","text":"<p>Returns a copy of the model.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset where the DatasetItem belongs.</p> required <code>deep</code> <code>bool</code> <p>Set to <code>True</code> to make a deep copy of the model.</p> <code>False</code> <p>Returns:</p> Type Description <code>Self</code> <p>New model instance.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def model_copy(self, *, dataset: \"Dataset\", deep: bool = False) -&gt; Self:\n    \"\"\"Returns a copy of the model.\n\n    Args:\n        dataset: The dataset where the DatasetItem belongs.\n        deep: Set to `True` to make a deep copy of the model.\n\n    Returns:\n        New model instance.\n    \"\"\"\n    # Actual copy done by each schema to call our own model_copy method\n    data: dict[str, BaseSchema | list[BaseSchema] | None] = self.to_schemas_data(dataset.schema)\n    copied_data: dict[str, BaseSchema | list[BaseSchema] | None] = {}\n    for key, value in data.items():\n        if isinstance(value, list):\n            copied_data[key] = [item.model_copy(deep=deep) for item in value]\n        elif value is not None:\n            copied_data[key] = value.model_copy(deep=deep)\n        else:\n            copied_data[key] = None\n    copy_item = self.from_schemas_data(self.__class__, copied_data)  # type: ignore[arg-type]\n    return copy_item\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetItem.model_dump","title":"<code>model_dump(exclude_timestamps=False, **kwargs)</code>","text":"<p>Dump the model to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_timestamps</code> <code>bool</code> <p>Exclude timestamps \"created_at\" and \"updated_at\" from the model dump. Useful for comparing models without timestamps.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Arguments for pydantic <code>BaseModel.model_dump()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The model dump.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def model_dump(self, exclude_timestamps: bool = False, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Dump the model to a dictionary.\n\n    Args:\n        exclude_timestamps: Exclude timestamps \"created_at\" and \"updated_at\" from the model dump. Useful for\n            comparing models without timestamps.\n        kwargs: Arguments for pydantic `BaseModel.model_dump()`.\n\n    Returns:\n        The model dump.\n    \"\"\"\n    model_dump = super().model_dump(**kwargs)\n    if exclude_timestamps:\n        model_dump.pop(\"created_at\", None)\n        model_dump.pop(\"updated_at\", None)\n        for k, value in model_dump.items():\n            if isinstance(value, dict):\n                value.pop(\"created_at\", None)\n                value.pop(\"updated_at\", None)\n            elif isinstance(value, list):  # Only one level deep.\n                for item in value:\n                    if isinstance(item, dict):\n                        item.pop(\"created_at\", None)\n                        item.pop(\"updated_at\", None)\n    return model_dump\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetItem.to_dataset_schema","title":"<code>to_dataset_schema()</code>  <code>classmethod</code>","text":"<p>Convert a DatasetItem to a DatasetSchema.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>@classmethod\ndef to_dataset_schema(cls) -&gt; DatasetSchema:\n    \"\"\"Convert a DatasetItem to a DatasetSchema.\"\"\"\n    return DatasetSchema.from_dataset_item(cls)\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetItem.to_schemas_data","title":"<code>to_schemas_data(dataset_schema)</code>","text":"<p>Convert DatasetItem to schemas data.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_schema</code> <code>DatasetSchema</code> <p>DatasetSchema to convert to.</p> required <p>Returns:</p> Type Description <code>dict[str, BaseSchema | list[BaseSchema] | None]</code> <p>Schemas data.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def to_schemas_data(self, dataset_schema: DatasetSchema) -&gt; dict[str, BaseSchema | list[BaseSchema] | None]:\n    \"\"\"Convert DatasetItem to schemas data.\n\n    Args:\n        dataset_schema: DatasetSchema to convert to.\n\n    Returns:\n        Schemas data.\n    \"\"\"\n    schemas_data = {}\n    item_data = {}\n    for field_name in self.model_fields.keys():\n        if field_name in dataset_schema.schemas:\n            schemas_data[field_name] = getattr(self, field_name)\n        else:\n            item_data[field_name] = getattr(self, field_name)\n    schemas_data[SchemaGroup.ITEM.value] = dataset_schema.schemas[SchemaGroup.ITEM.value](**item_data)\n    return schemas_data\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetSchema","title":"<code>DatasetSchema(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A dataset schema that defines the tables and the relations between them.</p> <p>Attributes:</p> Name Type Description <code>schemas</code> <code>dict[str, type[BaseSchema]]</code> <p>The mapping between the table names and their schema.</p> <code>relations</code> <code>dict[str, dict[str, SchemaRelation]]</code> <p>The relations between the item table and the other tables.</p> <code>groups</code> <code>dict[SchemaGroup, set[str]]</code> <p>The groups of tables. It is filled automatically based on the schemas.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetSchema.add_schema","title":"<code>add_schema(table_name, schema, relation_item, overwrite_schema=False)</code>","text":"<p>Add a schema to the dataset schema.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to add to the dataset schema.</p> required <code>schema</code> <code>type[BaseSchema]</code> <p>Schema of the table.</p> required <code>relation_item</code> <code>SchemaRelation</code> <p>Relationship with the item schema.</p> required <code>overwrite_schema</code> <code>bool</code> <p>If True, existing schema will be overwritten, else raise ValueError.</p> <code>False</code> <p>Returns:</p> Type Description <code>Self</code> <p>The dataset schema.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def add_schema(\n    self, table_name: str, schema: type[BaseSchema], relation_item: SchemaRelation, overwrite_schema: bool = False\n) -&gt; Self:\n    \"\"\"Add a schema to the dataset schema.\n\n    Args:\n        table_name: Name of the table to add to the dataset schema.\n        schema: Schema of the table.\n        relation_item: Relationship with the item schema.\n        overwrite_schema: If True, existing schema will be overwritten, else raise ValueError.\n\n    Returns:\n        The dataset schema.\n    \"\"\"\n    table_name = self.format_table_name(table_name)\n    if not overwrite_schema and table_name in self.schemas:\n        raise ValueError(f\"Table {table_name} already exists in the schemas.\")\n    elif not issubclass(schema, BaseSchema):\n        raise ValueError(f\"Schema {schema} should be a subclass of BaseSchema.\")\n    elif not isinstance(relation_item, SchemaRelation):\n        raise ValueError(f\"Invalid relation {relation_item}.\")\n    found_group = False\n    for group, group_type in _SCHEMA_GROUP_TO_SCHEMA_DICT.items():\n        if issubclass(schema, group_type):\n            self.groups[group].add(table_name)\n            found_group = True\n            break\n    if not found_group:\n        raise ValueError(f\"Invalid table type {schema}\")\n    self.schemas[table_name] = schema\n    if relation_item == SchemaRelation.ONE_TO_ONE:\n        self.relations[SchemaGroup.ITEM.value][table_name] = SchemaRelation.ONE_TO_ONE\n        self.relations[table_name] = {SchemaGroup.ITEM.value: SchemaRelation.ONE_TO_ONE}\n    elif relation_item == SchemaRelation.ONE_TO_MANY:\n        self.relations[SchemaGroup.ITEM.value][table_name] = SchemaRelation.MANY_TO_ONE\n        self.relations[table_name] = {SchemaGroup.ITEM.value: SchemaRelation.ONE_TO_MANY}\n    elif relation_item == SchemaRelation.MANY_TO_ONE:\n        self.relations[SchemaGroup.ITEM.value][table_name] = SchemaRelation.ONE_TO_MANY\n        self.relations[table_name] = {SchemaGroup.ITEM.value: SchemaRelation.MANY_TO_ONE}\n    elif relation_item == SchemaRelation.MANY_TO_MANY:\n        self.relations[SchemaGroup.ITEM.value][table_name] = SchemaRelation.MANY_TO_MANY\n        self.relations[table_name] = {SchemaGroup.ITEM.value: SchemaRelation.MANY_TO_MANY}\n    return self\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetSchema.deserialize","title":"<code>deserialize(dataset_schema_json)</code>  <code>staticmethod</code>","text":"<p>Deserialize the dataset schema.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_schema_json</code> <code>dict[str, dict[str, Any]]</code> <p>Serialized dataset schema.</p> required <p>Returns:</p> Type Description <code>DatasetSchema</code> <p>The dataset schema.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>@staticmethod\ndef deserialize(dataset_schema_json: dict[str, dict[str, Any]]) -&gt; \"DatasetSchema\":\n    \"\"\"Deserialize the dataset schema.\n\n    Args:\n        dataset_schema_json: Serialized dataset schema.\n\n    Returns:\n        The dataset schema.\n    \"\"\"\n    dataset_schema_dict: dict[str, Any] = {\n        \"relations\": {\n            schema1: {schema2: SchemaRelation(relation) for schema2, relation in relations.items()}\n            for schema1, relations in dataset_schema_json[\"relations\"].items()\n        },\n        \"schemas\": {},\n        \"groups\": {SchemaGroup(group): set(schemas) for group, schemas in dataset_schema_json[\"groups\"].items()},\n    }\n    for table_name, schema in dataset_schema_json[\"schemas\"].items():\n        dataset_schema_dict[\"schemas\"][table_name] = BaseSchema.deserialize(schema)\n    return DatasetSchema(**dataset_schema_dict)\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetSchema.format_table_name","title":"<code>format_table_name(table_name)</code>  <code>staticmethod</code>","text":"<p>Format table name.</p> <p>It converts the table name to lowercase and replaces spaces with underscores.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>str</code> <p>the formatted table name.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>@staticmethod\ndef format_table_name(table_name: str) -&gt; str:\n    \"\"\"Format table name.\n\n    It converts the table name to lowercase and replaces spaces with underscores.\n\n    Args:\n        table_name: Table name.\n\n    Returns:\n        the formatted table name.\n    \"\"\"\n    return table_name.lower().replace(\" \", \"_\")\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetSchema.from_dataset_item","title":"<code>from_dataset_item(dataset_item)</code>  <code>staticmethod</code>","text":"<p>Create a dataset schema from a DatasetItem.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_item</code> <code>type[DatasetItem]</code> <p>The dataset item.</p> required <p>Returns:</p> Type Description <code>DatasetSchema</code> <p>The dataset schema.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>@staticmethod\ndef from_dataset_item(dataset_item: type[\"DatasetItem\"]) -&gt; \"DatasetSchema\":\n    \"\"\"Create a dataset schema from a [DatasetItem][pixano.datasets.DatasetItem].\n\n    Args:\n        dataset_item: The dataset item.\n\n    Returns:\n        The dataset schema.\n    \"\"\"\n    item_fields = {}\n\n    # table schemas\n    dataset_schema_dict: dict[str, Any] = {}\n    dataset_schema_dict[\"relations\"] = {SchemaGroup.ITEM.value: {}}\n    schemas = {}\n\n    for field_name, field in dataset_item.model_fields.items():\n        # Check if field is a generic alias (list or tuple)\n        if isinstance(field.annotation, GenericAlias):\n            origin = field.annotation.__origin__\n            args = field.annotation.__args__\n\n            # Check if field is list or tuple\n            if origin in [list, tuple]:\n                # Categorizing list of schemas as schemas and keeping track of the relation\n                if issubclass(args[0], tuple(_SCHEMA_REGISTRY.values())):\n                    schemas[field_name] = args[0]\n                    dataset_schema_dict[\"relations\"][SchemaGroup.ITEM.value][field_name] = (\n                        SchemaRelation.ONE_TO_MANY\n                    )\n                    dataset_schema_dict[\"relations\"][field_name] = {\n                        SchemaGroup.ITEM.value: SchemaRelation.MANY_TO_ONE\n                    }\n                else:\n                    item_fields[field_name] = (list[args[0]], ...)  # type: ignore[valid-type]\n            else:\n                # Default case: categorize as item attribute\n                item_fields[field_name] = (args[0], ...)  # type: ignore[valid-type]\n        # Check if field is a schema\n        elif issubclass(field.annotation, tuple(_SCHEMA_REGISTRY.values())):\n            schemas[field_name] = field.annotation\n            dataset_schema_dict[\"relations\"][SchemaGroup.ITEM.value][field_name] = SchemaRelation.ONE_TO_ONE\n            dataset_schema_dict[\"relations\"][field_name] = {SchemaGroup.ITEM.value: SchemaRelation.ONE_TO_ONE}\n        else:\n            # Default case: item attribute\n            item_fields[field_name] = (field.annotation, ...)\n\n    CustomItem = create_model(\"Item\", **item_fields, __base__=Item)\n\n    schemas[SchemaGroup.ITEM.value] = CustomItem\n    dataset_schema_dict[\"schemas\"] = schemas\n\n    return DatasetSchema(**dataset_schema_dict)\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetSchema.from_json","title":"<code>from_json(json_fp)</code>  <code>staticmethod</code>","text":"<p>Read a dataset schema from JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>json_fp</code> <code>Path</code> <p>JSON file path</p> required <p>Returns:</p> Type Description <code>DatasetSchema</code> <p>The dataset schema.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>@staticmethod\ndef from_json(\n    json_fp: Path,\n) -&gt; \"DatasetSchema\":\n    \"\"\"Read a dataset schema from JSON file.\n\n    Args:\n        json_fp: JSON file path\n\n    Returns:\n        The dataset schema.\n    \"\"\"\n    schema_json = json.loads(json_fp.read_text(encoding=\"utf-8\"))\n\n    return DatasetSchema.deserialize(schema_json)\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetSchema.get_table_group","title":"<code>get_table_group(table_name)</code>","text":"<p>Get the group of a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>SchemaGroup</code> <p>The group of the table.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def get_table_group(self, table_name: str) -&gt; SchemaGroup:\n    \"\"\"Get the group of a table.\n\n    Args:\n        table_name: Table name.\n\n    Returns:\n        The group of the table.\n    \"\"\"\n    for group, tables in self.groups.items():\n        if table_name in tables:\n            return group\n    raise ValueError(f\"Table {table_name} not found in groups.\")\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetSchema.serialize","title":"<code>serialize()</code>","text":"<p>Serialize the dataset schema.</p> <p>The serialized schema is a dictionary with the following format: {     \"relations\": {         \"item\": {             \"image\": \"one_to_one\",         }     },     \"schemas\": {         \"table1\": {             \"schema\": \"CustomItem\",             \"base_schema\": \"Item\",             \"fields\": {                 \"id\": {                     \"type\": \"str\",                     \"collection\": False                 },                 \"split\": {                     \"type\": \"str\",                     \"collection\": False                 },                 ...             }</p> <pre><code>    }\n}\n</code></pre> <p>}</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>The serialized dataset schema.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>@model_serializer\ndef serialize(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Serialize the dataset schema.\n\n    The serialized schema is a dictionary with the following format:\n    {\n        \"relations\": {\n            \"item\": {\n                \"image\": \"one_to_one\",\n            }\n        },\n        \"schemas\": {\n            \"table1\": {\n                \"schema\": \"CustomItem\",\n                \"base_schema\": \"Item\",\n                \"fields\": {\n                    \"id\": {\n                        \"type\": \"str\",\n                        \"collection\": False\n                    },\n                    \"split\": {\n                        \"type\": \"str\",\n                        \"collection\": False\n                    },\n                    ...\n                }\n\n            }\n        }\n    }\n\n    Returns:\n        The serialized dataset schema.\n    \"\"\"\n    dataset_schema_json: dict[str, dict[str, Any]] = {\n        \"relations\": {\n            schema1: {schema2: relation.value for schema2, relation in relations.items()}\n            for schema1, relations in self.relations.items()\n        },\n        \"schemas\": {},\n        \"groups\": {group.value: list(schemas) for group, schemas in self.groups.items()},\n    }\n    for table_name, schema in self.schemas.items():\n        dataset_schema_json[\"schemas\"][table_name] = schema.serialize()\n    return dataset_schema_json\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.DatasetSchema.to_json","title":"<code>to_json(json_fp)</code>","text":"<p>Save DatasetSchema to json file.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def to_json(self, json_fp: Path) -&gt; None:\n    \"\"\"Save DatasetSchema to json file.\"\"\"\n    if json_fp.exists():\n        old_json_content = json.loads(json_fp.read_text(encoding=\"utf-8\"))\n    else:\n        old_json_content = None\n\n    json_content = self.serialize()\n\n    # Keep the schema field from the old json content for custom schemas.\n    if old_json_content is not None:\n        for table, schema in json_content[\"schemas\"].items():\n            if table not in old_json_content[\"schemas\"]:\n                continue\n            schema[\"schema\"] = old_json_content[\"schemas\"][table][\"schema\"]\n    json_fp.write_text(json.dumps(json_content, indent=4), encoding=\"utf-8\")\n</code></pre>"},{"location":"api_reference/datasets/dataset_schema/#pixano.datasets.dataset_schema.SchemaRelation","title":"<code>SchemaRelation</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Relation between tables.</p> <p>Attributes:</p> Name Type Description <code>ONE_TO_MANY</code> <p>One to many relation.</p> <code>MANY_TO_ONE</code> <p>Many to one relation.</p> <code>ONE_TO_ONE</code> <p>One to one relation.</p> <code>MANY_TO_MANY</code> <p>Many to many relation</p>"},{"location":"api_reference/datasets/dataset_stat/","title":"dataset_stat","text":""},{"location":"api_reference/datasets/dataset_stat/#pixano.datasets.dataset_stat","title":"<code>pixano.datasets.dataset_stat</code>","text":""},{"location":"api_reference/datasets/dataset_stat/#pixano.datasets.dataset_stat.DatasetStatistic","title":"<code>DatasetStatistic(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A statistic of a dataset.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the statistic.</p> <code>type</code> <code>str</code> <p>The type ('numerical' or 'categorical') of the statistic.</p> <code>histogram</code> <code>list[dict[str, float | int | str]]</code> <p>The histogram of the statistic.</p> <code>range</code> <code>list[int | float] | None</code> <p>The range of the statistic.</p> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/datasets/dataset_stat/#pixano.datasets.dataset_stat.DatasetStatistic.from_json","title":"<code>from_json(json_fp)</code>  <code>staticmethod</code>","text":"<p>Read list of <code>DatasetStatistic</code> from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>json_fp</code> <code>Path</code> <p>JSON file path.</p> required <p>Returns:</p> Type Description <code>list[DatasetStatistic]</code> <p>A list of <code>DatasetStat</code>.</p> Source code in <code>pixano/datasets/dataset_stat.py</code> <pre><code>@staticmethod\ndef from_json(json_fp: Path) -&gt; list[\"DatasetStatistic\"]:\n    \"\"\"Read list of `DatasetStatistic` from a JSON file.\n\n    Args:\n        json_fp: JSON file path.\n\n    Returns:\n        A list of `DatasetStat`.\n    \"\"\"\n    stats_json = json.loads(json_fp.read_text(encoding=\"utf-8\"))\n\n    return [DatasetStatistic.model_validate(stat) for stat in stats_json]\n</code></pre>"},{"location":"api_reference/datasets/dataset_stat/#pixano.datasets.dataset_stat.DatasetStatistic.to_json","title":"<code>to_json(json_fp)</code>","text":"<p>Save <code>DatasetStatistic</code> to a json file.</p> <p>Replace the existing histogram in <code>json_fp</code>.</p> <p>Parameters:</p> Name Type Description Default <code>json_fp</code> <code>Path</code> <p>Save directory.</p> required Source code in <code>pixano/datasets/dataset_stat.py</code> <pre><code>def to_json(self, json_fp: Path) -&gt; None:\n    \"\"\"Save `DatasetStatistic` to a json file.\n\n    Replace the existing histogram in `json_fp`.\n\n    Args:\n        json_fp: Save directory.\n    \"\"\"\n    try:\n        stats_json = json.loads(json_fp.read_text(encoding=\"utf-8\"))\n    except FileNotFoundError:\n        stats_json = []\n    # keep all stats except the one with same name, we replace it if exist\n    stats_json = [stat for stat in stats_json if stat[\"name\"] != self.name]\n    stats_json.append({\"name\": self.name, \"type\": self.type, \"histogram\": self.histogram, \"range\": self.range})\n\n    json_fp.write_text(json.dumps(stats_json, indent=4), encoding=\"utf-8\")\n</code></pre>"},{"location":"api_reference/datasets/builders/dataset_builder/","title":"dataset_builder","text":""},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder","title":"<code>pixano.datasets.builders.dataset_builder</code>","text":""},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder","title":"<code>DatasetBuilder(target_dir, dataset_item, info)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for dataset builders.</p> <p>To build a dataset, inherit from this class, implement the <code>generate_data</code> method and launch the <code>build</code> method.</p> <p>Attributes:</p> Name Type Description <code>target_dir</code> <code>Path</code> <p>The target directory for the dataset.</p> <code>previews_path</code> <code>Path</code> <p>The path to the previews directory.</p> <code>info</code> <code>DatasetInfo</code> <p>Dataset information (name, description, ...).</p> <code>dataset_schema</code> <code>DatasetSchema</code> <p>The schema of the dataset.</p> <code>schemas</code> <code>dict[str, type[BaseSchema]]</code> <p>The schemas of the dataset tables.</p> <code>db</code> <code>DBConnection</code> <p>The connection to the <code>LanceDB</code> database of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir</code> <code>Path | str</code> <p>The target directory for the dataset.</p> required <code>dataset_item</code> <code>type[DatasetItem]</code> <p>The dataset item schema.</p> required <code>info</code> <code>DatasetInfo</code> <p>Dataset information (name, description, ...).</p> required Source code in <code>pixano/datasets/builders/dataset_builder.py</code> <pre><code>def __init__(\n    self,\n    target_dir: Path | str,\n    dataset_item: type[DatasetItem],\n    info: DatasetInfo,\n):\n    \"\"\"Initialize a DatasetBuilder instance.\n\n    Args:\n        target_dir: The target directory for the dataset.\n        dataset_item: The dataset item schema.\n        info: Dataset information (name, description, ...).\n    \"\"\"\n    self.target_dir: Path = Path(target_dir)\n    self.previews_path: Path = self.target_dir / Dataset._PREVIEWS_PATH\n\n    self.info: DatasetInfo = info\n    self.dataset_schema: DatasetSchema = dataset_item.to_dataset_schema()\n    self.schemas: dict[str, type[BaseSchema]] = self.dataset_schema.schemas\n\n    self.db: lancedb.DBConnection = lancedb.connect(self.target_dir / Dataset._DB_PATH)\n</code></pre>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.item_schema","title":"<code>item_schema</code>  <code>property</code>","text":"<p>The item schema for the dataset.</p>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.item_schema_name","title":"<code>item_schema_name</code>  <code>property</code>","text":"<p>The item schema name for the dataset.</p>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.add_ground_truth_source","title":"<code>add_ground_truth_source(metadata={})</code>","text":"<p>Add a ground truth source to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>str | dict[str, Any]</code> <p>Metadata of the ground truth source.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The id of the ground truth source.</p> Source code in <code>pixano/datasets/builders/dataset_builder.py</code> <pre><code>def add_ground_truth_source(self, metadata: str | dict[str, Any] = {}) -&gt; str:\n    \"\"\"Add a ground truth source to the dataset.\n\n    Args:\n        metadata: Metadata of the ground truth source.\n\n    Returns:\n        The id of the ground truth source.\n    \"\"\"\n    return self.add_source(\n        id=SourceKind.GROUND_TRUTH.value, name=\"Ground Truth\", kind=SourceKind.GROUND_TRUTH, metadata=metadata\n    )\n</code></pre>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.add_source","title":"<code>add_source(name, kind, metadata={}, id='')</code>","text":"<p>Add a source to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the source.</p> required <code>kind</code> <code>str | SourceKind</code> <p>Kind of source.</p> required <code>metadata</code> <code>str | dict[str, Any]</code> <p>Metadata of the source. If a dict is provided, it is converted to a JSON string.</p> <code>{}</code> <code>id</code> <code>str</code> <p>The id of the source. If not provided, a random id is generated.</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>The id of the source.</p> Source code in <code>pixano/datasets/builders/dataset_builder.py</code> <pre><code>def add_source(\n    self,\n    name: str,\n    kind: str | SourceKind,\n    metadata: str | dict[str, Any] = {},\n    id: str = \"\",\n) -&gt; str:\n    \"\"\"Add a source to the dataset.\n\n    Args:\n        name: Name of the source.\n        kind: Kind of source.\n        metadata: Metadata of the source. If a dict is provided, it is converted to a JSON string.\n        id: The id of the source. If not provided, a random id is generated.\n\n    Returns:\n        The id of the source.\n    \"\"\"\n    if id == \"\":\n        id = shortuuid.uuid()\n    if isinstance(kind, str):\n        kind = SourceKind(kind)\n    self.db.open_table(\"source\").add([Source(id=id, name=name, kind=kind.value, metadata=metadata)])\n    return id\n</code></pre>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.build","title":"<code>build(mode='create', flush_every_n_samples=None, compact_every_n_transactions=None, check_integrity='raise')</code>","text":"<p>Build the dataset.</p> <p>It generates data from the source directory and insert them in the tables of the database.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['add', 'create', 'overwrite']</code> <p>The mode for creating the tables in the database. The mode can be \"create\", \"overwrite\" or \"add\":     - \"create\": Create the tables in the database. If the tables already exist, an error is raised.     - \"overwrite\": Overwrite the tables in the database.     - \"add\": Append to the tables in the database.</p> <code>'create'</code> <code>flush_every_n_samples</code> <code>int | None</code> <p>The number of samples accumulated from <code>generate_data</code> before they are flushed in tables. The counter is per table. If None, data are inserted at each iteration.</p> <code>None</code> <code>compact_every_n_transactions</code> <code>int | None</code> <p>The number of transactions before compacting each table. If None, the dataset is compacted only at the end.</p> <code>None</code> <code>check_integrity</code> <code>Literal['raise', 'warn', 'none']</code> <p>The integrity check to perform after building the dataset. It can be \"raise\", \"warn\" or \"none\":     - \"raise\": Raise an error if integrity errors are found.     - \"warn\": Print a warning if integrity errors are found.     - \"none\": Do not check integrity.</p> <code>'raise'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The built dataset.</p> Source code in <code>pixano/datasets/builders/dataset_builder.py</code> <pre><code>def build(\n    self,\n    mode: Literal[\"add\", \"create\", \"overwrite\"] = \"create\",\n    flush_every_n_samples: int | None = None,\n    compact_every_n_transactions: int | None = None,\n    check_integrity: Literal[\"raise\", \"warn\", \"none\"] = \"raise\",\n) -&gt; Dataset:\n    \"\"\"Build the dataset.\n\n    It generates data from the source directory and insert them in the tables of the database.\n\n    Args:\n        mode: The mode for creating the tables in the database.\n            The mode can be \"create\", \"overwrite\" or \"add\":\n                - \"create\": Create the tables in the database. If the tables already exist, an error is raised.\n                - \"overwrite\": Overwrite the tables in the database.\n                - \"add\": Append to the tables in the database.\n        flush_every_n_samples: The number of samples accumulated from `generate_data` before they are\n            flushed in tables. The counter is per table. If None, data are inserted at each iteration.\n        compact_every_n_transactions: The number of transactions before compacting each table.\n            If None, the dataset is compacted only at the end.\n        check_integrity: The integrity check to perform after building the dataset. It can be \"raise\",\n            \"warn\" or \"none\":\n                - \"raise\": Raise an error if integrity errors are found.\n                - \"warn\": Print a warning if integrity errors are found.\n                - \"none\": Do not check integrity.\n\n    Returns:\n        The built dataset.\n    \"\"\"\n    if mode not in [\"add\", \"create\", \"overwrite\"]:\n        raise ValueError(f\"mode should be 'add', 'create' or 'overwrite' but got {mode}\")\n    if check_integrity not in [\"raise\", \"warn\", \"none\"]:\n        raise ValueError(f\"check_integrity should be 'raise', 'warn' or 'none' but got {check_integrity}\")\n    if flush_every_n_samples is not None and flush_every_n_samples &lt;= 0:\n        raise ValueError(f\"flush_every_n_samples should be greater than 0 but got {flush_every_n_samples}\")\n    if compact_every_n_transactions is not None and compact_every_n_transactions &lt;= 0:\n        raise ValueError(\n            f\"compact_every_n_transactions should be greater than 0 but got {compact_every_n_transactions}\"\n        )\n\n    # save info.json\n    self.info.id = shortuuid.uuid() if self.info.id == \"\" else self.info.id\n    self.info.to_json(self.target_dir / Dataset._INFO_FILE)\n\n    # save features_values.json\n    # TMP: empty now\n    DatasetFeaturesValues().to_json(self.target_dir / Dataset._FEATURES_VALUES_FILE)\n\n    # remove previous schema.json if any\n    if (self.target_dir / Dataset._SCHEMA_FILE).exists():\n        (self.target_dir / Dataset._SCHEMA_FILE).unlink()\n    # save schema.json\n    self.dataset_schema.to_json(self.target_dir / Dataset._SCHEMA_FILE)\n\n    if mode == \"add\":\n        tables = self.open_tables()\n    else:\n        tables = self.create_tables(mode)\n\n    # accumulate items to insert in tables\n    accumulate_data_tables: dict[str, list] = {table_name: [] for table_name in tables.keys()}\n    # count transactions per table\n    transactions_per_table: dict[str, int] = dict.fromkeys(tables.keys(), 0)\n\n    logger.info(f\"Building dataset {self.info.name}\")\n    for items in tqdm.tqdm(self.generate_data(), desc=f\"Generate data for dataset {self.info.name}\"):\n        # assert that items have keys that are in tables\n        for table_name, item_value in items.items():\n            if item_value is None or item_value == []:\n                continue\n            if table_name not in tables:\n                raise KeyError(f\"Table {table_name} not found in tables\")\n\n            accumulate_data_tables[table_name].extend(item_value if isinstance(item_value, list) else [item_value])\n\n            # make transaction every n iterations per table\n            if len(accumulate_data_tables[table_name]) &gt; 0 and (\n                flush_every_n_samples is None\n                or len(accumulate_data_tables[table_name]) % flush_every_n_samples == 0\n            ):\n                table = tables[table_name]\n                table.add(accumulate_data_tables[table_name])\n                transactions_per_table[table_name] += 1\n                accumulate_data_tables[table_name] = []\n\n            # compact dataset every n transactions per table\n            if (\n                compact_every_n_transactions is not None\n                and transactions_per_table[table_name] % compact_every_n_transactions == 0\n                and transactions_per_table[table_name] &gt; 0\n            ):\n                self.compact_table(table_name)\n\n    # make transaction for final batch\n    for table_name, table in tables.items():\n        if len(accumulate_data_tables[table_name]) &gt; 0:\n            table.add(accumulate_data_tables[table_name])\n    self.compact_dataset()\n\n    logger.info(f\"Dataset {self.info.name} built in {self.target_dir} with id {self.info.id}\")\n\n    dataset = Dataset(self.target_dir)\n\n    if check_integrity != \"none\":\n        logger.info(f\"Checking dataset {dataset.info.name} integrity...\")\n        handle_integrity_errors(check_dataset_integrity(dataset), raise_or_warn=check_integrity)\n\n    logger.info(f\"Dataset {dataset.info.name} built successfully.\")\n    return dataset\n</code></pre>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.compact_dataset","title":"<code>compact_dataset()</code>","text":"<p>Compact the dataset by calling <code>compact_table</code> for each table in the database.</p> Source code in <code>pixano/datasets/builders/dataset_builder.py</code> <pre><code>def compact_dataset(self) -&gt; None:\n    \"\"\"Compact the dataset by calling `compact_table` for each table in the database.\"\"\"\n    for table_name in self.schemas.keys():\n        self.compact_table(table_name)\n</code></pre>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.compact_table","title":"<code>compact_table(table_name)</code>","text":"<p>Compact a table by cleaning up old versions and compacting files.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to compact.</p> required Source code in <code>pixano/datasets/builders/dataset_builder.py</code> <pre><code>def compact_table(self, table_name: str) -&gt; None:\n    \"\"\"Compact a table by cleaning up old versions and compacting files.\n\n    Args:\n        table_name: The name of the table to compact.\n    \"\"\"\n    table = self.db.open_table(table_name)\n    table.compact_files(\n        target_rows_per_fragment=1048576, max_rows_per_group=1024, materialize_deletions=False, num_threads=None\n    )\n    table.cleanup_old_versions(older_than=timedelta(days=0), delete_unverified=True)\n</code></pre>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.create_tables","title":"<code>create_tables(mode='create')</code>","text":"<p>Create tables in the database.</p> <p>Returns:</p> Type Description <code>dict[str, Table]</code> <p>The tables in the database.</p> Source code in <code>pixano/datasets/builders/dataset_builder.py</code> <pre><code>def create_tables(\n    self,\n    mode: Literal[\"create\", \"overwrite\"] = \"create\",\n) -&gt; dict[str, Table]:\n    \"\"\"Create tables in the database.\n\n    Returns:\n        The tables in the database.\n    \"\"\"\n    tables = {}\n    for key, schema in self.schemas.items():\n        self.db.create_table(key, schema=schema, mode=mode)\n\n        tables[key] = self.db.open_table(key)\n    self.db.create_table(\"source\", schema=Source, mode=mode)\n    tables[\"source\"] = self.db.open_table(\"source\")\n\n    return tables\n</code></pre>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.generate_data","title":"<code>generate_data()</code>  <code>abstractmethod</code>","text":"<p>Generate data from the source directory.</p> <p>It should yield a dictionary with keys corresponding to the table names and values corresponding to the data.</p> <p>It must be implemented in the subclass.</p> <p>Returns:</p> Type Description <code>Iterator[dict[str, BaseSchema | list[BaseSchema]]]</code> <p>An iterator over the data following the dataset schemas.</p> Source code in <code>pixano/datasets/builders/dataset_builder.py</code> <pre><code>@abstractmethod\ndef generate_data(self) -&gt; Iterator[dict[str, BaseSchema | list[BaseSchema]]]:\n    \"\"\"Generate data from the source directory.\n\n    It should yield a dictionary with keys corresponding to the table names and values corresponding to the data.\n\n    It must be implemented in the subclass.\n\n    Returns:\n        An iterator over the data following the dataset schemas.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/datasets/builders/dataset_builder/#pixano.datasets.builders.dataset_builder.DatasetBuilder.open_tables","title":"<code>open_tables()</code>","text":"<p>Open the tables in the database.</p> <p>Returns:</p> Type Description <code>dict[str, Table]</code> <p>The tables in the database.</p> Source code in <code>pixano/datasets/builders/dataset_builder.py</code> <pre><code>def open_tables(self) -&gt; dict[str, Table]:\n    \"\"\"Open the tables in the database.\n\n    Returns:\n        The tables in the database.\n    \"\"\"\n    tables = {}\n    for key in self.schemas.keys():\n        tables[key] = self.db.open_table(key)\n    tables[\"source\"] = self.db.open_table(\"source\")\n\n    return tables\n</code></pre>"},{"location":"api_reference/datasets/builders/folders/base/","title":"base","text":""},{"location":"api_reference/datasets/builders/folders/base/#pixano.datasets.builders.folders.base","title":"<code>pixano.datasets.builders.folders.base</code>","text":""},{"location":"api_reference/datasets/builders/folders/base/#pixano.datasets.builders.folders.base.FolderBaseBuilder","title":"<code>FolderBaseBuilder(media_dir, library_dir, info, dataset_path, dataset_item=None, use_image_name_as_id=False)</code>","text":"<p>               Bases: <code>DatasetBuilder</code></p> <p>This is a class for building datasets based on a folder structure.</p> The folder structure should be as follows <ul> <li>source_dir/{split}/{item}.{ext}</li> <li>source_dir/{split}/metadata.jsonl</li> </ul> <p>The metadata file should be a jsonl file with the following format: <pre><code>    [\n        {\n            \"item\": \"item1\",\n            \"metadata1\": \"value1\",\n            \"metadata2\": \"value2\",\n            ...\n            \"entities\": {\n                \"attr1\": [val1, val2, ...],\n                \"attr2\": [val1, val2, ...],\n                ...\n            }\n        },\n        {\n            \"item\": \"item2\",\n            \"metadata1\": \"value1\",\n            \"metadata2\": \"value2\",\n            ...\n            \"entities\": {\n                \"attr1\": [val1, val2, ...],\n                \"attr2\": [val1, val2, ...],\n                ...\n            }\n        },\n        ...\n    ]\n</code></pre></p> Note <p>Only one view is supported in folder builders. If you give a list of images, it will be put in a mosaic.</p> <p>Attributes:</p> Name Type Description <code>source_dir</code> <p>The source directory for the dataset.</p> <code>view_name</code> <p>The name of the view schema.</p> <code>view_schema</code> <p>The schema of the view.</p> <code>entity_name</code> <p>The name of the entities schema.</p> <code>entity_schema</code> <p>The schema of the entities.</p> <code>METADATA_FILENAME</code> <code>str</code> <p>The metadata filename.</p> <code>EXTENSIONS</code> <code>list[str]</code> <p>The list of supported extensions.</p> <code>WORKSPACE_TYPE</code> <p>The workspace type of the dataset. Subclass should override this attribute if workspace is known.</p> <p>Parameters:</p> Name Type Description Default <code>media_dir</code> <code>Path | str</code> <p>The global media directory.</p> required <code>library_dir</code> <code>Path | str</code> <p>The global directory for Pixano datasets library.</p> required <code>dataset_item</code> <code>type[DatasetItem] | None</code> <p>The dataset item schema.</p> <code>None</code> <code>info</code> <code>DatasetInfo</code> <p>User informations (name, description, ...) for the dataset.</p> required <code>dataset_path</code> <code>Path | str</code> <p>Path to dataset, relative to media_dir.</p> required <code>use_image_name_as_id</code> <code>bool</code> <p>If True, use image base name as image id.                   Images MUST have unique base names.                   When no metadata file exists, also use it as item id,                   else, use 'item_#'                   This allows to reuse image embeddings after dataset overwrite.</p> <code>False</code> Source code in <code>pixano/datasets/builders/folders/base.py</code> <pre><code>def __init__(\n    self,\n    media_dir: Path | str,\n    library_dir: Path | str,\n    info: DatasetInfo,\n    dataset_path: Path | str,\n    dataset_item: type[DatasetItem] | None = None,\n    use_image_name_as_id: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the `FolderBaseBuilder`.\n\n    Args:\n        media_dir: The global media directory.\n        library_dir: The global directory for Pixano datasets library.\n        dataset_item: The dataset item schema.\n        info: User informations (name, description, ...) for the dataset.\n        dataset_path: Path to dataset, relative to media_dir.\n        use_image_name_as_id: If True, use image base name as image id.\n                              Images MUST have unique base names.\n                              When no metadata file exists, also use it as item id,\n                              else, use 'item_#'\n                              This allows to reuse image embeddings after dataset overwrite.\n    \"\"\"\n    info.workspace = self.WORKSPACE_TYPE\n    if self.DEFAULT_SCHEMA is not None and dataset_item is None:\n        dataset_item = self.DEFAULT_SCHEMA\n    if dataset_item is None:\n        raise ValueError(\"A schema is required.\")\n\n    self.use_image_name_as_id = use_image_name_as_id\n\n    self.media_dir = Path(media_dir)\n    dataset_path = Path(dataset_path)\n    self.source_dir = self.media_dir / dataset_path\n    if not self.source_dir.is_dir():\n        raise ValueError(\"A source path (media_dir / dataset_path) is required.\")\n\n    target_dir = Path(library_dir) / \"_\".join(dataset_path.parts)\n    super().__init__(target_dir=target_dir, dataset_item=dataset_item, info=info)\n\n    self.views_schema: dict[str, type[View]] = {}\n    self.entities_schema: dict[str, type[Entity]] = {}\n    self.annotations_schema: dict[str, type[Annotation]] = {}\n\n    for k, s in self.schemas.items():\n        if is_view(s):\n            self.views_schema.update({k: s})\n        elif is_entity(s):\n            self.entities_schema.update({k: s})\n        elif is_annotation(s):\n            self.annotations_schema.update({k: s})\n    if not self.views_schema or not self.entities_schema:\n        raise ValueError(\"At least one View and one Entity schema must be defined in the schemas argument.\")\n\n    # TODO - allow multiview in base FolderBuilder\n    if len(self.views_schema) &gt; 1:\n        raise ValueError(\"Only one view schema is supported in folder based builders.\")\n</code></pre>"},{"location":"api_reference/datasets/builders/folders/base/#pixano.datasets.builders.folders.base.FolderBaseBuilder.generate_data","title":"<code>generate_data()</code>","text":"<p>Generate data from the source directory.</p> <p>Returns:</p> Type Description <code>Iterator[dict[str, BaseSchema | list[BaseSchema]]]</code> <p>An iterator over the data following the dataset schemas.</p> Source code in <code>pixano/datasets/builders/folders/base.py</code> <pre><code>def generate_data(\n    self,\n) -&gt; Iterator[dict[str, BaseSchema | list[BaseSchema]]]:\n    \"\"\"Generate data from the source directory.\n\n    Returns:\n        An iterator over the data following the dataset schemas.\n    \"\"\"\n    self.source_id = self.add_source(\"Builder\", SourceKind.OTHER)\n    for split in self.source_dir.glob(\"*\"):\n        if not split.is_dir() or split.name.startswith(\".\"):\n            continue\n\n        try:\n            dataset_pieces = self._read_metadata(split / self.METADATA_FILENAME)\n        except FileNotFoundError:\n            dataset_pieces = None\n\n        if dataset_pieces is None:\n            for view_file in sorted(split.glob(\"**/*\")):\n                # only consider {split}/**/{item}.{ext} files\n                if not view_file.is_file() or view_file.suffix not in self.EXTENSIONS:\n                    continue\n                # create item with default values for custom fields\n                custom_item_metadata = self._build_default_custom_metadata_item()\n                custom_item_metadata[\"id\"] = view_file.stem\n                custom_item_metadata[\"split\"] = split.name\n                item = self._create_item(**custom_item_metadata)\n                # create view\n                view_name_nojsonl, view_schema_nojsonl = list(self.views_schema.items())[0]  # only one view\n                view = self._create_view(item, view_file, view_schema_nojsonl)\n                yield {\n                    self.item_schema_name: item,\n                    view_name_nojsonl: view,\n                }\n                # if schema contain a Conversation, add one\n                for entity_name, entity_schema_nojsonl in self.entities_schema.items():\n                    if entity_schema_nojsonl is not None and is_conversation(entity_schema_nojsonl):\n                        default_view_ref = ViewRef(id=view.id, name=view_name_nojsonl)\n                        conversation = create_conversation(\n                            id=shortuuid.uuid(),\n                            kind=\"vqa\",\n                            item_ref=ItemRef(id=item.id),\n                            view_ref=default_view_ref,\n                        )\n                        yield {\"conversations\": conversation}\n\n            continue\n\n        for i, dataset_piece in enumerate(dataset_pieces):\n            item_metadata = {}\n            for k in dataset_piece.keys():\n                if (\n                    k not in self.views_schema\n                    and k not in self.entities_schema\n                    and k not in self.annotations_schema\n                ):\n                    item_metadata.update({k: dataset_piece.get(k, None)})\n            for k in item_metadata.keys():\n                dataset_piece.pop(k, None)\n\n            # create item\n            if \"id\" not in item_metadata:\n                item_metadata[\"id\"] = f\"item_{split.name}_{i}\" if self.use_image_name_as_id else shortuuid.uuid()\n            if \"split\" not in item_metadata:\n                item_metadata[\"split\"] = split.name\n            item = self._create_item(**item_metadata)\n\n            # create view\n            views_data: list[tuple[str, View]] = []\n            for k, v in dataset_piece.items():\n                if k in self.views_schema:\n                    view_name = k\n                    view_schema = self.views_schema.get(view_name)\n                    if view_schema is not None:\n                        if isinstance(v, list):\n                            if len(v) == 0:\n                                continue\n                            if len(v) &gt; 1:\n                                # create a mosaic from item images\n                                mosaic_file = mosaic(self.source_dir, split.name, v, view_name)\n                                view_file = self.source_dir / mosaic_file\n                                if not view_file.is_file():  # no split path in metadata.jsonl\n                                    view_file = self.source_dir / split.name / mosaic_file\n                            else:\n                                view_file = self.source_dir / Path(v[0])\n                                if not view_file.is_file():  # no split path in metadata.jsonl\n                                    view_file = self.source_dir / split.name / Path(v[0])\n                            if view_file.is_file() and view_file.suffix in self.EXTENSIONS:\n                                view = self._create_view(item, view_file, view_schema)\n                                views_data.append((view_name, view))\n                        else:\n                            view_file = self.source_dir / (\n                                Path(v) if split.name == Path(v).parts[0] else Path(split.name) / Path(v)\n                            )\n                            if view_file.is_file() and view_file.suffix in self.EXTENSIONS:\n                                view = self._create_view(item, view_file, view_schema)\n                                views_data.append((view_name, view))\n\n            all_entities_data: dict[str, list[Entity]] = defaultdict(list)\n            all_annotations_data: dict[str, list[Annotation]] = defaultdict(list)\n            for k, v in dataset_piece.items():\n                if k in self.entities_schema and v is not None:\n                    entity_name = k\n                    raw_entities_data = v\n                    entity_schema = self.entities_schema.get(entity_name)\n                    if entity_schema is not None:\n                        if is_conversation(entity_schema):\n                            entities_data, annotations_data = self._create_vqa_entities(\n                                item, views_data, entity_name, entity_schema, raw_entities_data\n                            )\n                        else:  # classic entity\n                            entities_data, annotations_data = self._create_objects_entities(\n                                item, views_data, entity_name, entity_schema, raw_entities_data\n                            )\n\n                        for name, entities in entities_data.items():\n                            all_entities_data[name].extend(entities)\n\n                        for name, annotations in annotations_data.items():\n                            all_annotations_data[name].extend(annotations)\n\n            yield {self.item_schema_name: item}\n            for view_name, view in views_data:\n                yield {view_name: view}\n\n            if all_entities_data is None:\n                continue\n\n            yield all_entities_data\n            yield all_annotations_data\n</code></pre>"},{"location":"api_reference/datasets/builders/folders/image/","title":"image","text":""},{"location":"api_reference/datasets/builders/folders/image/#pixano.datasets.builders.folders.image","title":"<code>pixano.datasets.builders.folders.image</code>","text":""},{"location":"api_reference/datasets/builders/folders/image/#pixano.datasets.builders.folders.image.ImageFolderBuilder","title":"<code>ImageFolderBuilder(media_dir, library_dir, info, dataset_path, dataset_item=None, use_image_name_as_id=False)</code>","text":"<p>               Bases: <code>FolderBaseBuilder</code></p> <p>Builder for image datasets stored in a folder.</p> Source code in <code>pixano/datasets/builders/folders/base.py</code> <pre><code>def __init__(\n    self,\n    media_dir: Path | str,\n    library_dir: Path | str,\n    info: DatasetInfo,\n    dataset_path: Path | str,\n    dataset_item: type[DatasetItem] | None = None,\n    use_image_name_as_id: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the `FolderBaseBuilder`.\n\n    Args:\n        media_dir: The global media directory.\n        library_dir: The global directory for Pixano datasets library.\n        dataset_item: The dataset item schema.\n        info: User informations (name, description, ...) for the dataset.\n        dataset_path: Path to dataset, relative to media_dir.\n        use_image_name_as_id: If True, use image base name as image id.\n                              Images MUST have unique base names.\n                              When no metadata file exists, also use it as item id,\n                              else, use 'item_#'\n                              This allows to reuse image embeddings after dataset overwrite.\n    \"\"\"\n    info.workspace = self.WORKSPACE_TYPE\n    if self.DEFAULT_SCHEMA is not None and dataset_item is None:\n        dataset_item = self.DEFAULT_SCHEMA\n    if dataset_item is None:\n        raise ValueError(\"A schema is required.\")\n\n    self.use_image_name_as_id = use_image_name_as_id\n\n    self.media_dir = Path(media_dir)\n    dataset_path = Path(dataset_path)\n    self.source_dir = self.media_dir / dataset_path\n    if not self.source_dir.is_dir():\n        raise ValueError(\"A source path (media_dir / dataset_path) is required.\")\n\n    target_dir = Path(library_dir) / \"_\".join(dataset_path.parts)\n    super().__init__(target_dir=target_dir, dataset_item=dataset_item, info=info)\n\n    self.views_schema: dict[str, type[View]] = {}\n    self.entities_schema: dict[str, type[Entity]] = {}\n    self.annotations_schema: dict[str, type[Annotation]] = {}\n\n    for k, s in self.schemas.items():\n        if is_view(s):\n            self.views_schema.update({k: s})\n        elif is_entity(s):\n            self.entities_schema.update({k: s})\n        elif is_annotation(s):\n            self.annotations_schema.update({k: s})\n    if not self.views_schema or not self.entities_schema:\n        raise ValueError(\"At least one View and one Entity schema must be defined in the schemas argument.\")\n\n    # TODO - allow multiview in base FolderBuilder\n    if len(self.views_schema) &gt; 1:\n        raise ValueError(\"Only one view schema is supported in folder based builders.\")\n</code></pre>"},{"location":"api_reference/datasets/builders/folders/video/","title":"video","text":""},{"location":"api_reference/datasets/builders/folders/video/#pixano.datasets.builders.folders.video","title":"<code>pixano.datasets.builders.folders.video</code>","text":""},{"location":"api_reference/datasets/builders/folders/video/#pixano.datasets.builders.folders.video.VideoFolderBuilder","title":"<code>VideoFolderBuilder(media_dir, library_dir, info, dataset_path, dataset_item=None, use_image_name_as_id=False)</code>","text":"<p>               Bases: <code>FolderBaseBuilder</code></p> <p>Builder for video datasets stored in a folder.</p> Source code in <code>pixano/datasets/builders/folders/base.py</code> <pre><code>def __init__(\n    self,\n    media_dir: Path | str,\n    library_dir: Path | str,\n    info: DatasetInfo,\n    dataset_path: Path | str,\n    dataset_item: type[DatasetItem] | None = None,\n    use_image_name_as_id: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the `FolderBaseBuilder`.\n\n    Args:\n        media_dir: The global media directory.\n        library_dir: The global directory for Pixano datasets library.\n        dataset_item: The dataset item schema.\n        info: User informations (name, description, ...) for the dataset.\n        dataset_path: Path to dataset, relative to media_dir.\n        use_image_name_as_id: If True, use image base name as image id.\n                              Images MUST have unique base names.\n                              When no metadata file exists, also use it as item id,\n                              else, use 'item_#'\n                              This allows to reuse image embeddings after dataset overwrite.\n    \"\"\"\n    info.workspace = self.WORKSPACE_TYPE\n    if self.DEFAULT_SCHEMA is not None and dataset_item is None:\n        dataset_item = self.DEFAULT_SCHEMA\n    if dataset_item is None:\n        raise ValueError(\"A schema is required.\")\n\n    self.use_image_name_as_id = use_image_name_as_id\n\n    self.media_dir = Path(media_dir)\n    dataset_path = Path(dataset_path)\n    self.source_dir = self.media_dir / dataset_path\n    if not self.source_dir.is_dir():\n        raise ValueError(\"A source path (media_dir / dataset_path) is required.\")\n\n    target_dir = Path(library_dir) / \"_\".join(dataset_path.parts)\n    super().__init__(target_dir=target_dir, dataset_item=dataset_item, info=info)\n\n    self.views_schema: dict[str, type[View]] = {}\n    self.entities_schema: dict[str, type[Entity]] = {}\n    self.annotations_schema: dict[str, type[Annotation]] = {}\n\n    for k, s in self.schemas.items():\n        if is_view(s):\n            self.views_schema.update({k: s})\n        elif is_entity(s):\n            self.entities_schema.update({k: s})\n        elif is_annotation(s):\n            self.annotations_schema.update({k: s})\n    if not self.views_schema or not self.entities_schema:\n        raise ValueError(\"At least one View and one Entity schema must be defined in the schemas argument.\")\n\n    # TODO - allow multiview in base FolderBuilder\n    if len(self.views_schema) &gt; 1:\n        raise ValueError(\"Only one view schema is supported in folder based builders.\")\n</code></pre>"},{"location":"api_reference/datasets/builders/folders/vqa/","title":"vqa","text":""},{"location":"api_reference/datasets/builders/folders/vqa/#pixano.datasets.builders.folders.vqa","title":"<code>pixano.datasets.builders.folders.vqa</code>","text":""},{"location":"api_reference/datasets/builders/folders/vqa/#pixano.datasets.builders.folders.vqa.VQAFolderBuilder","title":"<code>VQAFolderBuilder(media_dir, library_dir, info, dataset_path, dataset_item=None, use_image_name_as_id=False)</code>","text":"<p>               Bases: <code>ImageFolderBuilder</code></p> <p>Builder for vqa datasets stored in a folder.</p> Source code in <code>pixano/datasets/builders/folders/base.py</code> <pre><code>def __init__(\n    self,\n    media_dir: Path | str,\n    library_dir: Path | str,\n    info: DatasetInfo,\n    dataset_path: Path | str,\n    dataset_item: type[DatasetItem] | None = None,\n    use_image_name_as_id: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the `FolderBaseBuilder`.\n\n    Args:\n        media_dir: The global media directory.\n        library_dir: The global directory for Pixano datasets library.\n        dataset_item: The dataset item schema.\n        info: User informations (name, description, ...) for the dataset.\n        dataset_path: Path to dataset, relative to media_dir.\n        use_image_name_as_id: If True, use image base name as image id.\n                              Images MUST have unique base names.\n                              When no metadata file exists, also use it as item id,\n                              else, use 'item_#'\n                              This allows to reuse image embeddings after dataset overwrite.\n    \"\"\"\n    info.workspace = self.WORKSPACE_TYPE\n    if self.DEFAULT_SCHEMA is not None and dataset_item is None:\n        dataset_item = self.DEFAULT_SCHEMA\n    if dataset_item is None:\n        raise ValueError(\"A schema is required.\")\n\n    self.use_image_name_as_id = use_image_name_as_id\n\n    self.media_dir = Path(media_dir)\n    dataset_path = Path(dataset_path)\n    self.source_dir = self.media_dir / dataset_path\n    if not self.source_dir.is_dir():\n        raise ValueError(\"A source path (media_dir / dataset_path) is required.\")\n\n    target_dir = Path(library_dir) / \"_\".join(dataset_path.parts)\n    super().__init__(target_dir=target_dir, dataset_item=dataset_item, info=info)\n\n    self.views_schema: dict[str, type[View]] = {}\n    self.entities_schema: dict[str, type[Entity]] = {}\n    self.annotations_schema: dict[str, type[Annotation]] = {}\n\n    for k, s in self.schemas.items():\n        if is_view(s):\n            self.views_schema.update({k: s})\n        elif is_entity(s):\n            self.entities_schema.update({k: s})\n        elif is_annotation(s):\n            self.annotations_schema.update({k: s})\n    if not self.views_schema or not self.entities_schema:\n        raise ValueError(\"At least one View and one Entity schema must be defined in the schemas argument.\")\n\n    # TODO - allow multiview in base FolderBuilder\n    if len(self.views_schema) &gt; 1:\n        raise ValueError(\"Only one view schema is supported in folder based builders.\")\n</code></pre>"},{"location":"api_reference/datasets/exporters/coco_dataset_exporter/","title":"coco_dataset_exporter","text":""},{"location":"api_reference/datasets/exporters/coco_dataset_exporter/#pixano.datasets.exporters.coco_dataset_exporter","title":"<code>pixano.datasets.exporters.coco_dataset_exporter</code>","text":""},{"location":"api_reference/datasets/exporters/coco_dataset_exporter/#pixano.datasets.exporters.coco_dataset_exporter.COCODatasetExporter","title":"<code>COCODatasetExporter(dataset, export_dir, overwrite=False, category_format='coco91', custom_category_dict=None)</code>","text":"<p>               Bases: <code>DatasetExporter</code></p> <p>Default JSON dataset exporter.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to be exported.</p> required <code>export_dir</code> <code>str | Path</code> <p>The directory where the exported files will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing directory.</p> <code>False</code> <code>category_format</code> <code>str</code> <p>Category format for name to ID conversion (\"coco91\", \"coco80\", \"voc\").</p> <code>'coco91'</code> <code>custom_category_dict</code> <code>dict[str, int] | None</code> <p>Custom category dictionary for name to ID conversion (supersedes category_format).</p> <code>None</code> Source code in <code>pixano/datasets/exporters/coco_dataset_exporter.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset,\n    export_dir: str | Path,\n    overwrite: bool = False,\n    category_format: str = \"coco91\",\n    custom_category_dict: dict[str, int] | None = None,\n):\n    \"\"\"Initialize a new instance of the DatasetExporter class.\n\n    Args:\n        dataset: The dataset to be exported.\n        export_dir: The directory where the exported files will be saved.\n        overwrite: Whether to overwrite existing directory.\n        category_format: Category format for name to ID conversion (\"coco91\", \"coco80\", \"voc\").\n        custom_category_dict: Custom category dictionary for name to ID conversion (supersedes category_format).\n    \"\"\"\n    self.dataset = dataset\n    self.export_dir = Path(export_dir)\n    self._overwrite = overwrite\n    self.category_dict = (\n        custom_category_dict if custom_category_dict is not None else CATEGORY_IDS[category_format]\n    )\n</code></pre>"},{"location":"api_reference/datasets/exporters/coco_dataset_exporter/#pixano.datasets.exporters.coco_dataset_exporter.COCODatasetExporter.export_dataset_item","title":"<code>export_dataset_item(export_data, dataset_item)</code>","text":"<p>Store the dataset item in the <code>export_data</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>export_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the data to be exported.</p> required <code>dataset_item</code> <code>DatasetItem</code> <p>The dataset item to be exported.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the data to be exported.</p> Source code in <code>pixano/datasets/exporters/coco_dataset_exporter.py</code> <pre><code>def export_dataset_item(self, export_data: dict[str, Any], dataset_item: DatasetItem) -&gt; dict[str, Any]:\n    \"\"\"Store the dataset item in the `export_data` dictionary.\n\n    Args:\n        export_data: A dictionary containing the data to be exported.\n        dataset_item: The dataset item to be exported.\n\n    Returns:\n        A dictionary containing the data to be exported.\n    \"\"\"\n    data: dict[str, BaseSchema | list[BaseSchema] | None] = dataset_item.to_schemas_data(self.dataset.schema)\n    # Keep annotations in a dictionary to merge BBox, CompressedRLE, and Category before adding to export_data\n    anns = {s[\"id\"]: s for s in export_data[\"annotations\"]}\n    for schema_name, schema_data in data.items():\n        if schema_data:\n            schema_data = schema_data if isinstance(schema_data, list) else [schema_data]\n            group = schema_to_group(schema_data[0])\n            if group == SchemaGroup.VIEW:\n                export_data[\"images\"].extend(\n                    [coco_image(s, schema_name) for s in schema_data if isinstance(s, Image)]\n                )\n            elif group == SchemaGroup.ENTITY:\n                for schema in schema_data:\n                    if isinstance(schema, Entity) and hasattr(schema, \"category\"):\n                        ann_id = f\"{schema.view_ref.id}_{schema.id}\"\n                        anns[ann_id] = coco_annotation(\n                            ann=schema,\n                            existing_coco_ann=anns[ann_id] if ann_id in anns.keys() else None,\n                            category_dict=self.category_dict,\n                        )\n            elif group == SchemaGroup.ANNOTATION:\n                for schema in schema_data:\n                    if isinstance(schema, BBox | CompressedRLE):\n                        ann_id = f\"{schema.view_ref.id}_{schema.entity_ref.id}\"\n                        anns[ann_id] = coco_annotation(\n                            ann=schema,\n                            existing_coco_ann=anns[ann_id] if ann_id in anns.keys() else None,\n                        )\n    export_data[\"annotations\"] = list(anns.values())\n    return export_data\n</code></pre>"},{"location":"api_reference/datasets/exporters/coco_dataset_exporter/#pixano.datasets.exporters.coco_dataset_exporter.COCODatasetExporter.initialize_export_data","title":"<code>initialize_export_data(info, sources)</code>","text":"<p>Initialize the dictionary or list of dictionaries to be exported.</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>DatasetInfo</code> <p>The dataset information.</p> required <code>sources</code> <code>list[Source]</code> <p>The list of sources.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the data to be exported.</p> Source code in <code>pixano/datasets/exporters/coco_dataset_exporter.py</code> <pre><code>def initialize_export_data(self, info: DatasetInfo, sources: list[Source]) -&gt; dict[str, Any]:\n    \"\"\"Initialize the dictionary or list of dictionaries to be exported.\n\n    Args:\n        info: The dataset information.\n        sources: The list of sources.\n\n    Returns:\n        A dictionary containing the data to be exported.\n    \"\"\"\n    export_data = {\n        \"info\": {\n            \"id\": info.id,\n            \"name\": info.name,\n            \"year\": datetime.now().year,\n            \"version\": datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n            \"description\": info.description,\n            \"contributor\": \"\",\n            \"url\": \"\",\n            \"date_created\": datetime.now(),\n        },\n        \"licenses\": [\n            {\n                \"id\": 0,\n                \"name\": \"Exported from Pixano\",\n                \"url\": \"\",\n            }\n        ],\n        \"images\": [],\n        \"annotations\": [],\n        \"categories\": [],\n    }\n    return export_data\n</code></pre>"},{"location":"api_reference/datasets/exporters/coco_dataset_exporter/#pixano.datasets.exporters.coco_dataset_exporter.COCODatasetExporter.save_data","title":"<code>save_data(export_data, split, file_name, file_num)</code>","text":"<p>Save data to the specified directory.</p> The saved directory has the following structure <p>export_dir/{split}{file_name}_0.json           /...           /{split}{file_num}.json           /...           /{split}_n.json</p> <p>Parameters:</p> Name Type Description Default <code>export_data</code> <code>dict[str, Any]</code> <p>The dictionary containing the data to be saved.</p> required <code>split</code> <code>str</code> <p>The split of the dataset item being saved.</p> required <code>file_name</code> <code>str</code> <p>The name of the file to save the data in.</p> required <code>file_num</code> <code>int</code> <p>The number of the file to save the data in.</p> required Source code in <code>pixano/datasets/exporters/coco_dataset_exporter.py</code> <pre><code>def save_data(self, export_data: dict[str, Any], split: str, file_name: str, file_num: int) -&gt; None:\n    \"\"\"Save data to the specified directory.\n\n    The saved directory has the following structure:\n        export_dir/{split}_{file_name}_0.json\n                  /...\n                  /{split}_{file_name}_{file_num}.json\n                  /...\n                  /{split}_{file_name}_n.json\n\n\n    Args:\n        export_data: The dictionary containing the data to be saved.\n        split: The split of the dataset item being saved.\n        file_name: The name of the file to save the data in.\n        file_num: The number of the file to save the data in.\n    \"\"\"\n    json_path = self.export_dir / f\"{split}_{file_name}_{file_num}.json\"\n    json_path.write_text(json.dumps(jsonable_encoder(export_data), indent=4), encoding=\"utf-8\")\n</code></pre>"},{"location":"api_reference/datasets/exporters/coco_dataset_exporter/#pixano.datasets.exporters.coco_dataset_exporter.coco_annotation","title":"<code>coco_annotation(ann, existing_coco_ann=None, category_dict=None)</code>","text":"<p>Return annotation in COCO format.</p> <p>Parameters:</p> Name Type Description Default <code>ann</code> <code>BBox | CompressedRLE | Entity</code> <p>Annotation</p> required <code>existing_coco_ann</code> <code>dict[str, Any] | None</code> <p>Existing annotation in COCO format to complete</p> <code>None</code> <code>category_dict</code> <code>dict[str, int] | None</code> <p>Category dictonary for name to ID conversion</p> <code>None</code> <p>Returns:     Annotation in COCO format</p> Source code in <code>pixano/datasets/exporters/coco_dataset_exporter.py</code> <pre><code>def coco_annotation(\n    ann: BBox | CompressedRLE | Entity,\n    existing_coco_ann: dict[str, Any] | None = None,\n    category_dict: dict[str, int] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Return annotation in COCO format.\n\n    Args:\n        ann: Annotation\n        existing_coco_ann: Existing annotation in COCO format to complete\n        category_dict: Category dictonary for name to ID conversion\n    Returns:\n        Annotation in COCO format\n    \"\"\"\n    # Load the existing COCO format or initialize a new one\n    coco_ann = (\n        existing_coco_ann\n        if existing_coco_ann is not None\n        else {\n            \"id\": f\"{ann.view_ref.id}_{ann.id if isinstance(ann, Entity) else ann.entity_ref.id}\",\n            \"image_id\": ann.view_ref.id,\n            \"category_id\": None,\n            \"category_name\": None,\n            \"segmentation\": None,\n            \"area\": None,\n            \"bbox\": None,\n            \"confidence\": None,\n            \"iscrowd\": 0,\n            \"pixano_entity_id\": ann.id if isinstance(ann, Entity) else ann.entity_ref.id,\n        }\n    )\n    # Add the specific elements from the annotation into the COCO format\n    if isinstance(ann, Entity):\n        category_name = str(ann.category).strip().lower()\n        coco_ann[\"category_name\"] = category_name\n        coco_ann[\"category_id\"] = (\n            category_dict[category_name] if category_dict is not None and category_name in category_dict else None\n        )\n    else:\n        if isinstance(ann, BBox):\n            coco_ann[\"pixano_bbox_id\"] = ann.id\n            coco_ann[\"bbox\"] = ann.xywh_coords\n            coco_ann[\"confidence\"] = ann.confidence\n        elif isinstance(ann, CompressedRLE):\n            coco_ann[\"pixano_segmentation_id\"] = ann.id\n            coco_ann[\"segmentation\"] = ann.to_polygons()\n            coco_ann[\"area\"] = ann.area\n    return coco_ann\n</code></pre>"},{"location":"api_reference/datasets/exporters/coco_dataset_exporter/#pixano.datasets.exporters.coco_dataset_exporter.coco_image","title":"<code>coco_image(image, view)</code>","text":"<p>Return image in COCO format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image</p> required <code>view</code> <code>str</code> <p>Image view</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Image in COCO format</p> Source code in <code>pixano/datasets/exporters/coco_dataset_exporter.py</code> <pre><code>def coco_image(image: Image, view: str) -&gt; dict[str, Any]:\n    \"\"\"Return image in COCO format.\n\n    Args:\n        image: Image\n        view: Image view\n\n    Returns:\n        Image in COCO format\n    \"\"\"\n    coco_img = {\n        \"id\": image.id,\n        \"view\": view,\n        \"width\": image.width,\n        \"height\": image.height,\n        \"file_name\": image.url,\n        \"license\": 0,\n        \"date_captured\": image.created_at,\n    }\n    if isinstance(image, SequenceFrame):\n        coco_img[\"timestamp\"] = image.timestamp\n        coco_img[\"frame_index\"] = image.frame_index\n    return coco_img\n</code></pre>"},{"location":"api_reference/datasets/exporters/dataset_exporter/","title":"dataset_exporter","text":""},{"location":"api_reference/datasets/exporters/dataset_exporter/#pixano.datasets.exporters.dataset_exporter","title":"<code>pixano.datasets.exporters.dataset_exporter</code>","text":""},{"location":"api_reference/datasets/exporters/dataset_exporter/#pixano.datasets.exporters.dataset_exporter.DatasetExporter","title":"<code>DatasetExporter(dataset, export_dir, overwrite=False)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for dataset exporters.</p> <p>To export a dataset, you need to implement this class and provide an implementation for the abstract methods.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <p>The dataset to be exported.</p> <code>export_dir</code> <p>The directory where the exported files will be saved.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to be exported.</p> required <code>export_dir</code> <code>str | Path</code> <p>The directory where the exported files will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing directory.</p> <code>False</code> Source code in <code>pixano/datasets/exporters/dataset_exporter.py</code> <pre><code>def __init__(self, dataset: Dataset, export_dir: str | Path, overwrite: bool = False):\n    \"\"\"Initialize a new instance of the DatasetExporter class.\n\n    Args:\n        dataset: The dataset to be exported.\n        export_dir: The directory where the exported files will be saved.\n        overwrite: Whether to overwrite existing directory.\n    \"\"\"\n    self.dataset = dataset\n    self.export_dir = Path(export_dir)\n    self._overwrite = overwrite\n</code></pre>"},{"location":"api_reference/datasets/exporters/dataset_exporter/#pixano.datasets.exporters.dataset_exporter.DatasetExporter.export","title":"<code>export(file_name='pixano_export', items_per_file=None, batch_size=None)</code>","text":"<p>Export the dataset to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the exported dataset.</p> <code>'pixano_export'</code> <code>items_per_file</code> <code>int | None</code> <p>The number of items to export per file. If not specified, all items will be exported in a single file.</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>The size of each batch when exporting data. If not specified, all data will be exported at once.</p> <code>None</code> Source code in <code>pixano/datasets/exporters/dataset_exporter.py</code> <pre><code>def export(\n    self, file_name: str = \"pixano_export\", items_per_file: int | None = None, batch_size: int | None = None\n) -&gt; None:\n    \"\"\"Export the dataset to the specified directory.\n\n    Args:\n        file_name: The name of the exported dataset.\n        items_per_file: The number of items to export per file. If not specified, all items will be exported in a\n            single file.\n        batch_size: The size of each batch when exporting data. If not specified, all data will be exported at\n            once.\n    \"\"\"\n    self.export_dir.mkdir(exist_ok=self._overwrite)\n\n    if items_per_file is not None and (not isinstance(items_per_file, int) or items_per_file &lt;= 0):\n        raise ValueError(\"Items per file must be a positive integer\")\n    if batch_size is not None and (not isinstance(batch_size, int) or batch_size &lt;= 0):\n        raise ValueError(\"Batch size must be a positive integer\")\n\n    info = self.dataset.info\n    sources = self.dataset.get_data(SchemaGroup.SOURCE.value)\n\n    item_table = self.dataset.open_table(SchemaGroup.ITEM.value)\n    item_table_lance = item_table.to_lance()  # noqa: F841\n    SQL_QUERY = \"SELECT split, COUNT(*) FROM item_table_lance GROUP BY split\"\n    arrow_results: pa.Table = duckdb.query(SQL_QUERY).to_arrow_table()\n    splits: dict[str, list[Any]] = arrow_results.to_pydict()\n\n    logger.info(f\"Exporting dataset {self.dataset.info.name} to {self.export_dir}.\")\n    for num_split_items, split in zip(splits[\"count_star()\"], splits[\"split\"]):\n        if items_per_file is None:\n            split_items_per_file = num_split_items\n        else:\n            split_items_per_file = items_per_file\n        if batch_size is None:\n            batch_size = num_split_items\n\n        export_data = self.initialize_export_data(info, sources)\n\n        file_num = 0  # Number of files exported so far\n        cur_items_exported = 0  # Number of items exported so far\n        logger.info(\n            f\"Exporting split {split} of dataset {self.dataset.info.name}: number of items {num_split_items}\"\n        )\n\n        for _ in tqdm.tqdm(\n            range(0, num_split_items, batch_size),\n            desc=f\"Exporting data split {split} of dataset {self.dataset.info.name}\",\n        ):\n            item_ids = list(\n                TableQueryBuilder(item_table)\n                .select(\"id\")\n                .where(f\"split='{split}'\")\n                .limit(batch_size)\n                .offset(cur_items_exported)\n                .to_polars()[\"id\"]\n            )\n            dataset_items: list[DatasetItem] = self.dataset.get_dataset_items(ids=item_ids)\n            for dataset_item in dataset_items:\n                cur_items_exported += 1\n\n                export_data = self.export_dataset_item(export_data, dataset_item)\n\n                if (\n                    cur_items_exported == num_split_items or cur_items_exported % split_items_per_file == 0\n                ):  # Export every n items\n                    self.save_data(export_data, split, file_name, file_num)\n\n                    file_num += 1\n                    if cur_items_exported != num_split_items:\n                        export_data = self.initialize_export_data(info, sources)\n\n        logger.info(\n            f\"Completed export split {split} of dataset {self.dataset.info.name} in {file_num} file\"\n            f\"{'s' if file_num &gt; 1 else ''}.\"\n        )\n</code></pre>"},{"location":"api_reference/datasets/exporters/dataset_exporter/#pixano.datasets.exporters.dataset_exporter.DatasetExporter.export_dataset_item","title":"<code>export_dataset_item(export_data, dataset_item)</code>  <code>abstractmethod</code>","text":"<p>Store the dataset item in the data structure to be exported.</p> <p>Parameters:</p> Name Type Description Default <code>export_data</code> <code>Any</code> <p>The data structure to be exported.</p> required <code>dataset_item</code> <code>DatasetItem</code> <p>The dataset item to be exported.</p> required <p>Returns: The data structure to be exported.</p> Source code in <code>pixano/datasets/exporters/dataset_exporter.py</code> <pre><code>@abstractmethod\ndef export_dataset_item(self, export_data: Any, dataset_item: DatasetItem) -&gt; Any:\n    \"\"\"Store the dataset item in the data structure to be exported.\n\n    Args:\n        export_data: The data structure to be exported.\n        dataset_item: The dataset item to be exported.\n\n    Returns: The data structure to be exported.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/datasets/exporters/dataset_exporter/#pixano.datasets.exporters.dataset_exporter.DatasetExporter.initialize_export_data","title":"<code>initialize_export_data(info, sources)</code>  <code>abstractmethod</code>","text":"<p>Initialize the data structure to be exported.</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>DatasetInfo</code> <p>The dataset information.</p> required <code>sources</code> <code>list[Source]</code> <p>The list of sources.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data structure to be exported.</p> Source code in <code>pixano/datasets/exporters/dataset_exporter.py</code> <pre><code>@abstractmethod\ndef initialize_export_data(self, info: DatasetInfo, sources: list[Source]) -&gt; Any:\n    \"\"\"Initialize the data structure to be exported.\n\n    Args:\n        info: The dataset information.\n        sources: The list of sources.\n\n    Returns:\n        The data structure to be exported.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/datasets/exporters/dataset_exporter/#pixano.datasets.exporters.dataset_exporter.DatasetExporter.save_data","title":"<code>save_data(export_data, split, file_name, file_num)</code>  <code>abstractmethod</code>","text":"<p>Save data to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>export_data</code> <code>Any</code> <p>The data structure to be exported.</p> required <code>split</code> <code>str</code> <p>The split of the dataset item being saved.</p> required <code>file_name</code> <code>str</code> <p>The name of the file to save the data in.</p> required <code>file_num</code> <code>int</code> <p>The number of the file to save the data in.</p> required Source code in <code>pixano/datasets/exporters/dataset_exporter.py</code> <pre><code>@abstractmethod\ndef save_data(self, export_data: Any, split: str, file_name: str, file_num: int) -&gt; None:\n    \"\"\"Save data to the specified directory.\n\n    Args:\n        export_data: The data structure to be exported.\n        split: The split of the dataset item being saved.\n        file_name: The name of the file to save the data in.\n        file_num: The number of the file to save the data in.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/datasets/exporters/default_json_dataset_exporter/","title":"default_json_dataset_exporter","text":""},{"location":"api_reference/datasets/exporters/default_json_dataset_exporter/#pixano.datasets.exporters.default_json_dataset_exporter","title":"<code>pixano.datasets.exporters.default_json_dataset_exporter</code>","text":""},{"location":"api_reference/datasets/exporters/default_json_dataset_exporter/#pixano.datasets.exporters.default_json_dataset_exporter.DefaultJSONDatasetExporter","title":"<code>DefaultJSONDatasetExporter(dataset, export_dir, overwrite=False)</code>","text":"<p>               Bases: <code>DatasetExporter</code></p> <p>Default JSON dataset exporter.</p> Source code in <code>pixano/datasets/exporters/dataset_exporter.py</code> <pre><code>def __init__(self, dataset: Dataset, export_dir: str | Path, overwrite: bool = False):\n    \"\"\"Initialize a new instance of the DatasetExporter class.\n\n    Args:\n        dataset: The dataset to be exported.\n        export_dir: The directory where the exported files will be saved.\n        overwrite: Whether to overwrite existing directory.\n    \"\"\"\n    self.dataset = dataset\n    self.export_dir = Path(export_dir)\n    self._overwrite = overwrite\n</code></pre>"},{"location":"api_reference/datasets/exporters/default_json_dataset_exporter/#pixano.datasets.exporters.default_json_dataset_exporter.DefaultJSONDatasetExporter.export_dataset_item","title":"<code>export_dataset_item(export_data, dataset_item)</code>","text":"<p>Store the dataset item in the <code>export_data</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>export_data</code> <code>dict[str, Any]</code> <p>A dictionary containing the data to be exported.</p> required <code>dataset_item</code> <code>DatasetItem</code> <p>The dataset item to be exported.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the data to be exported.</p> Source code in <code>pixano/datasets/exporters/default_json_dataset_exporter.py</code> <pre><code>def export_dataset_item(self, export_data: dict[str, Any], dataset_item: DatasetItem) -&gt; dict[str, Any]:\n    \"\"\"Store the dataset item in the `export_data` dictionary.\n\n    Args:\n        export_data: A dictionary containing the data to be exported.\n        dataset_item: The dataset item to be exported.\n\n    Returns:\n        A dictionary containing the data to be exported.\n    \"\"\"\n    data: dict[str, BaseSchema | list[BaseSchema] | None] = dataset_item.to_schemas_data(self.dataset.schema)\n    for schema_name, schema_data in data.items():\n        if schema_data:\n            schema_data = schema_data if isinstance(schema_data, list) else [schema_data]\n            group = schema_to_group(schema_data[0])\n            if group == SchemaGroup.ITEM:\n                export_data[group_to_str(group, plural=True)].extend(\n                    [s.model_dump(exclude_timestamps=True) for s in schema_data]\n                )\n            else:\n                export_data[group_to_str(group, plural=True)][schema_name].extend(\n                    [s.model_dump(exclude_timestamps=True) for s in schema_data]\n                )\n    return export_data\n</code></pre>"},{"location":"api_reference/datasets/exporters/default_json_dataset_exporter/#pixano.datasets.exporters.default_json_dataset_exporter.DefaultJSONDatasetExporter.initialize_export_data","title":"<code>initialize_export_data(info, sources)</code>","text":"<p>Initialize the dictionary or list of dictionaries to be exported.</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>DatasetInfo</code> <p>The dataset information.</p> required <code>sources</code> <code>list[Source]</code> <p>The list of sources.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the data to be exported.</p> Source code in <code>pixano/datasets/exporters/default_json_dataset_exporter.py</code> <pre><code>def initialize_export_data(self, info: DatasetInfo, sources: list[Source]) -&gt; dict[str, Any]:\n    \"\"\"Initialize the dictionary or list of dictionaries to be exported.\n\n    Args:\n        info: The dataset information.\n        sources: The list of sources.\n\n    Returns:\n        A dictionary containing the data to be exported.\n    \"\"\"\n    export_data = {\"info\": info.model_dump()}\n\n    for group, schemas in self.dataset.schema.groups.items():\n        if group == SchemaGroup.EMBEDDING:\n            continue\n        elif group == SchemaGroup.ITEM:\n            export_data[group_to_str(group, plural=True)] = []\n        else:\n            export_data[group_to_str(group, plural=True)] = {schema: [] for schema in schemas}\n    export_data[group_to_str(SchemaGroup.SOURCE, plural=True)] = [\n        s.model_dump(exclude_timestamps=True) for s in sources\n    ]\n    return export_data\n</code></pre>"},{"location":"api_reference/datasets/exporters/default_json_dataset_exporter/#pixano.datasets.exporters.default_json_dataset_exporter.DefaultJSONDatasetExporter.save_data","title":"<code>save_data(export_data, split, file_name, file_num)</code>","text":"<p>Save data to the specified directory.</p> The saved directory has the following structure <p>export_dir/{split}{file_name}_0.json           /...           /{split}{file_num}.json           /...           /{split}_n.json</p> <p>Parameters:</p> Name Type Description Default <code>export_data</code> <code>dict[str, Any]</code> <p>The dictionary containing the data to be saved.</p> required <code>split</code> <code>str</code> <p>The split of the dataset item being saved.</p> required <code>file_name</code> <code>str</code> <p>The name of the file to save the data in.</p> required <code>file_num</code> <code>int</code> <p>The number of the file to save the data in.</p> required Source code in <code>pixano/datasets/exporters/default_json_dataset_exporter.py</code> <pre><code>def save_data(self, export_data: dict[str, Any], split: str, file_name: str, file_num: int) -&gt; None:\n    \"\"\"Save data to the specified directory.\n\n    The saved directory has the following structure:\n        export_dir/{split}_{file_name}_0.json\n                  /...\n                  /{split}_{file_name}_{file_num}.json\n                  /...\n                  /{split}_{file_name}_n.json\n\n\n    Args:\n        export_data: The dictionary containing the data to be saved.\n        split: The split of the dataset item being saved.\n        file_name: The name of the file to save the data in.\n        file_num: The number of the file to save the data in.\n    \"\"\"\n    json_path = self.export_dir / f\"{split}_{file_name}_{file_num}.json\"\n    json_path.write_text(json.dumps(jsonable_encoder(export_data), indent=4), encoding=\"utf-8\")\n</code></pre>"},{"location":"api_reference/datasets/exporters/default_jsonl_dataset_exporter/","title":"default_jsonl_dataset_exporter","text":""},{"location":"api_reference/datasets/exporters/default_jsonl_dataset_exporter/#pixano.datasets.exporters.default_jsonl_dataset_exporter","title":"<code>pixano.datasets.exporters.default_jsonl_dataset_exporter</code>","text":""},{"location":"api_reference/datasets/exporters/default_jsonl_dataset_exporter/#pixano.datasets.exporters.default_jsonl_dataset_exporter.DefaultJSONLDatasetExporter","title":"<code>DefaultJSONLDatasetExporter(dataset, export_dir, overwrite=False)</code>","text":"<p>               Bases: <code>DatasetExporter</code></p> <p>Default JSON Lines dataset exporter.</p> Source code in <code>pixano/datasets/exporters/dataset_exporter.py</code> <pre><code>def __init__(self, dataset: Dataset, export_dir: str | Path, overwrite: bool = False):\n    \"\"\"Initialize a new instance of the DatasetExporter class.\n\n    Args:\n        dataset: The dataset to be exported.\n        export_dir: The directory where the exported files will be saved.\n        overwrite: Whether to overwrite existing directory.\n    \"\"\"\n    self.dataset = dataset\n    self.export_dir = Path(export_dir)\n    self._overwrite = overwrite\n</code></pre>"},{"location":"api_reference/datasets/exporters/default_jsonl_dataset_exporter/#pixano.datasets.exporters.default_jsonl_dataset_exporter.DefaultJSONLDatasetExporter.export_dataset_item","title":"<code>export_dataset_item(export_data, dataset_item)</code>","text":"<p>Store the dataset item in the <code>export_data</code> list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>export_data</code> <code>list[dict[str, Any]]</code> <p>A list of dictionaries containing the dataset items to be exported.</p> required <code>dataset_item</code> <code>DatasetItem</code> <p>The dataset item to be exported.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries containing the dataset items to be exported.</p> Source code in <code>pixano/datasets/exporters/default_jsonl_dataset_exporter.py</code> <pre><code>def export_dataset_item(\n    self, export_data: list[dict[str, Any]], dataset_item: DatasetItem\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Store the dataset item in the `export_data` list of dictionaries.\n\n    Args:\n        export_data: A list of dictionaries containing the dataset items to be exported.\n        dataset_item: The dataset item to be exported.\n\n    Returns:\n        A list of dictionaries containing the dataset items to be exported.\n    \"\"\"\n    export_data.append(dataset_item.model_dump(exclude_timestamps=True))\n    return export_data\n</code></pre>"},{"location":"api_reference/datasets/exporters/default_jsonl_dataset_exporter/#pixano.datasets.exporters.default_jsonl_dataset_exporter.DefaultJSONLDatasetExporter.initialize_export_data","title":"<code>initialize_export_data(info, sources)</code>","text":"<p>Initialize a list of dictionaries to be exported.</p> <p>The first line contains the following elements: - dataset info - the sources</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>DatasetInfo</code> <p>The dataset information.</p> required <code>sources</code> <code>list[Source]</code> <p>The list of sources.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries containing the data to be exported.</p> Source code in <code>pixano/datasets/exporters/default_jsonl_dataset_exporter.py</code> <pre><code>def initialize_export_data(self, info: DatasetInfo, sources: list[Source]) -&gt; list[dict[str, Any]]:\n    \"\"\"Initialize a list of dictionaries to be exported.\n\n    The first line contains the following elements:\n    - dataset info\n    - the sources\n\n    Args:\n        info: The dataset information.\n        sources: The list of sources.\n\n    Returns:\n        A list of dictionaries containing the data to be exported.\n    \"\"\"\n    export_data = [\n        {\"info\": info.model_dump(), \"sources\": [s.model_dump(exclude_timestamps=True) for s in sources]}\n    ]\n    return export_data\n</code></pre>"},{"location":"api_reference/datasets/exporters/default_jsonl_dataset_exporter/#pixano.datasets.exporters.default_jsonl_dataset_exporter.DefaultJSONLDatasetExporter.save_data","title":"<code>save_data(export_data, split, file_name, file_num)</code>","text":"<p>Save data to the specified directory.</p> The saved directory has the following structure <p>export_dir/{split}{file_name}_0.jsonl           /...           /{split}{file_num}.jsonl           /...           /{split}_n.jsonl</p> <p>Parameters:</p> Name Type Description Default <code>export_data</code> <code>list[dict[str, Any]]</code> <p>The list of dictionaries containing the data to be saved.</p> required <code>split</code> <code>str</code> <p>The split of the dataset item being saved.</p> required <code>file_name</code> <code>str</code> <p>The name of the file to save the data in.</p> required <code>file_num</code> <code>int</code> <p>The number of the file to save the data in.</p> required Source code in <code>pixano/datasets/exporters/default_jsonl_dataset_exporter.py</code> <pre><code>def save_data(self, export_data: list[dict[str, Any]], split: str, file_name: str, file_num: int) -&gt; None:\n    \"\"\"Save data to the specified directory.\n\n    The saved directory has the following structure:\n        export_dir/{split}_{file_name}_0.jsonl\n                  /...\n                  /{split}_{file_name}_{file_num}.jsonl\n                  /...\n                  /{split}_{file_name}_n.jsonl\n\n\n    Args:\n        export_data: The list of dictionaries containing the data to be saved.\n        split: The split of the dataset item being saved.\n        file_name: The name of the file to save the data in.\n        file_num: The number of the file to save the data in.\n    \"\"\"\n    info, data = export_data[0], export_data[1:]\n\n    info_path = self.export_dir / \"info.json\"\n    info_path.write_text(json.dumps(info), encoding=\"utf-8\")\n\n    json_path = self.export_dir / f\"{split}_{file_name}_{file_num}.jsonl\"\n    json_path.write_text(\"\\n\".join([json.dumps(jsonable_encoder(d)) for d in data]), encoding=\"utf-8\")\n</code></pre>"},{"location":"api_reference/datasets/queries/table/","title":"table","text":""},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table","title":"<code>pixano.datasets.queries.table</code>","text":""},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder","title":"<code>TableQueryBuilder(table)</code>","text":"<p>Builder class for querying LanceTables.</p> <p>It supports the select, where, limit, offset, and order_by clauses: - The select clause can be used to select specific columns from the table. If not provided, all columns     are selected. - The where clause can be used to filter the rows of the table. - The limit clause can be used to limit the number of rows returned. - The offset clause can be used to skip the first n rows. - The order_by clause can be used to sort the rows of the table.</p> <p>The query is built and executed when calling to_pandas(), to_list(), to_pydantic(), or to_polars().</p> <p>Attributes:</p> Name Type Description <code>table</code> <code>LanceTable</code> <p>The LanceTable to query.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>LanceTable</code> <p>The LanceTable to query.</p> required Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def __init__(self, table: LanceTable):\n    \"\"\"Initializes the TableQueryBuilder.\n\n    Args:\n        table: The LanceTable to query.\n    \"\"\"\n    if not isinstance(table, LanceTable):\n        raise ValueError(\"table must be a LanceTable.\")\n\n    self.table: LanceTable = table\n    self._columns: list[str] | dict[str, str] | None = None\n    self._where: str | None = None\n    self._limit: int | None = None\n    self._offset: int | None = None\n    self._order_by: list[str] = []\n    self._descending: list[bool] = []\n    self._function_called: dict[str, bool] = {\n        \"select\": False,\n        \"where\": False,\n        \"limit\": False,\n        \"offset\": False,\n        \"order_by\": False,\n        \"build\": False,\n    }\n</code></pre>"},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder.limit","title":"<code>limit(limit)</code>","text":"<p>Sets the limit for the query.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>The number of rows to return.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The TableQueryBuilder instance.</p> Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def limit(self, limit: int | None) -&gt; Self:\n    \"\"\"Sets the limit for the query.\n\n    Args:\n        limit: The number of rows to return.\n\n    Returns:\n        The TableQueryBuilder instance.\n    \"\"\"\n    self._check_called(\"limit\")\n    if limit is not None:\n        if not isinstance(limit, int) or limit &lt; 0:\n            raise ValueError(\"limit must be None or a positive integer.\")\n    self._limit = limit\n    return self\n</code></pre>"},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder.offset","title":"<code>offset(offset)</code>","text":"<p>Sets the offset for the query.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int | None</code> <p>The number of rows to skip.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The TableQueryBuilder instance.</p> Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def offset(self, offset: int | None) -&gt; Self:\n    \"\"\"Sets the offset for the query.\n\n    Args:\n        offset: The number of rows to skip.\n\n    Returns:\n        The TableQueryBuilder instance.\n    \"\"\"\n    self._check_called(\"offset\")\n    if offset is not None:\n        if not isinstance(offset, int) or offset &lt; 0:\n            raise ValueError(\"offset must be None or a positive integer.\")\n    self._offset = offset\n    return self\n</code></pre>"},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder.order_by","title":"<code>order_by(order_by, descending=False)</code>","text":"<p>Sets the order_by clause for the query.</p> <p>Parameters:</p> Name Type Description Default <code>order_by</code> <code>str | list[str]</code> <p>The column(s) to sort by.</p> required <code>descending</code> <code>bool | list[bool]</code> <p>Whether to sort in descending order.</p> <code>False</code> <p>Returns:</p> Type Description <code>Self</code> <p>The TableQueryBuilder instance.</p> Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def order_by(self, order_by: str | list[str], descending: bool | list[bool] = False) -&gt; Self:\n    \"\"\"Sets the order_by clause for the query.\n\n    Args:\n        order_by: The column(s) to sort by.\n        descending: Whether to sort in descending order.\n\n    Returns:\n        The TableQueryBuilder instance.\n    \"\"\"\n    self._check_called(\"order_by\")\n    if isinstance(order_by, str):\n        order_by = [order_by]\n    elif not isinstance(order_by, list) or not all(isinstance(x, str) for x in order_by):\n        raise ValueError(\"order_by must be a string or a list of strings.\")\n    if isinstance(descending, bool):\n        descending = [descending] * len(order_by)\n    elif (\n        not isinstance(descending, list)\n        or not all(isinstance(x, bool) for x in descending)\n        or len(descending) != len(order_by)\n    ):\n        raise ValueError(\"descending must be a boolean or a list of booleans with the same length as order_by.\")\n\n    self._order_by = order_by\n    self._descending = descending\n    return self\n</code></pre>"},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder.select","title":"<code>select(columns)</code>","text":"<p>Selects columns to include in the query.</p> Note <p>'id' is always included in the select clause.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>str | list[str] | dict[str, str]</code> <p>The columns to include in the query. If a list, the columns are selected in the order they are provided. If a dictionary, the keys are the column names and the values are the aliases.</p> required Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def select(self, columns: str | list[str] | dict[str, str]) -&gt; Self:\n    \"\"\"Selects columns to include in the query.\n\n    Note:\n        'id' is always included in the select clause.\n\n    Args:\n        columns: The columns to include in the query. If a list, the columns are selected in the order they are\n            provided. If a dictionary, the keys are the column names and the values are the aliases.\n    \"\"\"\n    self._check_called(\"select\")\n    if isinstance(columns, str):\n        columns = [columns]\n\n    if isinstance(columns, list) or isinstance(columns, dict):\n        if isinstance(columns, list) and not all(isinstance(x, str) for x in columns):\n            raise ValueError(\"columns must be a list of strings.\")\n        elif isinstance(columns, dict) and not all(\n            isinstance(k, str) and isinstance(v, str) for k, v in columns.items()\n        ):\n            raise ValueError(\"columns must be a dictionary with string keys and values.\")\n        if isinstance(columns, list) and \"id\" not in columns:\n            columns = [\"id\"] + columns\n        elif isinstance(columns, dict) and \"id\" not in columns.values():\n            columns[\"id\"] = \"id\"\n        self._columns = columns\n    else:\n        raise ValueError(\"columns must be a string, a list of string or a string mapping dictionary.\")\n    return self\n</code></pre>"},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder.to_list","title":"<code>to_list()</code>","text":"<p>Builds the query and returns the result as a list of dictionaries.</p> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>The result as a list of dictionaries.</p> Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def to_list(self) -&gt; list[dict[str, Any]]:\n    \"\"\"Builds the query and returns the result as a list of dictionaries.\n\n    Returns:\n        The result as a list of dictionaries.\n    \"\"\"\n    return _PixanoEmptyQueryBuilder(self._execute()).to_list()\n</code></pre>"},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Builds the query and returns the result as a pandas DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The result as a pandas DataFrame.</p> Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Builds the query and returns the result as a pandas DataFrame.\n\n    Returns:\n        The result as a pandas DataFrame.\n    \"\"\"\n    return _PixanoEmptyQueryBuilder(self._execute()).to_pandas()\n</code></pre>"},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder.to_polars","title":"<code>to_polars()</code>","text":"<p>Builds the query and returns the result as a polars DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The result as a polars DataFrame.</p> Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Builds the query and returns the result as a polars DataFrame.\n\n    Returns:\n        The result as a polars DataFrame.\n    \"\"\"\n    return _PixanoEmptyQueryBuilder(self._execute()).to_polars()\n</code></pre>"},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder.to_pydantic","title":"<code>to_pydantic(model)</code>","text":"<p>Builds the query and returns the result as a list of Pydantic models.</p> <p>Returns:</p> Type Description <code>list[T]</code> <p>The result as a list of Pydantic models.</p> Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def to_pydantic(self, model: type[T]) -&gt; list[T]:\n    \"\"\"Builds the query and returns the result as a list of Pydantic models.\n\n    Returns:\n        The result as a list of Pydantic models.\n    \"\"\"\n    return _PixanoEmptyQueryBuilder(self._execute()).to_pydantic(model)\n</code></pre>"},{"location":"api_reference/datasets/queries/table/#pixano.datasets.queries.table.TableQueryBuilder.where","title":"<code>where(where)</code>","text":"<p>Sets the where clause for the query.</p> <p>Parameters:</p> Name Type Description Default <code>where</code> <code>str</code> <p>The condition to filter the rows.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The TableQueryBuilder instance.</p> Source code in <code>pixano/datasets/queries/table.py</code> <pre><code>def where(self, where: str) -&gt; Self:\n    \"\"\"Sets the where clause for the query.\n\n    Args:\n        where: The condition to filter the rows.\n\n    Returns:\n        The TableQueryBuilder instance.\n    \"\"\"\n    self._check_called(\"where\")\n    if not isinstance(where, str):\n        raise ValueError(\"where must be a string.\")\n    self._where = where\n    return self\n</code></pre>"},{"location":"api_reference/datasets/utils/errors/","title":"errors","text":""},{"location":"api_reference/datasets/utils/errors/#pixano.datasets.utils.errors","title":"<code>pixano.datasets.utils.errors</code>","text":""},{"location":"api_reference/datasets/utils/errors/#pixano.datasets.utils.errors.DatasetAccessError","title":"<code>DatasetAccessError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Error raised when accessing a dataset.</p>"},{"location":"api_reference/datasets/utils/errors/#pixano.datasets.utils.errors.DatasetIntegrityError","title":"<code>DatasetIntegrityError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Error raised when dataset integrity is compromised.</p>"},{"location":"api_reference/datasets/utils/errors/#pixano.datasets.utils.errors.DatasetPaginationError","title":"<code>DatasetPaginationError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Error raised when paginating a dataset.</p>"},{"location":"api_reference/datasets/utils/errors/#pixano.datasets.utils.errors.DatasetWriteError","title":"<code>DatasetWriteError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Error raised when writing to a dataset.</p>"},{"location":"api_reference/datasets/utils/integrity/","title":"integrity","text":""},{"location":"api_reference/datasets/utils/integrity/#pixano.datasets.utils.integrity","title":"<code>pixano.datasets.utils.integrity</code>","text":""},{"location":"api_reference/datasets/utils/integrity/#pixano.datasets.utils.integrity.IntegrityCheck","title":"<code>IntegrityCheck</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Integrity check types.</p> <p>Attributes:</p> Name Type Description <code>DEFINED_ID</code> <p>Check if the id field is defined.</p> <code>UNIQUE_ID</code> <p>Check if the id field is unique.</p> <code>REF_NAME</code> <p>Check if the ref name is defined in the schema.</p> <code>REF_TYPE</code> <p>Check if the ref type is defined in the schema.</p> <code>REF_ID</code> <p>Check if the ref id is stored in the referenced table.</p>"},{"location":"api_reference/datasets/utils/integrity/#pixano.datasets.utils.integrity.check_dataset_integrity","title":"<code>check_dataset_integrity(dataset)</code>","text":"<p>Check the integrity of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to check.</p> required <p>Returns:</p> Type Description <code>List of errors as tuples with the following values</code> <code>-check_type</code> <p>Check type.</p> <code>-table</code> <p>Table name.</p> <code>-field_name</code> <p>Field name that caused the error.</p> <code>-schema_id</code> <p>Schema id that raised the error.</p> <code>-field</code> <p>Field value that caused the error.</p> Source code in <code>pixano/datasets/utils/integrity.py</code> <pre><code>def check_dataset_integrity(dataset: \"Dataset\") -&gt; list[tuple[IntegrityCheck, str, str, str, Any]]:\n    \"\"\"Check the integrity of a dataset.\n\n    Args:\n        dataset: Dataset to check.\n\n    Returns:\n        List of errors as tuples with the following values:\n        - check_type: Check type.\n        - table: Table name.\n        - field_name: Field name that caused the error.\n        - schema_id: Schema id that raised the error.\n        - field: Field value that caused the error.\n    \"\"\"\n    check_errors: list[tuple[IntegrityCheck, str, str, str, Any]] = []\n    for table_name in dataset.schema.schemas.keys():\n        check_errors.extend(check_table_integrity(table_name, dataset))\n    return check_errors\n</code></pre>"},{"location":"api_reference/datasets/utils/integrity/#pixano.datasets.utils.integrity.check_table_integrity","title":"<code>check_table_integrity(table_name, dataset, schemas=None, updating=False, ignore_checks=None)</code>","text":"<p>Check the integrity of schemas against a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name.</p> required <code>dataset</code> <code>Dataset</code> <p>Dataset that contains the table.</p> required <code>schemas</code> <code>list[BaseSchema] | None</code> <p>List of schemas to insert in table. If None, the table is checked, otherwise the schemas are checked against the table.</p> <code>None</code> <code>updating</code> <code>bool</code> <p>If True, the table is being updated. It is used to avoid checking the id uniqueness when updating schemas.</p> <code>False</code> <code>ignore_checks</code> <code>list[IntegrityCheck] | None</code> <p>List of integrity checks to ignore.</p> <code>None</code> <p>Returns:</p> Type Description <code>List of errors as tuples with the following values</code> <code>-check_type</code> <p>Check type.</p> <code>-table</code> <p>Table name.</p> <code>-field_name</code> <p>Field name that caused the error.</p> <code>-schema_id</code> <p>Schema id that raised the error.</p> <code>-field</code> <p>Field value that caused the error.</p> Source code in <code>pixano/datasets/utils/integrity.py</code> <pre><code>def check_table_integrity(\n    table_name: str,\n    dataset: \"Dataset\",\n    schemas: list[BaseSchema] | None = None,\n    updating: bool = False,\n    ignore_checks: list[IntegrityCheck] | None = None,\n) -&gt; list[tuple[IntegrityCheck, str, str, str, Any]]:\n    \"\"\"Check the integrity of schemas against a table.\n\n    Args:\n        table_name: Table name.\n        dataset: Dataset that contains the table.\n        schemas: List of schemas to insert in table. If None, the table is checked, otherwise the schemas are checked\n            against the table.\n        updating: If True, the table is being updated. It is used to avoid checking the id uniqueness when updating\n            schemas.\n        ignore_checks: List of integrity checks to ignore.\n\n    Returns:\n        List of errors as tuples with the following values:\n        - check_type: Check type.\n        - table: Table name.\n        - field_name: Field name that caused the error.\n        - schema_id: Schema id that raised the error.\n        - field: Field value that caused the error.\n    \"\"\"\n    table = dataset.open_table(table_name)\n\n    if ignore_checks is not None:\n        ignore_checks_set: set[IntegrityCheck] = {IntegrityCheck(check) for check in ignore_checks}\n    else:\n        ignore_checks_set = set()\n    table_schema = Source if table_name == \"source\" else dataset.schema.schemas[table_name]\n\n    checking_table = schemas is None\n    if schemas is None:\n        if updating:\n            raise ValueError(\"schemas must be provided when updating a table.\")\n        table_schema = cast(BaseSchema, table_schema)\n        fields_to_check = [\"id\"] + [\n            field_name\n            for field_name, field in table_schema.model_fields.items()\n            if field_name != \"id\"\n            and not isinstance(field.annotation, GenericAlias)\n            and is_schema_ref(field.annotation)\n        ]\n        model = create_model(\n            \"_Schema\",\n            __base__=LanceModel,\n            **{field_name: (table_schema.model_fields[field_name].annotation, ...) for field_name in fields_to_check},\n        )\n        schemas = TableQueryBuilder(table).select(fields_to_check).to_pydantic(model)\n\n    table_ids = [schema.id for schema in schemas]\n    count_ids: dict[str, int] = {}\n    for id in table_ids:\n        count_ids[id] = count_ids.get(id, 0) + 1\n    integrity_checks = get_integry_checks_from_schemas(schemas, table_name)\n    check_errors: dict[str, tuple[IntegrityCheck, str, str, str, Any]] = {}\n    ids_to_check: dict[str, str] = {}\n    schemas_refs_to_check: dict[str, list[tuple[str, str, SchemaRef, str]]] = {}\n\n    for check_type_id, checks in enumerate(integrity_checks):\n        check_type = IntegrityCheck(check_type_id)\n        if check_type in ignore_checks_set:\n            continue\n        for check_id, _, schema_id, field_name, field in checks:\n            if check_id in check_errors:\n                continue\n            if check_type == IntegrityCheck.DEFINED_ID and field == \"\":  # id is not defined\n                check_errors[check_id] = (check_type, table_name, field_name, schema_id, field)\n            elif check_type == IntegrityCheck.UNIQUE_ID:\n                if count_ids[schema_id] &gt; 1:  # id is not unique\n                    check_errors[check_id] = (check_type, table_name, field_name, schema_id, field)\n                elif not checking_table:\n                    ids_to_check[schema_id] = check_id\n            elif check_type == IntegrityCheck.REF_NAME:\n                field = cast(SchemaRef, field)\n                if field.name != \"\" and field.name not in (\n                    list(dataset.schema.schemas.keys()) + [\"source\"]\n                ):  # ref name is not defined\n                    check_errors[check_id] = (check_type, table_name, field_name, schema_id, field)\n            elif check_type == IntegrityCheck.REF_TYPE:\n                field = cast(SchemaRef, field)\n                if field.name == \"\":\n                    continue\n                field_type = type(field)\n                if is_view_ref(field_type):  # field is a view ref\n                    field = cast(ViewRef, field)\n                    if field.name not in dataset.schema.groups[SchemaGroup.VIEW]:  # field name is not a view\n                        check_errors[check_id] = (check_type, table_name, field_name, schema_id, field)\n                elif is_annotation_ref(field_type):  # field is an annotation ref\n                    field = cast(AnnotationRef, field)\n                    if (\n                        field.name not in dataset.schema.groups[SchemaGroup.ANNOTATION]\n                    ):  # field name is not an annotation\n                        check_errors[check_id] = (check_type, table_name, field_name, schema_id, field)\n                elif is_embedding_ref(field_type):  # field is an embedding ref\n                    field = cast(EmbeddingRef, field)\n                    if (\n                        field.name not in dataset.schema.groups[SchemaGroup.EMBEDDING]\n                    ):  # field name is not an embedding\n                        check_errors[check_id] = (check_type, table_name, field_name, schema_id, field)\n                elif is_entity_ref(field_type):  # field is an entity ref\n                    field = cast(EntityRef, field)\n                    if field.name not in dataset.schema.groups[SchemaGroup.ENTITY]:  # field name is not an entity\n                        check_errors[check_id] = (check_type, table_name, field_name, schema_id, field)\n                elif is_item_ref(field_type) or is_source_ref(field_type):\n                    pass  # item_ref and source_ref are validated before.\n            elif check_type == IntegrityCheck.REF_ID:  # ref id and ref item relation checked below\n                field = cast(SchemaRef, field)\n                if field_name == \"\":\n                    continue\n                # If the field is empty, the reference is to the table itself so no need to check\n                if field.id == \"\":\n                    continue\n                if field.name not in schemas_refs_to_check:\n                    schemas_refs_to_check[field.name] = []\n                schemas_refs_to_check[field.name].append((check_id, schema_id, field, field_name))\n\n    if not checking_table and not updating and len(ids_to_check) &gt; 0:\n        for id, found in dataset.find_ids_in_table(table_name, set(ids_to_check.keys())).items():\n            if found:\n                check_errors[ids_to_check[id]] = (IntegrityCheck.UNIQUE_ID, table_name, \"id\", id, id)\n\n    if len(check_errors) == len(\n        {check_id for check_id, *_ in integrity_checks[IntegrityCheck.REF_ID.value]}\n    ):  # all checks failed, no need to check later checks that are costly\n        return list(check_errors.values())\n\n    for ref_schema_name, refs in schemas_refs_to_check.items():\n        if ref_schema_name == \"\":\n            continue\n        ref_ids_to_check = {field_ref.id for check_id, _, field_ref, _ in refs if check_id not in check_errors}\n        found_ref_ids = dataset.find_ids_in_table(ref_schema_name, ref_ids_to_check)\n        for check_id, schema_id, field_ref, field_name in refs:\n            if check_id in check_errors:\n                continue\n            if not found_ref_ids[field_ref.id]:\n                check_errors[check_id] = (\n                    IntegrityCheck.REF_ID,\n                    table_name,\n                    field_name,\n                    schema_id,\n                    field_ref,\n                )\n\n    return list(check_errors.values())\n</code></pre>"},{"location":"api_reference/datasets/utils/integrity/#pixano.datasets.utils.integrity.get_integry_checks_from_schemas","title":"<code>get_integry_checks_from_schemas(schemas, table_name)</code>","text":"<p>Get the integrity checks to perform on a table.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list[BaseSchema]</code> <p>List of schemas to check.</p> required <code>table_name</code> <code>str</code> <p>Table name.</p> required <p>Returns:</p> Type Description <code>list[list[tuple[str, str, str, str, Any]]]</code> <p>List of integrity checks to perform on the table. The checks are grouped by type.</p> <code>-check_id</code> <p>Check id (unique identifier for the checks). It is used to avoid checking subsequent checks with the same id when an error is found.</p> <code>-table</code> <p>Table name.</p> <code>-schema_id</code> <p>Schema id which is the id field value from the schema.</p> <code>-field_name</code> <p>Field name to check.</p> <code>-field</code> <p>Field value to check.</p> Source code in <code>pixano/datasets/utils/integrity.py</code> <pre><code>def get_integry_checks_from_schemas(\n    schemas: list[BaseSchema], table_name: str\n) -&gt; list[list[tuple[str, str, str, str, Any]]]:\n    \"\"\"Get the integrity checks to perform on a table.\n\n    Args:\n        schemas: List of schemas to check.\n        table_name: Table name.\n\n    Returns:\n        List of integrity checks to perform on the table. The checks are grouped by type.\n        - check_id: Check id (unique identifier for the checks). It is used to avoid checking subsequent checks with\n            the same id when an error is found.\n        - table: Table name.\n        - schema_id: Schema id which is the id field value from the schema.\n        - field_name: Field name to check.\n        - field: Field value to check.\n    \"\"\"\n    checks: list[list[tuple[str, str, str, str, Any]]] = [[] for _ in IntegrityCheck]\n    for schema in schemas:\n        schema_id = schema.id\n        check_id = shortuuid.uuid()\n        checks[IntegrityCheck.DEFINED_ID.value].append((check_id, table_name, schema_id, \"id\", schema_id))\n        checks[IntegrityCheck.UNIQUE_ID.value].append((check_id, table_name, schema_id, \"id\", schema_id))\n        for field_name, field in schema.model_fields.items():\n            if field_name == \"id\":\n                continue\n            if isinstance(field.annotation, GenericAlias):\n                continue\n            type_field = field.annotation\n            if is_schema_ref(type_field):\n                checks[IntegrityCheck.REF_NAME.value].append(\n                    (check_id, table_name, schema_id, field_name, getattr(schema, field_name))\n                )\n                checks[IntegrityCheck.REF_TYPE.value].append(\n                    (check_id, table_name, schema_id, field_name, getattr(schema, field_name))\n                )\n                checks[IntegrityCheck.REF_ID.value].append(\n                    (check_id, table_name, schema_id, field_name, getattr(schema, field_name))\n                )\n\n    return checks\n</code></pre>"},{"location":"api_reference/datasets/utils/integrity/#pixano.datasets.utils.integrity.handle_integrity_errors","title":"<code>handle_integrity_errors(check_errors, raise_or_warn='raise')</code>","text":"<p>Handle integrity check errors.</p> <p>Parameters:</p> Name Type Description Default <code>check_errors</code> <code>list[tuple[IntegrityCheck, str, str, str, Any]]</code> <p>List of errors.</p> required <code>raise_or_warn</code> <code>Literal['raise', 'warn']</code> <p>If \"raise\", raise a ValueError with the errors. If \"warn\", warns a UserWarning with the errors.</p> <code>'raise'</code> Source code in <code>pixano/datasets/utils/integrity.py</code> <pre><code>def handle_integrity_errors(\n    check_errors: list[tuple[IntegrityCheck, str, str, str, Any]],\n    raise_or_warn: Literal[\"raise\", \"warn\"] = \"raise\",\n) -&gt; None:\n    \"\"\"Handle integrity check errors.\n\n    Args:\n        check_errors: List of errors.\n        raise_or_warn: If \"raise\", raise a ValueError with the errors. If \"warn\", warns a UserWarning with the errors.\n    \"\"\"\n    if len(check_errors) == 0:\n        return\n    message = \"Integrity check errors:\\n\"\n    for check_type, table_name, field_name, schema_id, field in check_errors:\n        message += \"- \"\n        if check_type == IntegrityCheck.DEFINED_ID:\n            message += f\"An id is not defined in table {table_name}.\\n\"\n        elif check_type == IntegrityCheck.UNIQUE_ID:\n            message += f\"The id {schema_id} is not unique in table {table_name}.\\n\"\n        elif check_type == IntegrityCheck.REF_NAME:\n            message += f\"The reference {field_name} from {schema_id} to the table {field.name} does not exist.\\n\"\n        elif check_type == IntegrityCheck.REF_TYPE:\n            message += (\n                f\"The reference {field_name} from {schema_id} to the table {field.name} is to an invalid type. \"\n                f\"Got {type(field)}.\\n\"\n            )\n        elif check_type == IntegrityCheck.REF_ID:\n            message += (\n                f\"The reference {field_name} from {schema_id} to the table {field.name} has an invalid id. Got \"\n                f\"{field.id}.\\n\"\n            )\n    if raise_or_warn == \"raise\":\n        raise DatasetIntegrityError(message)\n    else:\n        warnings.warn(message, category=UserWarning)\n</code></pre>"},{"location":"api_reference/datasets/utils/labels/","title":"labels","text":""},{"location":"api_reference/datasets/utils/labels/#pixano.datasets.utils.labels","title":"<code>pixano.datasets.utils.labels</code>","text":""},{"location":"api_reference/datasets/utils/labels/#pixano.datasets.utils.labels.category_id","title":"<code>category_id(category_name, category_dict='coco91')</code>","text":"<p>Return category ID based on category name.</p> <p>Parameters:</p> Name Type Description Default <code>category_name</code> <code>str</code> <p>Category name</p> required <code>category_dict</code> <code>str</code> <p>Which category dictionary to use (\"coco91\", \"coco80\", \"voc\")</p> <code>'coco91'</code> <p>Returns:</p> Type Description <code>int</code> <p>Category ID</p> Source code in <code>pixano/datasets/utils/labels.py</code> <pre><code>def category_id(category_name: str, category_dict: str = \"coco91\") -&gt; int:\n    \"\"\"Return category ID based on category name.\n\n    Args:\n        category_name: Category name\n        category_dict: Which category dictionary to use (\"coco91\", \"coco80\", \"voc\")\n\n    Returns:\n        Category ID\n    \"\"\"\n    return CATEGORY_IDS[category_dict][str(category_name).strip().lower()]\n</code></pre>"},{"location":"api_reference/datasets/utils/labels/#pixano.datasets.utils.labels.category_name","title":"<code>category_name(category_id, category_dict='coco91')</code>","text":"<p>Return category name based on category ID.</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> <code>int</code> <p>Category ID</p> required <code>category_dict</code> <code>str</code> <p>Which category dictionary to use (\"coco91\", \"coco80\", \"voc\")</p> <code>'coco91'</code> <p>Returns:</p> Type Description <code>str</code> <p>Category name</p> Source code in <code>pixano/datasets/utils/labels.py</code> <pre><code>def category_name(category_id: int, category_dict: str = \"coco91\") -&gt; str:\n    \"\"\"Return category name based on category ID.\n\n    Args:\n        category_id: Category ID\n        category_dict: Which category dictionary to use (\"coco91\", \"coco80\", \"voc\")\n\n    Returns:\n        Category name\n    \"\"\"\n    return CATEGORY_NAMES[category_dict][int(category_id)]\n</code></pre>"},{"location":"api_reference/datasets/utils/labels/#pixano.datasets.utils.labels.coco_ids_80to91","title":"<code>coco_ids_80to91(category_id)</code>","text":"<p>Convert COCO category ID from 80 to 91 classes.</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> <code>int</code> <p>Category ID (80 classes)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Category ID (91 classes)</p> Source code in <code>pixano/datasets/utils/labels.py</code> <pre><code>def coco_ids_80to91(category_id: int) -&gt; int:\n    \"\"\"Convert COCO category ID from 80 to 91 classes.\n\n    Args:\n        category_id: Category ID (80 classes)\n\n    Returns:\n        Category ID (91 classes)\n    \"\"\"\n    coco_dict = {\n        1: 1,\n        2: 2,\n        3: 3,\n        4: 4,\n        5: 5,\n        6: 6,\n        7: 7,\n        8: 8,\n        9: 9,\n        10: 10,\n        11: 11,\n        12: 13,\n        13: 14,\n        14: 15,\n        15: 16,\n        16: 17,\n        17: 18,\n        18: 19,\n        19: 20,\n        20: 21,\n        21: 22,\n        22: 23,\n        23: 24,\n        24: 25,\n        25: 27,\n        26: 28,\n        27: 31,\n        28: 32,\n        29: 33,\n        30: 34,\n        31: 35,\n        32: 36,\n        33: 37,\n        34: 38,\n        35: 39,\n        36: 40,\n        37: 41,\n        38: 42,\n        39: 43,\n        40: 44,\n        41: 46,\n        42: 47,\n        43: 48,\n        44: 49,\n        45: 50,\n        46: 51,\n        47: 52,\n        48: 53,\n        49: 54,\n        50: 55,\n        51: 56,\n        52: 57,\n        53: 58,\n        54: 59,\n        55: 60,\n        56: 61,\n        57: 62,\n        58: 63,\n        59: 64,\n        60: 65,\n        61: 67,\n        62: 70,\n        63: 72,\n        64: 73,\n        65: 74,\n        66: 75,\n        67: 76,\n        68: 77,\n        69: 78,\n        70: 79,\n        71: 80,\n        72: 81,\n        73: 82,\n        74: 84,\n        75: 85,\n        76: 86,\n        77: 87,\n        78: 88,\n        79: 89,\n        80: 90,\n    }\n\n    return coco_dict[int(category_id)]\n</code></pre>"},{"location":"api_reference/datasets/utils/mosaic/","title":"mosaic","text":""},{"location":"api_reference/datasets/utils/mosaic/#pixano.datasets.utils.mosaic","title":"<code>pixano.datasets.utils.mosaic</code>","text":""},{"location":"api_reference/datasets/utils/mosaic/#pixano.datasets.utils.mosaic.add_label_above","title":"<code>add_label_above(image, text, label_height=30)</code>","text":"<p>Add a label at top left of image.</p> Source code in <code>pixano/datasets/utils/mosaic.py</code> <pre><code>def add_label_above(image, text, label_height=30):\n    \"\"\"Add a label at top left of image.\"\"\"\n    width, height = image.size\n\n    label_img = Image.new(\"RGB\", (width, label_height), (0, 0, 0))  # Fond noir\n    draw = ImageDraw.Draw(label_img)\n\n    # get Arial font, else system font\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 20)\n    except IOError:\n        font = ImageFont.load_default()\n\n    # Center text\n    text_size = draw.textbbox((0, 0), text, font=font)  # Mesurer le texte\n    text_width = text_size[2] - text_size[0]\n    text_height = text_size[3] - text_size[1]\n\n    text_x = (width - text_width) // 2\n    text_y = (label_height - text_height) // 2\n\n    # Draw text (white)\n    draw.text((text_x, text_y), text, fill=(255, 255, 255), font=font)\n\n    # Concat label and original image\n    new_img = Image.new(\"RGB\", (width, height + label_height))\n    new_img.paste(label_img, (0, 0))  # Add label over\n    new_img.paste(image, (0, label_height))  # put image under\n\n    return new_img\n</code></pre>"},{"location":"api_reference/datasets/utils/mosaic/#pixano.datasets.utils.mosaic.arrange_grid","title":"<code>arrange_grid(num_images)</code>","text":"<p>Get optimal grid distribution (rows, cols) for num_images.</p> Source code in <code>pixano/datasets/utils/mosaic.py</code> <pre><code>def arrange_grid(num_images):\n    \"\"\"Get optimal grid distribution (rows, cols) for num_images.\"\"\"\n    cols = math.ceil(math.sqrt(num_images))\n    rows = math.ceil(num_images / cols)\n    return rows, cols\n</code></pre>"},{"location":"api_reference/datasets/utils/mosaic/#pixano.datasets.utils.mosaic.compute_average_size","title":"<code>compute_average_size(images)</code>","text":"<p>Compute images average size.</p> Source code in <code>pixano/datasets/utils/mosaic.py</code> <pre><code>def compute_average_size(images):\n    \"\"\"Compute images average size.\"\"\"\n    widths, heights = zip(*(img.size for img in images))\n    avg_width = sum(widths) // len(images)\n    avg_height = sum(heights) // len(images)\n    return avg_width, avg_height\n</code></pre>"},{"location":"api_reference/datasets/utils/mosaic/#pixano.datasets.utils.mosaic.create_mosaic","title":"<code>create_mosaic(source_dir, split, image_files, output_path, label_prefix='image', padding=10, label_height=30)</code>","text":"<p>Create a mosaic from image_files with black background.</p> Source code in <code>pixano/datasets/utils/mosaic.py</code> <pre><code>def create_mosaic(\n    source_dir: Path, split, image_files, output_path, label_prefix=\"image\", padding=10, label_height=30\n):\n    \"\"\"Create a mosaic from image_files with black background.\"\"\"\n    if not image_files:\n        return\n\n    need_split = False\n    images = []\n    for f in image_files:\n        try:\n            image = Image.open(source_dir / f)\n        except FileNotFoundError:\n            image = Image.open(source_dir / split / f)\n            need_split = True\n        image = image.convert(\"RGBA\")\n        images.append(image)\n\n    rows, cols = arrange_grid(len(images))\n    avg_width, avg_height = compute_average_size(images)\n\n    images_resized = []\n    for idx, img in enumerate(images):\n        img_resized = img.resize((avg_width, avg_height), Image.LANCZOS)\n        img_rgb = Image.new(\"RGB\", img_resized.size, (0, 0, 0))\n        img_rgb.paste(img_resized, mask=img_resized.split()[3])  # Apply alpha channel\n        # Add label\n        if label_prefix != \"\":\n            label_text = f\"{label_prefix} {idx + 1}\"\n            img_labeled = add_label_above(img_rgb, label_text, label_height)\n            images_resized.append(img_labeled)\n        else:\n            images_resized.append(img_rgb)\n\n    mosaic_width = cols * (avg_width + padding) - padding\n    mosaic_height = rows * (avg_height + label_height + padding) - padding\n    mosaic = Image.new(\"RGB\", (mosaic_width, mosaic_height), (0, 0, 0))\n\n    for idx, img in enumerate(images_resized):\n        x_offset = (idx % cols) * (avg_width + padding)\n        y_offset = (idx // cols) * (avg_height + label_height + padding)\n        mosaic.paste(img, (x_offset, y_offset))\n\n    if need_split:\n        mosaic.save(source_dir / split / output_path, format=\"JPEG\", quality=85, optimize=True)\n    else:\n        mosaic.save(source_dir / output_path, format=\"JPEG\", quality=85, optimize=True)\n</code></pre>"},{"location":"api_reference/datasets/utils/mosaic/#pixano.datasets.utils.mosaic.generate_mosaic_name","title":"<code>generate_mosaic_name(image_files, extension='.jpg')</code>","text":"<p>Generate a name for mosaic file whime keeping most common path / filename as possible.</p> Source code in <code>pixano/datasets/utils/mosaic.py</code> <pre><code>def generate_mosaic_name(image_files, extension=\".jpg\"):\n    \"\"\"Generate a name for mosaic file whime keeping most common path / filename as possible.\"\"\"\n    if not image_files:\n        return \"mosaic\" + extension\n    common_path = os.path.commonpath(image_files)\n    filenames = [os.path.basename(f) for f in image_files]\n    common_prefix = os.path.commonprefix(filenames).rstrip(\"_- \")\n    mosaic_name = f\"{common_prefix}_mosaic{extension}\" if common_prefix else f\"mosaic{extension}\"\n    return os.path.join(common_path, mosaic_name)\n</code></pre>"},{"location":"api_reference/datasets/utils/mosaic/#pixano.datasets.utils.mosaic.mosaic","title":"<code>mosaic(source_dir, split, image_files, view_name, mosaic_filename='')</code>","text":"<p>Create a mosaic from input images. Add a label if view_name is not empty string.</p> Source code in <code>pixano/datasets/utils/mosaic.py</code> <pre><code>def mosaic(source_dir: Path, split: str, image_files: list[str], view_name: str, mosaic_filename: str = \"\") -&gt; str:\n    \"\"\"Create a mosaic from input images. Add a label if view_name is not empty string.\"\"\"\n    if not mosaic_filename:\n        mosaic_filename = generate_mosaic_name(image_files)\n    create_mosaic(source_dir, split, image_files, mosaic_filename, label_prefix=view_name, label_height=30, padding=5)\n    return mosaic_filename\n</code></pre>"},{"location":"api_reference/datasets/utils/video/","title":"video","text":""},{"location":"api_reference/datasets/utils/video/#pixano.datasets.utils.video","title":"<code>pixano.datasets.utils.video</code>","text":""},{"location":"api_reference/datasets/utils/video/#pixano.datasets.utils.video.create_video_preview","title":"<code>create_video_preview(path, frame_urls, fps=25, scale=0.5)</code>","text":"<p>Create a video preview by writing a sequence of frames to a video file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the output video file.</p> required <code>frame_urls</code> <code>Iterable[str]</code> <p>URLs pointing to the frames of the video.</p> required <code>fps</code> <code>int</code> <p>The frames per second of the output video.</p> <code>25</code> <code>scale</code> <code>float</code> <p>The scale factor to resize the frames.</p> <code>0.5</code> Source code in <code>pixano/datasets/utils/video.py</code> <pre><code>def create_video_preview(path: Path, frame_urls: Iterable[str], fps: int = 25, scale: float = 0.5):\n    \"\"\"Create a video preview by writing a sequence of frames to a video file.\n\n    Args:\n        path: The path to the output video file.\n        frame_urls: URLs pointing to the frames of the video.\n        fps: The frames per second of the output video.\n        scale: The scale factor to resize the frames.\n    \"\"\"\n    # Import mediapy only when needed to avoid unnecessary dependencies.\n    import mediapy\n\n    frames = [mediapy.read_image(url) for url in frame_urls]\n\n    if scale &lt; 1:\n        size = frames[0].shape[:2]\n        target_size = (int(size[0] * scale), int(size[1] * scale))\n        frames = mediapy.resize_video(frames, target_size)\n\n    mediapy.write_video(path, frames, fps=fps)\n</code></pre>"},{"location":"api_reference/datasets/workspaces/dataset_items/","title":"dataset_items","text":""},{"location":"api_reference/datasets/workspaces/dataset_items/#pixano.datasets.workspaces.dataset_items","title":"<code>pixano.datasets.workspaces.dataset_items</code>","text":""},{"location":"api_reference/datasets/workspaces/dataset_items/#pixano.datasets.workspaces.dataset_items.DefaultImageDatasetItem","title":"<code>DefaultImageDatasetItem(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>DatasetItem</code></p> <p>Default Image DatasetItem Schema.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/datasets/workspaces/dataset_items/#pixano.datasets.workspaces.dataset_items.DefaultVQADatasetItem","title":"<code>DefaultVQADatasetItem(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>DatasetItem</code></p> <p>Default VQA DatasetItem Schema.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/datasets/workspaces/dataset_items/#pixano.datasets.workspaces.dataset_items.DefaultVideoDatasetItem","title":"<code>DefaultVideoDatasetItem(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>DatasetItem</code></p> <p>Default Video DatasetItem Schema.</p> Source code in <code>pixano/datasets/dataset_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/pyarrow_utils/","title":"pyarrow_utils","text":""},{"location":"api_reference/features/pyarrow_utils/#pixano.features.pyarrow_utils","title":"<code>pixano.features.pyarrow_utils</code>","text":""},{"location":"api_reference/features/schemas/base_schema/","title":"base_schema","text":""},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema","title":"<code>pixano.features.schemas.base_schema</code>","text":""},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema.BaseSchema","title":"<code>BaseSchema(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>LanceModel</code></p> <p>Base class for all schemas.</p> <p>All schemas should inherit from this class and therefore all elements in the dataset contains an id.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>the id of the manipulated object.</p> <code>created_at</code> <code>datetime</code> <p>the creation date of the object.</p> <code>updated_at</code> <code>datetime</code> <p>the last modification date of the object.</p> Note <p>If the <code>created_at</code> and <code>updated_at</code> fields are not provided, they are set to the current date and time.</p> <p>Raises <code>ValidationError</code> if the input data cannot be validated to form a valid model.</p> <p><code>self</code> is explicitly positional-only to allow <code>self</code> as a field name.</p> <p>Parameters:</p> Name Type Description Default <code>created_at</code> <code>datetime | None</code> <p>The creation date of the object.</p> <code>None</code> <code>updated_at</code> <code>datetime | None</code> <p>The last modification date of the object.</p> <code>None</code> <code>data</code> <code>Any</code> <p>The data of the object validated by Pydantic.</p> <code>{}</code> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema.BaseSchema.dataset","title":"<code>dataset</code>  <code>property</code> <code>writable</code>","text":"<p>Get the dataset.</p>"},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema.BaseSchema.table_name","title":"<code>table_name</code>  <code>property</code> <code>writable</code>","text":"<p>Get the table name.</p>"},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema.BaseSchema.deserialize","title":"<code>deserialize(dataset_schema_json)</code>  <code>staticmethod</code>","text":"<p>Deserialize the dataset schema.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_schema_json</code> <code>dict[str, str | dict[str, Any]]</code> <p>Serialized dataset schema.</p> required <p>Returns:</p> Type Description <code>type['BaseSchema']</code> <p>The dataset schema.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>@staticmethod\ndef deserialize(dataset_schema_json: dict[str, str | dict[str, Any]]) -&gt; type[\"BaseSchema\"]:\n    \"\"\"Deserialize the dataset schema.\n\n    Args:\n        dataset_schema_json: Serialized dataset schema.\n\n    Returns:\n        The dataset schema.\n    \"\"\"\n    from .registry import _PIXANO_SCHEMA_REGISTRY, _SCHEMA_REGISTRY\n\n    json_fields = dataset_schema_json[\"fields\"]\n    if not isinstance(json_fields, dict):\n        raise ValueError(\"Fields should be a dictionary.\")\n\n    fields: dict[str, Any] = {}\n    for key, value in json_fields.items():\n        if value[\"type\"] in _TYPES_REGISTRY:\n            type_ = _TYPES_REGISTRY[value[\"type\"]]\n        elif value[\"type\"] == \"FixedSizeList\":  # LanceDB Vector\n            type_ = value[\"type\"]\n            dim = value[\"dim\"]\n            value_type = DESERIALIZE_PYARROW_DATATYPE[value[\"value_type\"]]\n            type_ = Vector(dim, value_type)\n        else:\n            raise ValueError(f\"Type {value['type']} not registered\")\n        if value[\"collection\"]:\n            type_ = list[type_]  # type: ignore[valid-type]\n        fields[key] = (type_, ...)\n\n    schema, base_schema = dataset_schema_json[\"schema\"], dataset_schema_json[\"base_schema\"]\n\n    if not isinstance(schema, str) or not isinstance(base_schema, str):\n        raise ValueError(\"Schema and base schema should be strings.\")\n\n    if schema in _SCHEMA_REGISTRY:\n        table_type = _SCHEMA_REGISTRY[schema]\n    else:\n        table_type = _PIXANO_SCHEMA_REGISTRY[base_schema]\n\n    model = create_model(dataset_schema_json[\"schema\"], **fields, __base__=table_type)\n\n    return model\n</code></pre>"},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema.BaseSchema.model_copy","title":"<code>model_copy(*, update=None, deep=False)</code>","text":"<p>Returns a copy of the model.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>dict[str, Any] | None</code> <p>Values to change/add in the new model.</p> <code>None</code> <code>deep</code> <code>bool</code> <p>Set to <code>True</code> to make a deep copy of the model.</p> <code>False</code> <p>Returns:</p> Type Description <code>Self</code> <p>New model instance.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def model_copy(self, *, update: dict[str, Any] | None = None, deep: bool = False) -&gt; Self:\n    \"\"\"Returns a copy of the model.\n\n    Args:\n        update: Values to change/add in the new model.\n        deep: Set to `True` to make a deep copy of the model.\n\n    Returns:\n        New model instance.\n    \"\"\"\n    # Wrap the pydantic `model_copy` method to prevent copying the dataset.\n    dataset = self._dataset\n    self._dataset = None\n\n    copy = super().model_copy(update=update, deep=deep)\n    copy.dataset = dataset\n    return copy\n</code></pre>"},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema.BaseSchema.model_dump","title":"<code>model_dump(exclude_timestamps=False, **kwargs)</code>","text":"<p>Dump the model to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_timestamps</code> <code>bool</code> <p>Exclude timestamps \"created_at\" and \"updated_at\" from the model dump. Useful for comparing models without timestamps.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Arguments for pydantic <code>BaseModel.model_dump()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The model dump.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def model_dump(self, exclude_timestamps: bool = False, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Dump the model to a dictionary.\n\n    Args:\n        exclude_timestamps: Exclude timestamps \"created_at\" and \"updated_at\" from the model dump. Useful for\n            comparing models without timestamps.\n        kwargs: Arguments for pydantic `BaseModel.model_dump()`.\n\n    Returns:\n        The model dump.\n    \"\"\"\n    model_dump = super().model_dump(**kwargs)\n    if exclude_timestamps:\n        model_dump.pop(\"created_at\", None)\n        model_dump.pop(\"updated_at\", None)\n    return model_dump\n</code></pre>"},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema.BaseSchema.resolve_ref","title":"<code>resolve_ref(ref)</code>","text":"<pre><code>resolve_ref(ref: 'ItemRef') -&gt; 'Item'\n</code></pre><pre><code>resolve_ref(ref: 'ViewRef') -&gt; 'View'\n</code></pre><pre><code>resolve_ref(ref: 'EmbeddingRef') -&gt; 'Embedding'\n</code></pre><pre><code>resolve_ref(ref: 'EntityRef') -&gt; 'Entity'\n</code></pre><pre><code>resolve_ref(ref: 'AnnotationRef') -&gt; 'Annotation'\n</code></pre><pre><code>resolve_ref(ref: 'SourceRef') -&gt; 'Source'\n</code></pre><pre><code>resolve_ref(ref: 'SchemaRef') -&gt; 'BaseSchema'\n</code></pre> <p>Resolve a reference to a schema object in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>'SchemaRef' | 'ItemRef' | 'ViewRef' | 'EmbeddingRef' | 'EntityRef' | 'AnnotationRef' | 'SourceRef'</code> <p>The reference to resolve.</p> required <p>Returns:</p> Type Description <code>'BaseSchema' | 'Item' | 'View' | 'Embedding' | 'Entity' | 'Annotation' | 'Source'</code> <p>The resolved schema object.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def resolve_ref(\n    self, ref: \"SchemaRef\" | \"ItemRef\" | \"ViewRef\" | \"EmbeddingRef\" | \"EntityRef\" | \"AnnotationRef\" | \"SourceRef\"\n) -&gt; \"BaseSchema\" | \"Item\" | \"View\" | \"Embedding\" | \"Entity\" | \"Annotation\" | \"Source\":\n    \"\"\"Resolve a reference to a schema object in the dataset.\n\n    Args:\n        ref: The reference to resolve.\n\n    Returns:\n        The resolved schema object.\n    \"\"\"\n    return self.dataset.resolve_ref(ref)\n</code></pre>"},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema.BaseSchema.serialize","title":"<code>serialize()</code>  <code>classmethod</code>","text":"<p>Serialize the table.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>@classmethod\ndef serialize(cls) -&gt; dict[str, str | dict[str, Any]]:\n    \"\"\"Serialize the table.\"\"\"\n    from .registry import _PIXANO_SCHEMA_REGISTRY\n\n    # schema can be customized by the user\n    # base_schema is the closest schema in the registry\n    super_type = get_super_type_from_dict(cls, _PIXANO_SCHEMA_REGISTRY)\n    if super_type is None:\n        raise ValueError(f\"Schema {cls.__name__} does not have a super type in the registry.\")\n    json: dict[str, str | dict[str, Any]] = {\n        \"schema\": cls.__name__,\n        \"base_schema\": super_type.__name__,\n    }\n    fields: dict[str, Any] = {}\n    for field_name, field in cls.model_fields.items():\n        if isinstance(field.annotation, GenericAlias):\n            origin = field.annotation.__origin__\n            args = field.annotation.__args__\n\n            if origin in [list, tuple]:\n                if issubclass(args[0], tuple(_TYPES_REGISTRY.values())):\n                    fields[field_name] = {\n                        \"type\": args[0].__name__,\n                        \"collection\": True,\n                    }\n                else:\n                    fields[field_name] = {\n                        \"type\": args[0].__name__,\n                        \"collection\": True,\n                    }\n            else:\n                raise NotImplementedError(\"Should be a list or tuple.\")\n        else:\n            if issubclass(field.annotation, tuple(_TYPES_REGISTRY.values())):\n                fields[field_name] = {\n                    \"type\": field.annotation.__name__,\n                    \"collection\": False,\n                }\n            elif issubclass(field.annotation, FixedSizeListMixin):  # LanceDB Vector\n                fields[field_name] = {\n                    \"type\": field.annotation.__name__,\n                    \"collection\": False,\n                    \"dim\": field.annotation.dim(),\n                    \"value_type\": SERIALIZE_PYARROW_DATATYPE[field.annotation.value_arrow_type()],\n                }\n            else:\n                fields[field_name] = {\n                    \"type\": field.annotation.__name__,\n                    \"collection\": False,\n                }\n    json[\"fields\"] = fields\n    return json\n</code></pre>"},{"location":"api_reference/features/schemas/base_schema/#pixano.features.schemas.base_schema.is_base_schema","title":"<code>is_base_schema(cls, strict=False)</code>","text":"<p>Check if a class is a <code>BaseSchema</code> or subclass of <code>BaseSchema</code>.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def is_base_schema(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `BaseSchema` or subclass of `BaseSchema`.\"\"\"\n    return issubclass_strict(cls, BaseSchema, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/registry/","title":"registry","text":""},{"location":"api_reference/features/schemas/registry/#pixano.features.schemas.registry","title":"<code>pixano.features.schemas.registry</code>","text":""},{"location":"api_reference/features/schemas/registry/#pixano.features.schemas.registry.register_schema","title":"<code>register_schema(cls)</code>","text":"<p>Class decorator to register a schema.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[BaseSchema]</code> <p>The schema to register.</p> required Source code in <code>pixano/features/schemas/registry.py</code> <pre><code>def register_schema(cls: type[BaseSchema]):\n    \"\"\"Class decorator to register a schema.\n\n    Args:\n        cls: The schema to register.\n    \"\"\"\n    _add_schema_to_registry(cls, _SCHEMA_REGISTRY)\n    return cls\n</code></pre>"},{"location":"api_reference/features/schemas/schema_group/","title":"schema_group","text":""},{"location":"api_reference/features/schemas/schema_group/#pixano.features.schemas.schema_group","title":"<code>pixano.features.schemas.schema_group</code>","text":""},{"location":"api_reference/features/schemas/schema_group/#pixano.features.schemas.schema_group.SchemaGroup","title":"<code>SchemaGroup</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Schema group.</p> <p>It defines the different schema groups to which a schema can belong.</p> <p>Attributes:</p> Name Type Description <code>ANNOTATION</code> <p>Annotation schema group.</p> <code>EMBEDDING</code> <p>Embedding schema group.</p> <code>ITEM</code> <p>Item schema group.</p> <code>ENTITY</code> <p>Entity schema group.</p> <code>VIEW</code> <p>View schema group.</p>"},{"location":"api_reference/features/schemas/schema_group/#pixano.features.schemas.schema_group.group_to_str","title":"<code>group_to_str(group, plural=False)</code>","text":"<p>Convert the schema group to a string.</p> <p>Attributes:</p> Name Type Description <code>group</code> <p>The schema group.</p> <code>plural</code> <p>Whether to use the plural form of the word.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string for the given schema group.</p> Source code in <code>pixano/features/schemas/schema_group.py</code> <pre><code>def group_to_str(group: SchemaGroup, plural: bool = False) -&gt; str:\n    \"\"\"Convert the schema group to a string.\n\n    Attributes:\n        group: The schema group.\n        plural: Whether to use the plural form of the word.\n\n    Returns:\n        The string for the given schema group.\n    \"\"\"\n    if group == SchemaGroup.SOURCE:\n        return \"sources\" if plural else \"source\"\n    elif group == SchemaGroup.ITEM:\n        return \"items\" if plural else \"item\"\n    elif group == SchemaGroup.ENTITY:\n        return \"entities\" if plural else \"entity\"\n    elif group == SchemaGroup.VIEW:\n        return \"views\" if plural else \"view\"\n    elif group == SchemaGroup.ANNOTATION:\n        return \"annotations\" if plural else \"annotation\"\n    elif group == SchemaGroup.EMBEDDING:\n        return \"embeddings\" if plural else \"embedding\"\n    raise ValueError(f\"Unknown schema group: {group}\")\n</code></pre>"},{"location":"api_reference/features/schemas/schema_group/#pixano.features.schemas.schema_group.schema_to_group","title":"<code>schema_to_group(schema_type)</code>","text":"<p>Get the schema group of a given schema type.</p> Source code in <code>pixano/features/schemas/schema_group.py</code> <pre><code>def schema_to_group(schema_type: BaseSchema | type) -&gt; SchemaGroup:\n    \"\"\"Get the schema group of a given schema type.\"\"\"\n    try:\n        issubclass(schema_type, BaseSchema)\n    except TypeError:\n        is_class = False\n    else:\n        is_class = True\n    if isinstance(schema_type, Embedding) or is_class and issubclass(schema_type, Embedding):\n        return SchemaGroup.EMBEDDING\n    elif isinstance(schema_type, Item) or is_class and issubclass(schema_type, Item):\n        return SchemaGroup.ITEM\n    elif isinstance(schema_type, Entity) or is_class and issubclass(schema_type, Entity):\n        return SchemaGroup.ENTITY\n    elif isinstance(schema_type, Annotation) or is_class and issubclass(schema_type, Annotation):\n        return SchemaGroup.ANNOTATION\n    elif isinstance(schema_type, View) or is_class and issubclass(schema_type, View):\n        return SchemaGroup.VIEW\n    elif isinstance(schema_type, Source) or is_class and issubclass(schema_type, Source):\n        return SchemaGroup.SOURCE\n    else:\n        raise ValueError(f\"Unknown schema type: {schema_type}\")\n</code></pre>"},{"location":"api_reference/features/schemas/source/","title":"source","text":""},{"location":"api_reference/features/schemas/source/#pixano.features.schemas.source","title":"<code>pixano.features.schemas.source</code>","text":""},{"location":"api_reference/features/schemas/source/#pixano.features.schemas.source.Source","title":"<code>Source(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchema</code></p> <p>Source of the annotation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the source.</p> <code>kind</code> <code>str</code> <p>Kind of source.</p> <code>metadata</code> <code>str</code> <p>Metadata of the source. dict[str, Any] encoded in a string.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/source/#pixano.features.schemas.source.Source.create_ground_truth","title":"<code>create_ground_truth(metadata={})</code>  <code>classmethod</code>","text":"<p>Create a ground truth source.</p> Source code in <code>pixano/features/schemas/source.py</code> <pre><code>@classmethod\ndef create_ground_truth(cls, metadata: str | dict[str, Any] = {}) -&gt; \"Source\":\n    \"\"\"Create a ground truth source.\"\"\"\n    return cls(\n        id=SourceKind.GROUND_TRUTH.value,\n        name=\"Ground Truth\",\n        kind=SourceKind.GROUND_TRUTH.value,\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/source/#pixano.features.schemas.source.SourceKind","title":"<code>SourceKind</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Kind of source that produced the annotation.</p> <p>Attributes:</p> Name Type Description <code>MODEL</code> <p>Source produced by a model.</p> <code>HUMAN</code> <p>Source produced by a human.</p> <code>GROUND_TRUTH</code> <p>The source is a ground truth.</p> <code>OTHER</code> <p>Source produced by other means.</p>"},{"location":"api_reference/features/schemas/source/#pixano.features.schemas.source.create_source","title":"<code>create_source(id, name, kind, metadata)</code>","text":"<p>Create a <code>Source</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Identifier of the source.</p> required <code>name</code> <code>str</code> <p>Name of the source.</p> required <code>kind</code> <code>Literal['model', 'human', 'ground_truth', 'other'] | SourceKind</code> <p>Kind of source.</p> required <code>metadata</code> <code>str | dict[str, Any]</code> <p>Metadata of the source.</p> required <p>Returns:</p> Type Description <code>Source</code> <p>The created <code>Source</code> instance.</p> Source code in <code>pixano/features/schemas/source.py</code> <pre><code>def create_source(\n    id: str,\n    name: str,\n    kind: Literal[\"model\", \"human\", \"ground_truth\", \"other\"] | SourceKind,\n    metadata: str | dict[str, Any],\n) -&gt; Source:\n    \"\"\"Create a `Source` instance.\n\n    Args:\n        id: Identifier of the source.\n        name: Name of the source.\n        kind: Kind of source.\n        metadata: Metadata of the source.\n\n    Returns:\n        The created `Source` instance.\n    \"\"\"\n    return Source(id=id, name=name, kind=kind, metadata=metadata)\n</code></pre>"},{"location":"api_reference/features/schemas/source/#pixano.features.schemas.source.is_source","title":"<code>is_source(cls, strict=False)</code>","text":"<p>Check if a class is a Source or subclass of Source.</p> Source code in <code>pixano/features/schemas/source.py</code> <pre><code>def is_source(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a Source or subclass of Source.\"\"\"\n    return issubclass_strict(cls, Source, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/annotation/","title":"annotation","text":""},{"location":"api_reference/features/schemas/annotations/annotation/#pixano.features.schemas.annotations.annotation","title":"<code>pixano.features.schemas.annotations.annotation</code>","text":""},{"location":"api_reference/features/schemas/annotations/annotation/#pixano.features.schemas.annotations.annotation.Annotation","title":"<code>Annotation(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchema</code></p> <p>Annotations are used to annotate an entity in a dataset.</p> <p>It can refer to an entity, an item, and a view.</p> <p>Attributes:</p> Name Type Description <code>item_ref</code> <code>ItemRef</code> <p>Reference to the annotation's item.</p> <code>view_ref</code> <code>ViewRef</code> <p>Reference to the annotation's view.</p> <code>entity_ref</code> <code>EntityRef</code> <p>Reference to the annotation's entity.</p> <code>source_ref</code> <code>SourceRef</code> <p>Reference to the annotation's source.</p> <code>inference_metadata</code> <code>str</code> <p>Metadata of the inference model for the annotation if any. dict[str, Any] encoded in a string.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/annotation/#pixano.features.schemas.annotations.annotation.Annotation.entity","title":"<code>entity</code>  <code>property</code>","text":"<p>Get the annotation's entity.</p>"},{"location":"api_reference/features/schemas/annotations/annotation/#pixano.features.schemas.annotations.annotation.Annotation.item","title":"<code>item</code>  <code>property</code>","text":"<p>Get the annotation's item.</p>"},{"location":"api_reference/features/schemas/annotations/annotation/#pixano.features.schemas.annotations.annotation.Annotation.view","title":"<code>view</code>  <code>property</code>","text":"<p>Get the annotation's view.</p>"},{"location":"api_reference/features/schemas/annotations/annotation/#pixano.features.schemas.annotations.annotation.is_annotation","title":"<code>is_annotation(cls, strict=False)</code>","text":"<p>Check if a class is an Annotation or subclass of Annotation.</p> Source code in <code>pixano/features/schemas/annotations/annotation.py</code> <pre><code>def is_annotation(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is an Annotation or subclass of Annotation.\"\"\"\n    return issubclass_strict(cls, Annotation, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/","title":"bbox","text":""},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox","title":"<code>pixano.features.schemas.annotations.bbox</code>","text":""},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox","title":"<code>BBox(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>Bounding box using coordinates in xyxy or xywh format.</p> <p>Attributes:</p> Name Type Description <code>coords</code> <code>list[float]</code> <p>List of coordinates in given format.</p> <code>format</code> <code>str</code> <p>Coordinates format, 'xyxy' or 'xywh'.</p> <code>is_normalized</code> <code>bool</code> <p>True if coordinates are normalized to image size.</p> <code>confidence</code> <code>float</code> <p>Bounding box confidence if predicted. -1 if not predicted.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.xywh_coords","title":"<code>xywh_coords</code>  <code>property</code>","text":"<p>Return the bounding box xywh coordinates.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>Coordinates in xywh format.</p>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.xyxy_coords","title":"<code>xyxy_coords</code>  <code>property</code>","text":"<p>Return the bounding box xyxy coordinates.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>Coordinates in xyxy format.</p>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.denormalize","title":"<code>denormalize(height, width)</code>","text":"<p>Return the bounding box with coordinates denormalized relatively to the image size.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Image height.</p> required <code>width</code> <code>int</code> <p>Image width.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>Bounding box with coordinates denormalized relatively to the image size.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>def denormalize(self, height: int, width: int) -&gt; Self:\n    \"\"\"Return the bounding box with coordinates denormalized relatively to the image size.\n\n    Args:\n        height: Image height.\n        width: Image width.\n\n    Returns:\n        Bounding box with coordinates denormalized relatively to the image size.\n    \"\"\"\n    return BBox(\n        coords=bbox_utils.denormalize_coords(self.coords, height, width),\n        format=self.format,\n        is_normalized=False,\n        confidence=self.confidence,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.from_mask","title":"<code>from_mask(mask, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a bounding box using a NumPy array mask.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>NumPy array mask.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BBox</code> <p>The bounding box.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>@staticmethod\ndef from_mask(mask: np.ndarray, **kwargs: Any) -&gt; \"BBox\":\n    \"\"\"Create a bounding box using a NumPy array mask.\n\n    Args:\n        mask: NumPy array mask.\n        kwargs: Additional arguments.\n\n    Returns:\n        The bounding box.\n    \"\"\"\n    return BBox.from_xywh(\n        xywh=bbox_utils.mask_to_bbox(mask),\n        is_normalized=True,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.from_rle","title":"<code>from_rle(rle, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a bounding box using a RLE mask.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>CompressedRLE</code> <p>RLE mask.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BBox</code> <p>The bounding box.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>@staticmethod\ndef from_rle(\n    rle: CompressedRLE,\n    **kwargs: Any,\n) -&gt; \"BBox\":\n    \"\"\"Create a bounding box using a RLE mask.\n\n    Args:\n        rle: RLE mask.\n        kwargs: Additional arguments.\n\n    Returns:\n        The bounding box.\n    \"\"\"\n    return BBox.from_mask(mask=rle.to_mask(), **kwargs)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.from_xywh","title":"<code>from_xywh(xywh, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a bounding box using normalized xywh coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>xywh</code> <code>list[float]</code> <p>List of coordinates in xywh format.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BBox</code> <p>The bounding box.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>@staticmethod\ndef from_xywh(\n    xywh: list[float],\n    **kwargs: Any,\n) -&gt; \"BBox\":\n    \"\"\"Create a bounding box using normalized xywh coordinates.\n\n    Args:\n        xywh: List of coordinates in xywh format.\n        kwargs: Additional arguments.\n\n    Returns:\n        The bounding box.\n    \"\"\"\n    return BBox(\n        coords=xywh,\n        format=\"xywh\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.from_xyxy","title":"<code>from_xyxy(xyxy, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a bounding box using normalized xyxy coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>list[float]</code> <p>List of coordinates in xyxy format.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BBox</code> <p>The bounding box.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>@staticmethod\ndef from_xyxy(\n    xyxy: list[float],\n    **kwargs: Any,\n) -&gt; \"BBox\":\n    \"\"\"Create a bounding box using normalized xyxy coordinates.\n\n    Args:\n        xyxy: List of coordinates in xyxy format.\n        kwargs: Additional arguments.\n\n    Returns:\n        The bounding box.\n    \"\"\"\n    return BBox(\n        coords=xyxy,\n        format=\"xyxy\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a <code>None</code> equivalent. Should be removed as soon as Lance manages <code>None</code> value.</p> <p>Returns:</p> Type Description <code>Self</code> <p>\"None\" <code>BBox</code>.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Utility function to get a `None` equivalent.\n    Should be removed as soon as Lance manages `None` value.\n\n    Returns:\n        \"None\" `BBox`.\n    \"\"\"\n    return cls(\n        id=\"\",\n        item=ItemRef.none(),\n        view=ViewRef.none(),\n        entity=EntityRef.none(),\n        coords=[0.0, 0.0, 0.0, 0.0],\n        format=\"xywh\",\n        is_normalized=True,\n        confidence=-1,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.normalize","title":"<code>normalize(height, width)</code>","text":"<p>Return the bounding box with coordinates normalized relatively to the image size.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Image height.</p> required <code>width</code> <code>int</code> <p>Image width.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>Bounding box with coordinates normalized relatively to the image size.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>def normalize(self, height: int, width: int) -&gt; Self:\n    \"\"\"Return the bounding box with coordinates normalized relatively to the image size.\n\n    Args:\n        height: Image height.\n        width: Image width.\n\n    Returns:\n        Bounding box with coordinates normalized relatively to the image size.\n    \"\"\"\n    return BBox(\n        coords=bbox_utils.normalize_coords(self.coords, height, width),\n        format=self.format,\n        is_normalized=True,\n        confidence=self.confidence,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.to_xywh","title":"<code>to_xywh()</code>","text":"<p>Return the bounding box in xywh format.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Bounding box in xyxy format.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>def to_xywh(self) -&gt; Self:\n    \"\"\"Return the bounding box in xywh format.\n\n    Returns:\n        Bounding box in xyxy format.\n    \"\"\"\n    return BBox(\n        coords=self.xywh_coords,\n        format=\"xywh\",\n        is_normalized=self.is_normalized,\n        confidence=self.confidence,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox.to_xyxy","title":"<code>to_xyxy()</code>","text":"<p>Return the bounding box in xyxy format.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Bounding box in xyxy format.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>def to_xyxy(self) -&gt; Self:\n    \"\"\"Return the bounding box in xyxy format.\n\n    Returns:\n        Bounding box in xyxy format.\n    \"\"\"\n    return BBox(\n        coords=self.xyxy_coords,\n        format=\"xyxy\",\n        is_normalized=self.is_normalized,\n        confidence=self.confidence,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox3D","title":"<code>BBox3D(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>A 3D bounding Box.</p> <p>Attributes:</p> Name Type Description <code>coords</code> <code>list[float]</code> <p>List of coordinates in given format.</p> <code>format</code> <code>str</code> <p>Coordinates format, 'xyzxyz' or 'xyzwhd'.</p> <code>heading</code> <code>list[float]</code> <p>Orientation of the bounding box.</p> <code>is_normalized</code> <code>bool</code> <p>True if coordinates are normalized to image size.</p> <code>confidence</code> <code>float</code> <p>Bounding box confidence if predicted. -1 if not predicted.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.BBox3D.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a <code>None</code> equivalent. Should be removed as soon as Lance manages <code>None</code> value.</p> <p>Returns:</p> Type Description <code>Self</code> <p>\"None\" BBox3D.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Utility function to get a `None` equivalent.\n    Should be removed as soon as Lance manages `None` value.\n\n    Returns:\n        \"None\" BBox3D.\n    \"\"\"\n    return cls(\n        id=\"\",\n        item=ItemRef.none(),\n        view=ViewRef.none(),\n        entity=EntityRef.none(),\n        coords=[0, 0, 0, 0, 0, 0],\n        format=\"xyzwhd\",\n        heading=[0, 0, 0],\n        is_normalized=True,\n        confidence=-1,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.create_bbox","title":"<code>create_bbox(coords, format, is_normalized, confidence=-1, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none())</code>","text":"<p>Create a <code>BBox</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list[float]</code> <p>List of coordinates in given format.</p> required <code>format</code> <code>Literal['xyxy', 'xywh']</code> <p>Coordinates format, 'xyxy' or 'xywh'.</p> required <code>is_normalized</code> <code>bool</code> <p>True if coordinates are normalized to image size.</p> required <code>confidence</code> <code>float</code> <p>Bounding box confidence if predicted.</p> <code>-1</code> <code>id</code> <code>str</code> <p>BBox ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>BBox</code> <p>The created <code>BBox</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>def create_bbox(\n    coords: list[float],\n    format: Literal[\"xyxy\", \"xywh\"],\n    is_normalized: bool,\n    confidence: float = -1,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n) -&gt; BBox:\n    \"\"\"Create a `BBox` instance.\n\n    Args:\n        coords: List of coordinates in given format.\n        format: Coordinates format, 'xyxy' or 'xywh'.\n        is_normalized: True if coordinates are normalized to image size.\n        confidence: Bounding box confidence if predicted.\n        id: BBox ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n\n    Returns:\n        The created `BBox` instance.\n    \"\"\"\n    return BBox(\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n        coords=coords,\n        format=str(format),\n        is_normalized=is_normalized,\n        confidence=confidence,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.create_bbox3d","title":"<code>create_bbox3d(coords, format, heading, is_normalized, confidence=-1.0, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none())</code>","text":"<p>Create a <code>BBox3D</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list[float]</code> <p>The 3D position coordinates.</p> required <code>format</code> <code>Literal['xyzxyz', 'xyzwhd']</code> <p>Coordinates format, 'xyzxyz' or 'xyzwhd'.</p> required <code>heading</code> <code>list[float]</code> <p>The orientation.</p> required <code>is_normalized</code> <code>bool</code> <p>True if coordinates are normalized to image size.</p> required <code>confidence</code> <code>float</code> <p>Bounding box confidence if predicted.</p> <code>-1.0</code> <code>id</code> <code>str</code> <p>BBox3D ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>BBox3D</code> <p>The created <code>BBox3D</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>def create_bbox3d(\n    coords: list[float],\n    format: Literal[\"xyzxyz\", \"xyzwhd\"],\n    heading: list[float],\n    is_normalized: bool,\n    confidence: float = -1.0,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n) -&gt; BBox3D:\n    \"\"\"Create a `BBox3D` instance.\n\n    Args:\n        coords: The 3D position coordinates.\n        format: Coordinates format, 'xyzxyz' or 'xyzwhd'.\n        heading: The orientation.\n        is_normalized: True if coordinates are normalized to image size.\n        confidence: Bounding box confidence if predicted.\n        id: BBox3D ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n\n    Returns:\n        The created `BBox3D` instance.\n    \"\"\"\n    return BBox3D(\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n        coords=coords,\n        format=str(format),\n        heading=heading,\n        is_normalized=is_normalized,\n        confidence=confidence,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.is_bbox","title":"<code>is_bbox(cls, strict=False)</code>","text":"<p>Check if a class is a <code>BBox</code> or a subclass of <code>BBox</code>.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>def is_bbox(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `BBox` or a subclass of `BBox`.\"\"\"\n    return issubclass_strict(cls, BBox, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/bbox/#pixano.features.schemas.annotations.bbox.is_bbox3d","title":"<code>is_bbox3d(cls, strict=False)</code>","text":"<p>Check if a class is a <code>BBox3D</code> or subclass of <code>BBox3D</code>.</p> Source code in <code>pixano/features/schemas/annotations/bbox.py</code> <pre><code>def is_bbox3d(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `BBox3D` or subclass of `BBox3D`.\"\"\"\n    return issubclass_strict(cls, BBox3D, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/camcalibration/","title":"camcalibration","text":""},{"location":"api_reference/features/schemas/annotations/camcalibration/#pixano.features.schemas.annotations.camcalibration","title":"<code>pixano.features.schemas.annotations.camcalibration</code>","text":""},{"location":"api_reference/features/schemas/annotations/camcalibration/#pixano.features.schemas.annotations.camcalibration.BaseIntrinsics","title":"<code>BaseIntrinsics</code>","text":"<p>               Bases: <code>BaseType</code></p> <p>BaseIntrinsics (TODO: description?).</p> <p>Attributes:</p> Name Type Description <code>cx_offset_px</code> <code>float</code> <p>cx_offset_px</p> <code>cy_offset_px</code> <code>float</code> <p>cy_offset_px</p> <code>img_height_px</code> <code>int</code> <p>img_height_px</p> <code>img_width_px</code> <code>int</code> <p>img_width_px</p>"},{"location":"api_reference/features/schemas/annotations/camcalibration/#pixano.features.schemas.annotations.camcalibration.CamCalibration","title":"<code>CamCalibration(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>Camera calibration.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>Type of camera.</p> <code>base_intrinsics</code> <code>BaseIntrinsics</code> <p>Base intrinsics values.</p> <code>extrinsics</code> <code>Extrinsics</code> <p>Extrinsics values.</p> <code>intrinsics</code> <code>Intrinsics</code> <p>Intrinsics values.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/camcalibration/#pixano.features.schemas.annotations.camcalibration.CamCalibration.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a <code>None</code> equivalent. Should be removed as soon as Lance manages <code>None</code> value.</p> <p>Returns:</p> Type Description <code>Self</code> <p>\"None\" <code>CamCalibration</code>.</p> Source code in <code>pixano/features/schemas/annotations/camcalibration.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Utility function to get a `None` equivalent.\n    Should be removed as soon as Lance manages `None` value.\n\n    Returns:\n        \"None\" `CamCalibration`.\n    \"\"\"\n    return cls(\n        id=\"\",\n        item_ref=ItemRef.none(),\n        view_ref=ViewRef.none(),\n        entity_ref=EntityRef.none(),\n        type=\"\",\n        base_intrinsics=BaseIntrinsics(\n            cx_offset_px=0.0,\n            cy_offset_px=0.0,\n            img_height_px=0,\n            img_width_px=0,\n        ),\n        extrinsics=Extrinsics(\n            pos_x_m=0.0,\n            pos_y_m=0.0,\n            pos_z_m=0.0,\n            rot_x_deg=0.0,\n            rot_z1_deg=0.0,\n            rot_z2_deg=0.0,\n        ),\n        intrinsics=Intrinsics(\n            c1=0.0,\n            c2=0.0,\n            c3=0.0,\n            c4=0.0,\n            pixel_aspect_ratio=0.0,\n        ),\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/camcalibration/#pixano.features.schemas.annotations.camcalibration.Extrinsics","title":"<code>Extrinsics</code>","text":"<p>               Bases: <code>BaseType</code></p> <p>Extrinsics (TODO: description?).</p> <p>Attributes:</p> Name Type Description <code>pos_x_m</code> <code>float</code> <p>pos_x_m.</p> <code>pos_y_m</code> <code>float</code> <p>pos_y_m.</p> <code>pos_z_m</code> <code>float</code> <p>pos_z_m.</p> <code>rot_x_deg</code> <code>float</code> <p>rot_x_deg.</p> <code>rot_z1_deg</code> <code>float</code> <p>rot_z1_deg.</p> <code>rot_z2_deg</code> <code>float</code> <p>rot_z2_deg.</p>"},{"location":"api_reference/features/schemas/annotations/camcalibration/#pixano.features.schemas.annotations.camcalibration.Intrinsics","title":"<code>Intrinsics</code>","text":"<p>               Bases: <code>BaseType</code></p> <p>Intrinsics (TODO: description?).</p> <p>Attributes:</p> Name Type Description <code>c1</code> <code>float</code> <p>c1.</p> <code>c2</code> <code>float</code> <p>c2.</p> <code>c3</code> <code>float</code> <p>c3.</p> <code>c4</code> <code>float</code> <p>c4.</p> <code>pixel_aspect_ratio</code> <code>float</code> <p>pixel_aspect_ratio.</p>"},{"location":"api_reference/features/schemas/annotations/camcalibration/#pixano.features.schemas.annotations.camcalibration.create_cam_calibration","title":"<code>create_cam_calibration(type, base_intrinsics=None, extrinsics=None, intrinsics=None, cx_offset_px=None, cy_offset_px=None, img_height_px=None, img_width_px=None, pos_x_m=None, pos_y_m=None, pos_z_m=None, rot_x_deg=None, rot_z1_deg=None, rot_z2_deg=None, c1=None, c2=None, c3=None, c4=None, pixel_aspect_ratio=None, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none(), validate=True)</code>","text":"<p>Create a <code>CamCalibration</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>The type of camera.</p> required <code>base_intrinsics</code> <code>BaseIntrinsics | None</code> <p>The base intrinsics.</p> <code>None</code> <code>extrinsics</code> <code>Extrinsics | None</code> <p>The extrinsics.</p> <code>None</code> <code>intrinsics</code> <code>Intrinsics | None</code> <p>The intrinsics.</p> <code>None</code> <code>cx_offset_px</code> <code>float | None</code> <p>cx_offset_px.</p> <code>None</code> <code>cy_offset_px</code> <code>float | None</code> <p>cy_offset_px.</p> <code>None</code> <code>img_height_px</code> <code>int | None</code> <p>img_height_px.</p> <code>None</code> <code>img_width_px</code> <code>int | None</code> <p>img_width_px.</p> <code>None</code> <code>pos_x_m</code> <code>float | None</code> <p>pos_x_m.</p> <code>None</code> <code>pos_y_m</code> <code>float | None</code> <p>pos_y_m.</p> <code>None</code> <code>pos_z_m</code> <code>float | None</code> <p>pos_z_m.</p> <code>None</code> <code>rot_x_deg</code> <code>float | None</code> <p>rot_x_deg.</p> <code>None</code> <code>rot_z1_deg</code> <code>float | None</code> <p>rot_z1_deg.</p> <code>None</code> <code>rot_z2_deg</code> <code>float | None</code> <p>rot_z2_deg.</p> <code>None</code> <code>c1</code> <code>float | None</code> <p>c1.</p> <code>None</code> <code>c2</code> <code>float | None</code> <p>c2.</p> <code>None</code> <code>c3</code> <code>float | None</code> <p>c3.</p> <code>None</code> <code>c4</code> <code>float | None</code> <p>c4.</p> <code>None</code> <code>pixel_aspect_ratio</code> <code>float | None</code> <p>pixel_aspect_ratio.</p> <code>None</code> <code>id</code> <code>str</code> <p><code>CamCalibration</code> ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <code>validate</code> <code>bool</code> <p>Set to False to skip pydantic validation.</p> <code>True</code> <p>Returns:</p> Type Description <code>CamCalibration</code> <p>The created <code>CamCalibration</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/camcalibration.py</code> <pre><code>def create_cam_calibration(\n    type: str,\n    base_intrinsics: BaseIntrinsics | None = None,\n    extrinsics: Extrinsics | None = None,\n    intrinsics: Intrinsics | None = None,\n    cx_offset_px: float | None = None,\n    cy_offset_px: float | None = None,\n    img_height_px: int | None = None,\n    img_width_px: int | None = None,\n    pos_x_m: float | None = None,\n    pos_y_m: float | None = None,\n    pos_z_m: float | None = None,\n    rot_x_deg: float | None = None,\n    rot_z1_deg: float | None = None,\n    rot_z2_deg: float | None = None,\n    c1: float | None = None,\n    c2: float | None = None,\n    c3: float | None = None,\n    c4: float | None = None,\n    pixel_aspect_ratio: float | None = None,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n    validate: bool = True,\n) -&gt; CamCalibration:\n    \"\"\"Create a `CamCalibration` instance.\n\n    Args:\n        type: The type of camera.\n        base_intrinsics: The base intrinsics.\n        extrinsics: The extrinsics.\n        intrinsics: The intrinsics.\n        cx_offset_px: cx_offset_px.\n        cy_offset_px: cy_offset_px.\n        img_height_px: img_height_px.\n        img_width_px: img_width_px.\n        pos_x_m: pos_x_m.\n        pos_y_m: pos_y_m.\n        pos_z_m: pos_z_m.\n        rot_x_deg: rot_x_deg.\n        rot_z1_deg: rot_z1_deg.\n        rot_z2_deg: rot_z2_deg.\n        c1: c1.\n        c2: c2.\n        c3: c3.\n        c4: c4.\n        pixel_aspect_ratio: pixel_aspect_ratio.\n        id: `CamCalibration` ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n        validate: Set to False to skip pydantic validation.\n\n    Returns:\n        The created `CamCalibration` instance.\n    \"\"\"\n    none_obj_conditions = [\n        base_intrinsics is None,\n        extrinsics is None,\n        intrinsics is None,\n    ]\n    not_none_obj_conditions = [\n        base_intrinsics is not None,\n        extrinsics is not None,\n        intrinsics is not None,\n    ]\n\n    none_field_conditions = [\n        cx_offset_px is None,\n        cy_offset_px is None,\n        img_height_px is None,\n        img_width_px is None,\n        pos_x_m is None,\n        pos_y_m is None,\n        pos_z_m is None,\n        rot_x_deg is None,\n        rot_z1_deg is None,\n        rot_z2_deg is None,\n        c1 is None,\n        c2 is None,\n        c3 is None,\n        c4 is None,\n        pixel_aspect_ratio is None,\n    ]\n\n    not_none_field_conditions = [\n        cx_offset_px is not None,\n        cy_offset_px is not None,\n        img_height_px is not None,\n        img_width_px is not None,\n        pos_x_m is not None,\n        pos_y_m is not None,\n        pos_z_m is not None,\n        rot_x_deg is not None,\n        rot_z1_deg is not None,\n        rot_z2_deg is not None,\n        c1 is not None,\n        c2 is not None,\n        c3 is not None,\n        c4 is not None,\n        pixel_aspect_ratio is not None,\n    ]\n\n    if not all(none_obj_conditions) and not all(not_none_obj_conditions):\n        raise ValueError(\"base_intrinsics, extrinsics and intrinsics must be all defined or all None\")\n    elif not all(none_field_conditions) and not all(not_none_field_conditions):\n        raise ValueError(\n            \"cx_offset_px, cy_offset_px, img_height_px, img_width_px, pos_x_m, pos_y_m, pos_z_m, \"\n            \"rot_x_deg, rot_z1_deg, rot_z2_deg, c1, c2, c3, c4 and pixel_aspect_ratio must be all \"\n            \"defined or all None\"\n        )\n    elif any(not_none_obj_conditions) and any(not_none_field_conditions):\n        raise ValueError(\n            \"base_intrinsics, extrinsics and intrinsics must defined or cx_offset_px, cy_offset_px, img_height_px, \"\n            \"img_width_px, pos_x_m, pos_y_m, pos_z_m, rot_x_deg, rot_z1_deg, rot_z2_deg, c1, c2, c3, c4 and \"\n            \"pixel_aspect_ratio must be defined but not both\"\n        )\n    if any(not_none_field_conditions):\n        if validate:\n            base_intrinsics = BaseIntrinsics(\n                cx_offset_px=cx_offset_px,\n                cy_offset_px=cy_offset_px,\n                img_height_px=img_height_px,\n                img_width_px=img_width_px,\n            )\n            extrinsics = Extrinsics(\n                pos_x_m=pos_x_m,\n                pos_y_m=pos_y_m,\n                pos_z_m=pos_z_m,\n                rot_x_deg=rot_x_deg,\n                rot_z1_deg=rot_z1_deg,\n                rot_z2_deg=rot_z2_deg,\n            )\n            intrinsics = Intrinsics(\n                c1=c1,\n                c2=c2,\n                c3=c3,\n                c4=c4,\n                pixel_aspect_ratio=pixel_aspect_ratio,\n            )\n        else:\n            base_intrinsics = BaseIntrinsics.construct(\n                cx_offset_px=cx_offset_px,\n                cy_offset_px=cy_offset_px,\n                img_height_px=img_height_px,\n                img_width_px=img_width_px,\n            )\n            extrinsics = Extrinsics.construct(\n                pos_x_m=pos_x_m,\n                pos_y_m=pos_y_m,\n                pos_z_m=pos_z_m,\n                rot_x_deg=rot_x_deg,\n                rot_z1_deg=rot_z1_deg,\n                rot_z2_deg=rot_z2_deg,\n            )\n            intrinsics = Intrinsics.construct(\n                c1=c1,\n                c2=c2,\n                c3=c3,\n                c4=c4,\n                pixel_aspect_ratio=pixel_aspect_ratio,\n            )\n\n    if validate:\n        return CamCalibration(\n            type=type,\n            base_intrinsics=base_intrinsics,\n            extrinsics=extrinsics,\n            intrinsics=intrinsics,\n            id=id,\n            item_ref=item_ref,\n            view_ref=view_ref,\n            entity_ref=entity_ref,\n            source_ref=source_ref,\n        )\n    else:\n        return CamCalibration.construct(\n            type=type,\n            base_intrinsics=base_intrinsics,\n            extrinsics=extrinsics,\n            intrinsics=intrinsics,\n            id=id,\n            item_ref=item_ref,\n            view_ref=view_ref,\n            entity_ref=entity_ref,\n            source_ref=source_ref,\n        )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/camcalibration/#pixano.features.schemas.annotations.camcalibration.is_cam_calibration","title":"<code>is_cam_calibration(cls, strict=False)</code>","text":"<p>Check if a class is a <code>CamCalibration</code> or subclass of <code>CamCalibration</code>.</p> Source code in <code>pixano/features/schemas/annotations/camcalibration.py</code> <pre><code>def is_cam_calibration(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `CamCalibration` or subclass of `CamCalibration`.\"\"\"\n    return issubclass_strict(cls, CamCalibration, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/classification/","title":"classification","text":""},{"location":"api_reference/features/schemas/annotations/classification/#pixano.features.schemas.annotations.classification","title":"<code>pixano.features.schemas.annotations.classification</code>","text":""},{"location":"api_reference/features/schemas/annotations/classification/#pixano.features.schemas.annotations.classification.Classification","title":"<code>Classification(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>Classification at the media level (Image or Text).</p> <p>Attributes:</p> Name Type Description <code>labels</code> <code>list[str]</code> <p>List of class names.</p> <code>confidences</code> <code>list[float]</code> <p>List of prediction confidences.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/classification/#pixano.features.schemas.annotations.classification.Classification.predictions","title":"<code>predictions</code>  <code>property</code>","text":"<p>Get list of zipped predictions (labels and confidences).</p>"},{"location":"api_reference/features/schemas/annotations/classification/#pixano.features.schemas.annotations.classification.Classification.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a <code>None</code> equivalent. Should be removed as soon as Lance manages <code>None</code> value.</p> <p>Returns:</p> Type Description <code>Classification</code> <p>\"None\" Classification.</p> Source code in <code>pixano/features/schemas/annotations/classification.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; \"Classification\":\n    \"\"\"Utility function to get a `None` equivalent.\n    Should be removed as soon as Lance manages `None` value.\n\n    Returns:\n        \"None\" Classification.\n    \"\"\"\n    return cls(\n        id=\"\",\n        item=ItemRef.none(),\n        view=ViewRef.none(),\n        entity=EntityRef.none(),\n        labels=[],\n        confidences=[],\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/classification/#pixano.features.schemas.annotations.classification.create_classification","title":"<code>create_classification(labels, confidences, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none())</code>","text":"<p>Create a <code>Classification</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>list[str]</code> <p>List of class names.</p> required <code>confidences</code> <code>list[float]</code> <p>List of prediction confidences.</p> required <code>id</code> <code>str</code> <p><code>Classification</code> ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>Classification</code> <p>The created <code>Classification</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/classification.py</code> <pre><code>def create_classification(\n    labels: list[str],\n    confidences: list[float],\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n) -&gt; Classification:\n    \"\"\"Create a `Classification` instance.\n\n    Args:\n        labels: List of class names.\n        confidences: List of prediction confidences.\n        id: `Classification` ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n\n    Returns:\n        The created `Classification` instance.\n    \"\"\"\n    return Classification(\n        labels=labels,\n        confidences=confidences,\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/classification/#pixano.features.schemas.annotations.classification.is_classification","title":"<code>is_classification(cls, strict=False)</code>","text":"<p>Check if a class is a <code>Classification</code> or subclass of <code>Classification</code>.</p> Source code in <code>pixano/features/schemas/annotations/classification.py</code> <pre><code>def is_classification(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `Classification` or subclass of `Classification`.\"\"\"\n    return issubclass_strict(cls, Classification, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/","title":"compressed_rle","text":""},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle","title":"<code>pixano.features.schemas.annotations.compressed_rle</code>","text":""},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE","title":"<code>CompressedRLE(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>Compressed RLE mask type.</p> <p>Attributes:</p> Name Type Description <code>size</code> <code>list[int]</code> <p>Mask size.</p> <code>counts</code> <code>bytes</code> <p>Mask RLE encoding.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE.area","title":"<code>area</code>  <code>property</code>","text":"<p>Return mask area.</p> <p>Returns:</p> Type Description <code>float</code> <p>Mask area</p>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE.encode","title":"<code>encode(mask, height, width, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a compressed RLE mask from polygons / uncompressed RLE / compressed RLE.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list[list] | dict[str, list[int]]</code> <p>Mask as polygons / uncompressed RLE / compressed RLE.</p> required <code>height</code> <code>int</code> <p>Image height.</p> required <code>width</code> <code>int</code> <p>Image width.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CompressedRLE</code> <p>The compressed RLE mask.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>@staticmethod\ndef encode(mask: list[list] | dict[str, list[int]], height: int, width: int, **kwargs: Any) -&gt; \"CompressedRLE\":\n    \"\"\"Create a compressed RLE mask from polygons / uncompressed RLE / compressed RLE.\n\n    Args:\n        mask: Mask as polygons / uncompressed RLE / compressed RLE.\n        height: Image height.\n        width: Image width.\n        kwargs: Additional arguments.\n\n    Returns:\n        The compressed RLE mask.\n    \"\"\"\n    return CompressedRLE(**image_utils.encode_rle(mask, height, width), **kwargs)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE.from_mask","title":"<code>from_mask(mask, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a compressed RLE mask from a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Image | ndarray</code> <p>The mask as a NumPy array.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CompressedRLE</code> <p>The compressed RLE mask.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>@staticmethod\ndef from_mask(mask: pil_image.Image | np.ndarray, **kwargs: Any) -&gt; \"CompressedRLE\":\n    \"\"\"Create a compressed RLE mask from a NumPy array.\n\n    Args:\n        mask: The mask as a NumPy array.\n        kwargs: Additional arguments.\n\n    Returns:\n        The compressed RLE mask.\n    \"\"\"\n    rle = image_utils.mask_to_rle(mask)\n    return CompressedRLE(size=rle[\"size\"], counts=rle[\"counts\"], **kwargs)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE.from_polygons","title":"<code>from_polygons(polygons, height, width, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a compressed RLE mask from polygons.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>list[list]</code> <p>The mask as polygons.</p> required <code>height</code> <code>int</code> <p>Image height.</p> required <code>width</code> <code>int</code> <p>Image width.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CompressedRLE</code> <p>The compressed RLE mask.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>@staticmethod\ndef from_polygons(\n    polygons: list[list],\n    height: int,\n    width: int,\n    **kwargs: Any,\n) -&gt; \"CompressedRLE\":\n    \"\"\"Create a compressed RLE mask from polygons.\n\n    Args:\n        polygons: The mask as polygons.\n        height: Image height.\n        width: Image width.\n        kwargs: Additional arguments.\n\n    Returns:\n        The compressed RLE mask.\n    \"\"\"\n    return CompressedRLE(**image_utils.polygons_to_rle(polygons, height, width), **kwargs)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE.from_urle","title":"<code>from_urle(urle, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a compressed RLE mask from an uncompressed RLE.</p> <p>Parameters:</p> Name Type Description Default <code>urle</code> <code>dict[str, list[int]]</code> <p>The mask as an uncompressed RLE.</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CompressedRLE</code> <p>The compressed RLE mask.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>@staticmethod\ndef from_urle(urle: dict[str, list[int]], **kwargs: Any) -&gt; \"CompressedRLE\":\n    \"\"\"Create a compressed RLE mask from an uncompressed RLE.\n\n    Args:\n        urle: The mask as an uncompressed RLE.\n        kwargs: Additional arguments.\n\n    Returns:\n        The compressed RLE mask.\n    \"\"\"\n    return CompressedRLE(**image_utils.urle_to_rle(urle), **kwargs)  #  type: ignore[arg-type]\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a <code>None</code> equivalent. Should be removed as soon as Lance manages <code>None</code> value.</p> <p>Returns:</p> Type Description <code>Self</code> <p>\"None\" <code>CompressedRLE</code>.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Utility function to get a `None` equivalent.\n    Should be removed as soon as Lance manages `None` value.\n\n    Returns:\n        \"None\" `CompressedRLE`.\n    \"\"\"\n    return cls(\n        id=\"\",\n        item=ItemRef.none(),\n        view=ViewRef.none(),\n        entity=EntityRef.none(),\n        size=[0, 0],\n        counts=b\"\",\n        created_at=datetime(1970, 1, 1),\n        updated_at=datetime(1970, 1, 1),\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE.to_mask","title":"<code>to_mask()</code>","text":"<p>Convert the compressed RLE mask to a NumPy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The mask as a NumPy array.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>def to_mask(self) -&gt; np.ndarray:\n    \"\"\"Convert the compressed RLE mask to a NumPy array.\n\n    Returns:\n        The mask as a NumPy array.\n    \"\"\"\n    return image_utils.rle_to_mask({\"size\": self.size, \"counts\": self.counts})\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE.to_polygons","title":"<code>to_polygons()</code>","text":"<p>Convert the compressed RLE mask to poylgons.</p> <p>Returns:</p> Type Description <code>list[list]</code> <p>The mask as polygons.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>def to_polygons(self) -&gt; list[list]:\n    \"\"\"Convert the compressed RLE mask to poylgons.\n\n    Returns:\n        The mask as polygons.\n    \"\"\"\n    return image_utils.rle_to_polygons(self.model_dump())\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.CompressedRLE.to_urle","title":"<code>to_urle()</code>","text":"<p>Convert compressed RLE mask to uncompressed RLE.</p> <p>Returns:</p> Type Description <code>dict[str, list[int]]</code> <p>The mask as an uncompressed RLE.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>def to_urle(self) -&gt; dict[str, list[int]]:\n    \"\"\"Convert compressed RLE mask to uncompressed RLE.\n\n    Returns:\n        The mask as an uncompressed RLE.\n    \"\"\"\n    return image_utils.rle_to_urle(self.model_dump())\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.create_compressed_rle","title":"<code>create_compressed_rle(size, counts, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none())</code>","text":"<p>Create a <code>CompressedRLE</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>list[int]</code> <p>Mask size.</p> required <code>counts</code> <code>bytes</code> <p>Mask RLE encoding.</p> required <code>id</code> <code>str</code> <p><code>CompressedRLE</code> ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>CompressedRLE</code> <p>The compressed RLE instance.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>def create_compressed_rle(\n    size: list[int],\n    counts: bytes,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n) -&gt; CompressedRLE:\n    \"\"\"Create a `CompressedRLE` instance.\n\n    Args:\n        size: Mask size.\n        counts: Mask RLE encoding.\n        id: `CompressedRLE` ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n\n    Returns:\n        The compressed RLE instance.\n    \"\"\"\n    return CompressedRLE(\n        size=size,\n        counts=counts,\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/compressed_rle/#pixano.features.schemas.annotations.compressed_rle.is_compressed_rle","title":"<code>is_compressed_rle(cls, strict=False)</code>","text":"<p>Check if the given class is a subclass of <code>CompressedRLE</code>.</p> Source code in <code>pixano/features/schemas/annotations/compressed_rle.py</code> <pre><code>def is_compressed_rle(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is a subclass of `CompressedRLE`.\"\"\"\n    return issubclass_strict(cls, CompressedRLE, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/info_extraction/","title":"info_extraction","text":""},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction","title":"<code>pixano.features.schemas.annotations.info_extraction</code>","text":""},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.Relation","title":"<code>Relation(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>Observation of a relation between two annotations,    for instance between text-spans in a text.</p> <p>Attributes:</p> Name Type Description <code>predicate</code> <code>str</code> <p>type of relation, as in semantic-web (OWL, RDF, etc)</p> <code>subject_ref</code> <code>AnnotationRef</code> <p>annotation_ref to the subject Annotation (eg TextSpan)</p> <code>object_ref</code> <code>AnnotationRef</code> <p>annotation_ref to the object Annotation (eg TextSpan)</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.Relation.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a <code>None</code> equivalent. Should be removed as soon as Lance manages <code>None</code> value.</p> <p>Returns:</p> Type Description <code>Self</code> <p>\"None\" Relation.</p> Source code in <code>pixano/features/schemas/annotations/info_extraction.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Utility function to get a `None` equivalent.\n    Should be removed as soon as Lance manages `None` value.\n\n    Returns:\n        \"None\" Relation.\n    \"\"\"\n    return cls(\n        id=\"\",\n        predicate=\"\",\n        item_ref=ItemRef.none(),\n        view_ref=ViewRef.none(),\n        entity_ref=EntityRef.none(),\n        subject_ref=AnnotationRef.none(),\n        object_ref=AnnotationRef.none(),\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.TextSpan","title":"<code>TextSpan(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>Designation of a Text-Span in a text, especially in the    use-case of Named-Entity Recognition on a Text view    having a str 'content' attribute.</p> <p>Attributes:</p> Name Type Description <code>mention</code> <code>str</code> <p>text-span assembled mention.</p> <code>spans_start</code> <code>list[int]</code> <p>List of start offsets of the spans in the text.</p> <code>spans_end</code> <code>list[int]</code> <p>List of end offsets of the spans in the text.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.TextSpan.spans","title":"<code>spans</code>  <code>property</code>","text":"<p>Get the list of zipped spans offsets (starts and ends).</p>"},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.TextSpan.spans_length","title":"<code>spans_length</code>  <code>property</code>","text":"<p>Get the computed list of spans lengths.</p>"},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.TextSpan.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a <code>None</code> equivalent. Should be removed as soon as Lance manages <code>None</code> value.</p> <p>Returns:</p> Type Description <code>Self</code> <p>\"None\" TextSpan.</p> Source code in <code>pixano/features/schemas/annotations/info_extraction.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Utility function to get a `None` equivalent.\n    Should be removed as soon as Lance manages `None` value.\n\n    Returns:\n        \"None\" TextSpan.\n    \"\"\"\n    return cls(\n        id=\"\",\n        item_ref=ItemRef.none(),\n        view_ref=ViewRef.none(),\n        entity_ref=EntityRef.none(),\n        mention=\"\",\n        spans_start=[],\n        spans_end=[],\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.create_relation","title":"<code>create_relation(predicate, subject_ref=AnnotationRef.none(), object_ref=AnnotationRef.none(), id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none())</code>","text":"<p>Create a <code>Relation</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>str</code> <p>type of relation</p> required <code>subject_ref</code> <code>AnnotationRef</code> <p>annotation_ref to the subject TextSpan</p> <code>none()</code> <code>object_ref</code> <code>AnnotationRef</code> <p>annotation_ref to the object TextSpan</p> <code>none()</code> <code>id</code> <code>str</code> <p>Relation ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>Relation</code> <p>The created <code>Relation</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/info_extraction.py</code> <pre><code>def create_relation(\n    predicate: str,\n    subject_ref: AnnotationRef = AnnotationRef.none(),\n    object_ref: AnnotationRef = AnnotationRef.none(),\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n) -&gt; Relation:\n    \"\"\"Create a `Relation` instance.\n\n    Args:\n        predicate: type of relation\n        subject_ref: annotation_ref to the subject TextSpan\n        object_ref: annotation_ref to the object TextSpan\n        id: Relation ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n\n    Returns:\n        The created `Relation` instance.\n    \"\"\"\n    return Relation(\n        predicate=predicate,\n        subject_ref=subject_ref,\n        object_ref=object_ref,\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.create_text_span","title":"<code>create_text_span(mention, spans_start, spans_end, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none())</code>","text":"<p>Create a TextSpan instance.</p> <p>Parameters:</p> Name Type Description Default <code>mention</code> <code>str</code> <p>text-span observed mention.</p> required <code>spans_start</code> <code>list[int]</code> <p>List of start offsets of the spans in the text.</p> required <code>spans_end</code> <code>list[int]</code> <p>List of end offsets of the spans in the text.</p> required <code>id</code> <code>str</code> <p>TextSpan ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference toward a Text view having a str 'content' attribute.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>TextSpan</code> <p>The created <code>TextSpan</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/info_extraction.py</code> <pre><code>def create_text_span(\n    mention: str,\n    spans_start: list[int],\n    spans_end: list[int],\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n) -&gt; TextSpan:\n    \"\"\"Create a TextSpan instance.\n\n    Args:\n        mention: text-span observed mention.\n        spans_start: List of start offsets of the spans in the text.\n        spans_end: List of end offsets of the spans in the text.\n        id: TextSpan ID.\n        item_ref: Item reference.\n        view_ref: View reference toward a Text view having a str 'content' attribute.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n\n    Returns:\n        The created `TextSpan` instance.\n    \"\"\"\n    return TextSpan(\n        mention=mention,\n        spans_start=spans_start,\n        spans_end=spans_end,\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.is_relation","title":"<code>is_relation(cls, strict=False)</code>","text":"<p>Check if a class is a <code>Relation</code> or subclass of <code>Relation</code>.</p> Source code in <code>pixano/features/schemas/annotations/info_extraction.py</code> <pre><code>def is_relation(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `Relation` or subclass of `Relation`.\"\"\"\n    return issubclass_strict(cls, Relation, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/info_extraction/#pixano.features.schemas.annotations.info_extraction.is_text_span","title":"<code>is_text_span(cls, strict=False)</code>","text":"<p>Check if a class is a TextSpan or subclass of TextSpan.</p> Source code in <code>pixano/features/schemas/annotations/info_extraction.py</code> <pre><code>def is_text_span(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a TextSpan or subclass of TextSpan.\"\"\"\n    return issubclass_strict(cls, TextSpan, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/","title":"keypoints","text":""},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints","title":"<code>pixano.features.schemas.annotations.keypoints</code>","text":""},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.KeyPoints","title":"<code>KeyPoints(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>A set of keypoints.</p> <p>Attributes:</p> Name Type Description <code>template_id</code> <code>str</code> <p>Id of the keypoint template.</p> <code>coords</code> <code>list[float]</code> <p>List of 2D coordinates of the keypoints.</p> <code>states</code> <code>list[str]</code> <p>Status for each keypoint. (\"visible\", \"invisible\", \"hidden\").</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.KeyPoints.map_back2front_vertices","title":"<code>map_back2front_vertices()</code>","text":"<p>Utility function to map back format for KeyPoint to front vertices format.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If keypoints is ill-formed.</p> <p>Returns:</p> Type Description <code>list</code> <p>keypoint list for vertices front format.</p> Source code in <code>pixano/features/schemas/annotations/keypoints.py</code> <pre><code>def map_back2front_vertices(self) -&gt; list:\n    \"\"\"Utility function to map back format for KeyPoint to front vertices format.\n\n    Raises:\n        ValueError: If keypoints is ill-formed.\n\n    Returns:\n        keypoint list for vertices front format.\n    \"\"\"\n    # Check coords are even\n    if len(self.coords) % 2 != 0:\n        raise ValueError(\"There must be an even number of coords\")\n\n    result = []\n    if self.states is not None:\n        num_points = len(self.coords) // 2\n        if len(self.states) != num_points:\n            raise ValueError(\"There must be the same number of states than points\")\n\n        result = [\n            {\"x\": x, \"y\": y, \"features\": {\"state\": state}}\n            for (x, y), state in zip(zip(self.coords[0::2], self.coords[1::2]), self.states)\n        ]\n    else:\n        result = [{\"x\": x, \"y\": y} for x, y in zip(self.coords[0::2], self.coords[1::2])]\n    return result\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.KeyPoints.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a <code>None</code> equivalent. Should be removed as soon as Lance manages <code>None</code> value.</p> <p>Returns:</p> Type Description <code>KeyPoints</code> <p>\"None\" KeyPoints.</p> Source code in <code>pixano/features/schemas/annotations/keypoints.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; \"KeyPoints\":\n    \"\"\"Utility function to get a `None` equivalent.\n    Should be removed as soon as Lance manages `None` value.\n\n    Returns:\n        \"None\" KeyPoints.\n    \"\"\"\n    return cls(\n        id=\"\",\n        item=ItemRef.none(),\n        view=ViewRef.none(),\n        entity=EntityRef.none(),\n        template_id=\"\",\n        coords=[0, 0],\n        states=[\"invisible\"],\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.KeyPoints3D","title":"<code>KeyPoints3D(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>A set of 3D keypoints.</p> <p>Attributes:</p> Name Type Description <code>template_id</code> <code>str</code> <p>id of keypoint template.</p> <code>coords</code> <code>list[float]</code> <p>List of 3D coordinates of the keypoints.</p> <code>states</code> <code>list[str]</code> <p>Status for each keypoint.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.KeyPoints3D.map_back2front_vertices","title":"<code>map_back2front_vertices()</code>","text":"<p>Utility function to map back format for KeyPoint3D to front vertices format.</p> Warn <p>Not implemented for 3D keypoints.</p> Source code in <code>pixano/features/schemas/annotations/keypoints.py</code> <pre><code>def map_back2front_vertices(self) -&gt; list:\n    \"\"\"Utility function to map back format for KeyPoint3D to front vertices format.\n\n    Warn:\n        Not implemented for 3D keypoints.\n    \"\"\"\n    raise NotImplementedError(\"Not implemented for 3D keypoints.\")\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.KeyPoints3D.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a <code>None</code> equivalent. Should be removed as soon as Lance manages <code>None</code> value.</p> <p>Returns:</p> Type Description <code>KeyPoints3D</code> <p>\"None\" KeyPoints3D.</p> Source code in <code>pixano/features/schemas/annotations/keypoints.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; \"KeyPoints3D\":\n    \"\"\"Utility function to get a `None` equivalent.\n    Should be removed as soon as Lance manages `None` value.\n\n    Returns:\n        \"None\" KeyPoints3D.\n    \"\"\"\n    return cls(\n        id=\"\",\n        item=ItemRef.none(),\n        view=ViewRef.none(),\n        entity=EntityRef.none(),\n        template_id=\"\",\n        coords=[0, 0, 0],\n        states=[\"visible\"],\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.create_keypoints","title":"<code>create_keypoints(template_id, coords, states, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none())</code>","text":"<p>Create a <code>KeyPoints</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>template_id</code> <code>str</code> <p>id of keypoint template.</p> required <code>coords</code> <code>list[float]</code> <p>List of 2D coordinates of the keypoints.</p> required <code>states</code> <code>list[str]</code> <p>Status for each keypoint. (\"visible\", \"invisible\", \"hidden\").</p> required <code>id</code> <code>str</code> <p>Keypoints ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>KeyPoints</code> <p>The created <code>KeyPoints</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/keypoints.py</code> <pre><code>def create_keypoints(\n    template_id: str,\n    coords: list[float],\n    states: list[str],\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n) -&gt; KeyPoints:\n    \"\"\"Create a `KeyPoints` instance.\n\n    Args:\n        template_id: id of keypoint template.\n        coords: List of 2D coordinates of the keypoints.\n        states: Status for each keypoint. (\"visible\", \"invisible\", \"hidden\").\n        id: Keypoints ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n\n    Returns:\n        The created `KeyPoints` instance.\n    \"\"\"\n    return KeyPoints(\n        template_id=template_id,\n        coords=coords,\n        states=states,\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.create_keypoints3d","title":"<code>create_keypoints3d(template_id, coords, states, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none())</code>","text":"<p>Create a <code>KeyPoints3D</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>template_id</code> <code>str</code> <p>The id of the keypoint template.</p> required <code>coords</code> <code>list[float]</code> <p>The 3D coordinates of the keypoints.</p> required <code>states</code> <code>list[Literal['visible', 'invisble', 'hidden']]</code> <p>The visibility status for each keypoint.</p> required <code>id</code> <code>str</code> <p>Keypoints3D ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>KeyPoints3D</code> <p>The created <code>KeyPoints3D</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/keypoints.py</code> <pre><code>def create_keypoints3d(\n    template_id: str,\n    coords: list[float],\n    states: list[Literal[\"visible\", \"invisble\", \"hidden\"]],\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n) -&gt; KeyPoints3D:\n    \"\"\"Create a `KeyPoints3D` instance.\n\n    Args:\n        template_id: The id of the keypoint template.\n        coords: The 3D coordinates of the keypoints.\n        states: The visibility status for each keypoint.\n        id: Keypoints3D ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n\n    Returns:\n        The created `KeyPoints3D` instance.\n    \"\"\"\n    return KeyPoints3D(\n        template_id=template_id,\n        coords=coords,\n        states=states,\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.is_keypoints","title":"<code>is_keypoints(cls, strict=False)</code>","text":"<p>Check if a class is a <code>KeyPoints</code> or subclass of <code>KeyPoints</code>.</p> Source code in <code>pixano/features/schemas/annotations/keypoints.py</code> <pre><code>def is_keypoints(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `KeyPoints` or subclass of `KeyPoints`.\"\"\"\n    return issubclass_strict(cls, KeyPoints, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/keypoints/#pixano.features.schemas.annotations.keypoints.is_keypoints3d","title":"<code>is_keypoints3d(cls, strict=False)</code>","text":"<p>Check if a class is <code>Keypoints3D</code> or a subclass of <code>Keypoints3D</code>.</p> Source code in <code>pixano/features/schemas/annotations/keypoints.py</code> <pre><code>def is_keypoints3d(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is `Keypoints3D` or a subclass of `Keypoints3D`.\"\"\"\n    return issubclass_strict(cls, KeyPoints3D, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/text_generation/","title":"text_generation","text":""},{"location":"api_reference/features/schemas/annotations/text_generation/#pixano.features.schemas.annotations.text_generation","title":"<code>pixano.features.schemas.annotations.text_generation</code>","text":""},{"location":"api_reference/features/schemas/annotations/text_generation/#pixano.features.schemas.annotations.text_generation.Answer","title":"<code>Answer(content)</code>  <code>dataclass</code>","text":"<p>Holds the parts of an answer for multi-choices questions.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>full serialized content of the answer</p> <code>_choices</code> <code>list[str]</code> <p>parsed list of selected choices</p> <code>_explanation</code> <code>str</code> <p>parsed explanation message for the answer</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>in case of parsing error at initialisation</p> <p>Exemple of content : \"[[A;B;C]] this is a justification.\"</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>lazy formated content of the message.</p> required Source code in <code>pixano/features/schemas/annotations/text_generation.py</code> <pre><code>def __init__(self, content: str):\n    \"\"\"Constructor of the Answer from a lazy formated message.\n\n    Exemple of content : \"[[A;B;C]] this is a justification.\"\n\n    Args:\n        content (str): lazy formated content of the message.\n    \"\"\"\n    self.content, self._choices, self._explanation = Answer.parse(content)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/text_generation/#pixano.features.schemas.annotations.text_generation.Answer.__str__","title":"<code>__str__()</code>","text":"<p>Formats the message content for frontend.</p> <p>Returns:</p> Type Description <code>str</code> <p>formated message content</p> Source code in <code>pixano/features/schemas/annotations/text_generation.py</code> <pre><code>def __str__(self):\n    \"\"\"Formats the message content for frontend.\n\n    Returns:\n        str: formated message content\n    \"\"\"\n    return Answer.format(self._choices, self._explanation)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/text_generation/#pixano.features.schemas.annotations.text_generation.Answer.format","title":"<code>format(choices, explanation)</code>  <code>classmethod</code>","text":"<p>Formats the message content for frontend.</p> <p>Returns:</p> Type Description <code>str</code> <p>formated message content</p> Source code in <code>pixano/features/schemas/annotations/text_generation.py</code> <pre><code>@classmethod\ndef format(cls, choices, explanation):\n    \"\"\"Formats the message content for frontend.\n\n    Returns:\n        str: formated message content\n    \"\"\"\n    return \"[[{0}]] {1}\".format(\";\".join(choices), explanation)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/text_generation/#pixano.features.schemas.annotations.text_generation.Answer.parse","title":"<code>parse(content)</code>  <code>classmethod</code>","text":"<p>Utility function that parses a str content into the 3 fields of the Answer object.</p> <p>The input should be formated as follow \"[[A;B;C]] this is a justification.\"</p> <p>Returns:</p> Type Description <code>str</code> <p>content, the reformated content</p> <code>list[str]</code> <p>choices, the list of parsed answer elements</p> <code>str</code> <p>explanation, the explanation sentence</p> Source code in <code>pixano/features/schemas/annotations/text_generation.py</code> <pre><code>@classmethod\ndef parse(cls, content: str) -&gt; tuple[str, list[str], str]:\n    \"\"\"Utility function that parses a str content into the 3 fields of the Answer object.\n\n    The input should be formated as follow \"[[A;B;C]] this is a justification.\"\n\n    Returns:\n        content, the reformated content\n        choices, the list of parsed answer elements\n        explanation, the explanation sentence\n    \"\"\"\n    choices = []\n    explanation = \"\"\n    try:\n        re_choices_explanation = r\"\\[\\[([a-zA-Z0-9]+(\\;[a-zA-Z0-9]+)*)\\]\\]\\s*(.*)\"\n        match_1 = re.match(re_choices_explanation, content)\n        if match_1:\n            explanation = str(match_1.groups()[-1])\n            match_2 = re.findall(r\"[a-zA-Z0-9]+\", match_1.groups()[0])\n            for m in match_2:\n                choices.append(m)\n        content = Answer.format(choices, explanation)\n    except Exception:\n        raise ValueError(f\"could not parse answer message : {content} =&gt; {choices}, {explanation}\")\n    return content, choices, explanation\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/text_generation/#pixano.features.schemas.annotations.text_generation.Message","title":"<code>Message(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>Textual exchange in a question/answer conversation for image or text description and information extraction.</p> <p>Attributes:</p> Name Type Description <code>number</code> <code>int</code> <p>message number to associate different ANSWER messages to a QUESTION.</p> <code>content</code> <code>str</code> <p>actual text of the message.</p> <code>user</code> <code>str</code> <p>identify who is the author of the message (eg a human, a model, the ground truth, etc).</p> <code>choices</code> <code>list[str]</code> <p>list of allowed answers, for 'Multiple Choice Question' when type=QUESTION and question_type!=OPEN.</p> <code>timestamp</code> <code>datetime</code> <p>creation date of the message.</p> <code>type</code> <code>str</code> <p>type of the message within \"SYSTEM\", \"QUESTION\" or \"ANSWER\". - SYSTEM: used for prefix messages stating the context. No associated answer expected - QUESTION: used to ask a question about a View. Expecting at least one answer (same message number) - ANSWER: used to reply to a question message by refering its message number</p> <code>question_type</code> <code>str</code> <p>type of question, specifying how to read and parse the content field. Authorized calued within \"OPEN\", \"SINGLE_CHOICE\", \"SINGLE_CHOICE_EXPLANATION\",                          \"MULTI_CHOICE\", \"MULTI_CHOICE_EXPLANATION\". - OPEN: used for any open question, where no specific format of answer is expected - SINGLE_CHOICE: used for a multi-choice-question where only one answer is authorized. - SINGLE_CHOICE_EXPLANATION: similar to SINGLE_CHOICE, but an explanation is expected         after the choosen answer. Cf Answer object - MULTI_CHOICE: used for a multi-choice-question where multi answers are authorized. - MULTI_CHOICE_EXPLANATION: similar to MULTI_CHOICE, but an explanation is expected         after the choosen answers. Cf Answer object</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/text_generation/#pixano.features.schemas.annotations.text_generation.Message.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Utility function to get a None equivalent. Should be removed when Lance could manage None value.</p> <p>Returns:</p> Type Description <code>Message</code> <p>\"None\" Message.</p> Source code in <code>pixano/features/schemas/annotations/text_generation.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; \"Message\":\n    \"\"\"Utility function to get a None equivalent.\n    Should be removed when Lance could manage None value.\n\n    Returns:\n        \"None\" Message.\n    \"\"\"\n    return cls(\n        id=\"\",\n        item=ItemRef.none(),\n        view=ViewRef.none(),\n        entity=EntityRef.none(),\n        source_ref=SourceRef.none(),\n        number=0,\n        user=\"\",\n        type=\"QUESTION\",\n        content=\"\",\n        choices=[],\n        timestamp=datetime(1, 1, 1, 0, 0, 0, 0),\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/text_generation/#pixano.features.schemas.annotations.text_generation.create_message","title":"<code>create_message(number, user, type, content, choices=[], timestamp=datetime(1, 1, 1, 0, 0, 0, 0), id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none())</code>","text":"<p>Create a Message instance.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>int</code> <p>message number to associate diffrent ANSWER messages to a QUESTION</p> required <code>user</code> <code>str</code> <p>identify who is the author of the message (eg a human, a model, the ground truth, etc)</p> required <code>type</code> <code>Literal['SYSTEM', 'QUESTION', 'ANSWER']</code> <p>type of the message within \"SYSTEM\", \"QUESTION\" or\"ANSWER\"</p> required <code>content</code> <code>str</code> <p>actual text of the message</p> required <code>choices</code> <code>list[str]</code> <p>list of allowed answers</p> <code>[]</code> <code>timestamp</code> <code>datetime</code> <p>creation date of the message</p> <code>datetime(1, 1, 1, 0, 0, 0, 0)</code> <code>id</code> <code>str</code> <p>object id</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>Source reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>Message</code> <p>The created <code>Message</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/text_generation.py</code> <pre><code>def create_message(\n    number: int,\n    user: str,\n    type: Literal[\"SYSTEM\", \"QUESTION\", \"ANSWER\"],\n    content: str,\n    choices: list[str] = [],\n    timestamp: datetime = datetime(1, 1, 1, 0, 0, 0, 0),\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n) -&gt; Message:\n    \"\"\"Create a Message instance.\n\n    Args:\n        number: message number to associate diffrent ANSWER messages to a QUESTION\n        user: identify who is the author of the message (eg a human, a model, the ground truth, etc)\n        type: type of the message within \"SYSTEM\", \"QUESTION\" or\"ANSWER\"\n        content: actual text of the message\n        choices: list of allowed answers\n        timestamp: creation date of the message\n        id: object id\n        item_ref: Item reference.\n        view_ref: View reference.\n        entity_ref: Entity reference.\n        source_ref: Source reference.\n\n    Returns:\n        The created `Message` instance.\n    \"\"\"\n    return Message(\n        number=number,\n        user=user,\n        type=type,\n        content=content,\n        choices=choices,\n        timestamp=timestamp,\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/text_generation/#pixano.features.schemas.annotations.text_generation.is_message","title":"<code>is_message(cls, strict=False)</code>","text":"<p>Check if a class is a Message or subclass of Message.</p> Source code in <code>pixano/features/schemas/annotations/text_generation.py</code> <pre><code>def is_message(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a Message or subclass of Message.\"\"\"\n    return issubclass_strict(cls, Message, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/tracklet/","title":"tracklet","text":""},{"location":"api_reference/features/schemas/annotations/tracklet/#pixano.features.schemas.annotations.tracklet","title":"<code>pixano.features.schemas.annotations.tracklet</code>","text":""},{"location":"api_reference/features/schemas/annotations/tracklet/#pixano.features.schemas.annotations.tracklet.Tracklet","title":"<code>Tracklet(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>A <code>Tracklet</code> is a temporal segment of a video sequence.</p> <p>Attributes:</p> Name Type Description <code>start_timestep</code> <code>int</code> <p>The start timestep of the tracklet.</p> <code>end_timestep</code> <code>int</code> <p>The end timestep of the tracklet.</p> <code>start_timestamp</code> <code>float</code> <p>The start timestamp of the tracklet.</p> <code>end_timestamp</code> <code>float</code> <p>The end timestamp of the tracklet.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/tracklet/#pixano.features.schemas.annotations.tracklet.create_tracklet","title":"<code>create_tracklet(id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), entity_ref=EntityRef.none(), source_ref=SourceRef.none(), start_timestep=-1, end_timestep=-1, start_timestamp=-1.0, end_timestamp=-1.0)</code>","text":"<p>Create a <code>Tracklet</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The tracklet id.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>The item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>The view reference.</p> <code>none()</code> <code>entity_ref</code> <code>EntityRef</code> <p>The parent track reference.</p> <code>none()</code> <code>source_ref</code> <code>SourceRef</code> <p>The source reference.</p> <code>none()</code> <code>start_timestep</code> <code>int</code> <p>The start timestep of the tracklet.</p> <code>-1</code> <code>end_timestep</code> <code>int</code> <p>The end timestep of the tracklet.</p> <code>-1</code> <code>start_timestamp</code> <code>float</code> <p>The start timestamp of the tracklet.</p> <code>-1.0</code> <code>end_timestamp</code> <code>float</code> <p>The end timestamp of the tracklet.</p> <code>-1.0</code> <p>Returns:</p> Type Description <code>Tracklet</code> <p>The created <code>Tracklet</code> instance.</p> Source code in <code>pixano/features/schemas/annotations/tracklet.py</code> <pre><code>def create_tracklet(\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    entity_ref: EntityRef = EntityRef.none(),\n    source_ref: SourceRef = SourceRef.none(),\n    start_timestep: int = -1,\n    end_timestep: int = -1,\n    start_timestamp: float = -1.0,\n    end_timestamp: float = -1.0,\n) -&gt; Tracklet:\n    \"\"\"Create a `Tracklet` instance.\n\n    Args:\n        id: The tracklet id.\n        item_ref: The item reference.\n        view_ref: The view reference.\n        entity_ref: The parent track reference.\n        source_ref: The source reference.\n        start_timestep: The start timestep of the tracklet.\n        end_timestep: The end timestep of the tracklet.\n        start_timestamp: The start timestamp of the tracklet.\n        end_timestamp: The end timestamp of the tracklet.\n\n    Returns:\n        The created `Tracklet` instance.\n    \"\"\"\n    return Tracklet(\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        entity_ref=entity_ref,\n        source_ref=source_ref,\n        start_timestep=start_timestep,\n        end_timestep=end_timestep,\n        start_timestamp=start_timestamp,\n        end_timestamp=end_timestamp,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/annotations/tracklet/#pixano.features.schemas.annotations.tracklet.is_tracklet","title":"<code>is_tracklet(cls, strict=False)</code>","text":"<p>Check if the given class is a subclass of <code>Tracklet</code>.</p> Source code in <code>pixano/features/schemas/annotations/tracklet.py</code> <pre><code>def is_tracklet(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is a subclass of `Tracklet`.\"\"\"\n    return issubclass_strict(cls, Tracklet, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/embeddings/embedding/","title":"embedding","text":""},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding","title":"<code>pixano.features.schemas.embeddings.embedding</code>","text":""},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.Embedding","title":"<code>Embedding(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchema</code>, <code>ABC</code></p> <p>Embeddings are used to define an embedding vector for an item in a dataset.</p> <p>Attributes:</p> Name Type Description <code>item_ref</code> <code>ItemRef</code> <p>Reference to the embedding's item.</p> <code>vector</code> <code>Any</code> <p>The embedding vector that should be defined by subclasses.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.Embedding.item","title":"<code>item</code>  <code>property</code>","text":"<p>Get the embedding's item.</p>"},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.Embedding.to_arrow_schema","title":"<code>to_arrow_schema(remove_vector=False, remove_metadata=False)</code>  <code>classmethod</code>","text":"<p>Get the pyarrow schema of an <code>Embedding</code>.</p> <p>This function allows to remove the vector field and the metadata from the schema which can be useful for adding data with auto-vectorization.</p> <p>Parameters:</p> Name Type Description Default <code>remove_vector</code> <code>bool</code> <p>Remove the vector field.</p> <code>False</code> <code>remove_metadata</code> <code>bool</code> <p>Remove the metadata.</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema</code> <p>The pyarrow schema.</p> Source code in <code>pixano/features/schemas/embeddings/embedding.py</code> <pre><code>@classmethod\ndef to_arrow_schema(\n    cls,\n    remove_vector: bool = False,\n    remove_metadata: bool = False,\n) -&gt; pa.Schema:\n    \"\"\"Get the pyarrow schema of an `Embedding`.\n\n    This function allows to remove the vector field and the metadata from the schema which can be useful for adding\n    data with auto-vectorization.\n\n    Args:\n        remove_vector: Remove the vector field.\n        remove_metadata: Remove the metadata.\n\n    Returns:\n        The pyarrow schema.\n    \"\"\"\n    pa_schema = super().to_arrow_schema()\n    if remove_vector:\n        pa_schema = pa_schema.remove(pa_schema.get_field_index(\"vector\"))\n    if remove_metadata:\n        pa_schema = pa_schema.remove_metadata()\n    return pa_schema\n</code></pre>"},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.ViewEmbedding","title":"<code>ViewEmbedding(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Embedding</code>, <code>ABC</code></p> <p>ViewEmbeddings are used to define an embedding vector for a view in a dataset.</p> <p>Attributes:</p> Name Type Description <code>view_ref</code> <code>ViewRef</code> <p>Reference to the embedding's view.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.ViewEmbedding.view","title":"<code>view</code>  <code>property</code>","text":"<p>Get the embedding's view.</p>"},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.ViewEmbedding.create_schema","title":"<code>create_schema(embedding_fn, table_name, dataset, **embedding_function_kwargs)</code>  <code>classmethod</code>","text":"<p>Create a ViewEmbedding schema.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_fn</code> <code>str</code> <p>The embedding function.</p> required <code>table_name</code> <code>str</code> <p>The name of the table containing the schema.</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset to which the schema belongs.</p> required <code>embedding_function_kwargs</code> <code>Any</code> <p>The keyword arguments for creating the embedding function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>type[ViewEmbedding]</code> <p>The <code>ViewEmbedding</code> schema.</p> Source code in <code>pixano/features/schemas/embeddings/embedding.py</code> <pre><code>@classmethod\ndef create_schema(\n    cls,\n    embedding_fn: str,\n    table_name: str,\n    dataset: \"Dataset\",\n    **embedding_function_kwargs: Any,\n) -&gt; type[\"ViewEmbedding\"]:\n    \"\"\"Create a ViewEmbedding schema.\n\n    Args:\n        embedding_fn: The embedding function.\n        table_name: The name of the table containing the schema.\n        dataset: The dataset to which the schema belongs.\n        embedding_function_kwargs: The keyword arguments for creating the embedding function.\n\n    Returns:\n        The `ViewEmbedding` schema.\n    \"\"\"\n    lance_registry = get_registry()\n    if not isinstance(embedding_fn, str):\n        raise TypeError(f\"{embedding_fn} should be a string\")\n\n    pixano_name = _to_pixano_name(dataset, table_name, embedding_fn)\n    if pixano_name not in lance_registry._functions:\n        type_embedding_function = lance_registry.get(embedding_fn)\n        view_embedding_function: type[EmbeddingFunction] = create_view_embedding_function(\n            type_embedding_function, pixano_name, dataset\n        )\n    else:\n        view_embedding_function = lance_registry.get(pixano_name)\n\n    view_embedding_function = view_embedding_function.create(**embedding_function_kwargs)\n\n    embedding_fields = {\n        \"vector\": (Vector(view_embedding_function.ndims()), view_embedding_function.VectorField()),\n        \"view_ref\": (ViewRef, view_embedding_function.SourceField()),\n    }\n    return create_model(\n        \"ViewEmbedding\",\n        __base__=ViewEmbedding,\n        **embedding_fields,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.ViewEmbedding.get_embedding_fn_from_table","title":"<code>get_embedding_fn_from_table(dataset, table_name, metadata)</code>  <code>staticmethod</code>","text":"<p>Get the embedding function from a table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset containing the table.</p> required <code>table_name</code> <code>str</code> <p>The name of the table containing the embedding function.</p> required <code>metadata</code> <code>dict</code> <p>The pyarrow metadata of the table.</p> required <p>Returns:</p> Type Description <code>EmbeddingFunction</code> <p>The embedding function.</p> Source code in <code>pixano/features/schemas/embeddings/embedding.py</code> <pre><code>@staticmethod\ndef get_embedding_fn_from_table(dataset: \"Dataset\", table_name: str, metadata: dict) -&gt; EmbeddingFunction:\n    \"\"\"Get the embedding function from a table.\n\n    Args:\n        dataset: The dataset containing the table.\n        table_name: The name of the table containing the embedding function.\n        metadata: The pyarrow metadata of the table.\n\n    Returns:\n        The embedding function.\n    \"\"\"\n    registry = get_registry()\n\n    serialized = metadata[b\"embedding_functions\"]\n    raw_list = json.loads(serialized.decode(\"utf-8\"))\n\n    if len(raw_list) &gt; 1:\n        raise ValueError(\"Only one embedding function per table is supported\")\n\n    pixano_name = raw_list[0][\"name\"]\n    if pixano_name not in registry._functions:\n        name = _from_pixano_name(dataset, table_name, pixano_name)\n        create_view_embedding_function(registry._functions[name], pixano_name, dataset)\n    return registry.get(pixano_name)\n</code></pre>"},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.create_view_embedding_function","title":"<code>create_view_embedding_function(type_embedding_function, name, dataset)</code>","text":"<p>Create a <code>ViewEmbeddingFunction</code> based on an EmbeddingFunction.</p> Source code in <code>pixano/features/schemas/embeddings/embedding.py</code> <pre><code>def create_view_embedding_function(\n    type_embedding_function: type[EmbeddingFunction], name: str, dataset: \"Dataset\"\n) -&gt; type[EmbeddingFunction]:\n    \"\"\"Create a `ViewEmbeddingFunction` based on an\n    [EmbeddingFunction][lancedb.embeddings.base.EmbeddingFunction].\n    \"\"\"\n\n    @register(name)\n    class ViewEmbeddingFunction(type_embedding_function):\n        \"\"\"A `ViewEmbeddingFunction` based on an [EmbeddingFunction][lancedb.embeddings.base.EmbeddingFunction].\"\"\"\n\n        def _open_views(self, views: list[Any]) -&gt; list[Any]:\n            \"\"\"Open the views in the dataset.\"\"\"\n            return [view.open(dataset.media_dir, \"image\") for view in views]\n\n        def compute_source_embeddings(self, view_refs: pa.Table, *args, **kwargs) -&gt; list:\n            \"\"\"Compute the embeddings for the source column in the database.\"\"\"\n            views = [dataset.resolve_ref(ViewRef(**view_ref)) for view_ref in view_refs.to_pylist()]\n            view_type = type(views[0])\n            if is_image(view_type) or is_sequence_frame(view_type):\n                views = cast(list[Image], views)\n                return super().compute_source_embeddings(self._open_views(views=views), *args, **kwargs)\n            else:\n                raise ValueError(f\"View type {view_type} not supported for embedding.\")\n\n    return ViewEmbeddingFunction\n</code></pre>"},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.is_embedding","title":"<code>is_embedding(cls, strict=False)</code>","text":"<p>Check if a class is an <code>Embedding</code> or subclass of <code>Embedding</code>.</p> Source code in <code>pixano/features/schemas/embeddings/embedding.py</code> <pre><code>def is_embedding(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is an `Embedding` or subclass of `Embedding`.\"\"\"\n    return issubclass_strict(cls, Embedding, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/embeddings/embedding/#pixano.features.schemas.embeddings.embedding.is_view_embedding","title":"<code>is_view_embedding(cls, strict=False)</code>","text":"<p>Check if a class is an <code>ViewEmbedding</code> or subclass of <code>ViewEmbedding</code>.</p> Source code in <code>pixano/features/schemas/embeddings/embedding.py</code> <pre><code>def is_view_embedding(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is an `ViewEmbedding` or subclass of `ViewEmbedding`.\"\"\"\n    return issubclass_strict(cls, ViewEmbedding, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/conversation/","title":"conversation","text":""},{"location":"api_reference/features/schemas/entities/conversation/#pixano.features.schemas.entities.conversation","title":"<code>pixano.features.schemas.entities.conversation</code>","text":""},{"location":"api_reference/features/schemas/entities/conversation/#pixano.features.schemas.entities.conversation.Conversation","title":"<code>Conversation(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Entity</code></p> <p>A <code>Conversation</code> entity.</p> <p>A conversation is an object holding messages ordered by their attribute <code>number</code>. The Message annotations refer to the conversatioin via their <code>entity_ref</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>str</code> <p>Agnostic metadata to store information of the conversation.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/conversation/#pixano.features.schemas.entities.conversation.create_conversation","title":"<code>create_conversation(kind, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), parent_ref=EntityRef.none())</code>","text":"<p>Create a <code>Conversation</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>Agnostic metadata to store information of the conversation.</p> required <code>id</code> <code>str</code> <p>Conversation ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>parent_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>Conversation</code> <p>The created <code>Conversation</code> instance.</p> Source code in <code>pixano/features/schemas/entities/conversation.py</code> <pre><code>def create_conversation(\n    kind: str,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    parent_ref: EntityRef = EntityRef.none(),\n) -&gt; Conversation:\n    \"\"\"Create a `Conversation` instance.\n\n    Args:\n        kind: Agnostic metadata to store information of the conversation.\n        id: Conversation ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        parent_ref: Entity reference.\n\n    Returns:\n        The created `Conversation` instance.\n    \"\"\"\n    return Conversation(\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        parent_ref=parent_ref,\n        kind=kind,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/entities/conversation/#pixano.features.schemas.entities.conversation.is_conversation","title":"<code>is_conversation(cls, strict=False)</code>","text":"<p>Check if the given class is a <code>Conversation</code> or a subclass of <code>Conversation</code>.</p> Source code in <code>pixano/features/schemas/entities/conversation.py</code> <pre><code>def is_conversation(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is a `Conversation` or a subclass of `Conversation`.\"\"\"\n    return issubclass_strict(cls, Conversation, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/entity/","title":"entity","text":""},{"location":"api_reference/features/schemas/entities/entity/#pixano.features.schemas.entities.entity","title":"<code>pixano.features.schemas.entities.entity</code>","text":""},{"location":"api_reference/features/schemas/entities/entity/#pixano.features.schemas.entities.entity.Entity","title":"<code>Entity(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchema</code></p> <p><code>Entity</code> base class.</p> <p>Entities are used to define an entity in a dataset such as an object, a track. It can refer to an item, a view, and a parent entity.</p> <p>Attributes:</p> Name Type Description <code>item_ref</code> <code>ItemRef</code> <p>Reference to the entity's item.</p> <code>view_ref</code> <code>ViewRef</code> <p>Reference to the entity's view.</p> <code>parent_ref</code> <code>EntityRef</code> <p>Reference to the entity's parent entity.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/entity/#pixano.features.schemas.entities.entity.Entity.item","title":"<code>item</code>  <code>property</code>","text":"<p>Get the entity's item.</p>"},{"location":"api_reference/features/schemas/entities/entity/#pixano.features.schemas.entities.entity.Entity.parent","title":"<code>parent</code>  <code>property</code>","text":"<p>Get the entity's parent entity.</p>"},{"location":"api_reference/features/schemas/entities/entity/#pixano.features.schemas.entities.entity.Entity.view","title":"<code>view</code>  <code>property</code>","text":"<p>Get the entity's view.</p>"},{"location":"api_reference/features/schemas/entities/entity/#pixano.features.schemas.entities.entity.is_entity","title":"<code>is_entity(cls, strict=False)</code>","text":"<p>Check if a class is an Entity or subclass of Entity.</p> Source code in <code>pixano/features/schemas/entities/entity.py</code> <pre><code>def is_entity(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is an Entity or subclass of Entity.\"\"\"\n    return issubclass_strict(cls, Entity, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/multi_modal_entity/","title":"multi_modal_entity","text":""},{"location":"api_reference/features/schemas/entities/multi_modal_entity/#pixano.features.schemas.entities.multi_modal_entity","title":"<code>pixano.features.schemas.entities.multi_modal_entity</code>","text":""},{"location":"api_reference/features/schemas/entities/multi_modal_entity/#pixano.features.schemas.entities.multi_modal_entity.MultiModalEntity","title":"<code>MultiModalEntity(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>NamedEntity</code></p> <p>A <code>multimedia</code> entity.</p> <p>A MultiModalEntity represents an entity that is shared among multiple view of different type : image + text.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the Entity.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/multi_modal_entity/#pixano.features.schemas.entities.multi_modal_entity.create_multi_modal_entity","title":"<code>create_multi_modal_entity(name, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), parent_ref=EntityRef.none())</code>","text":"<p>Create a <code>MultiModalEntity</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the multimedia_entity.</p> required <code>id</code> <code>str</code> <p>MultiModalEntity ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>parent_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>MultiModalEntity</code> <p>The created <code>MultiModalEntity</code> instance.</p> Source code in <code>pixano/features/schemas/entities/multi_modal_entity.py</code> <pre><code>def create_multi_modal_entity(\n    name: str,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    parent_ref: EntityRef = EntityRef.none(),\n) -&gt; MultiModalEntity:\n    \"\"\"Create a `MultiModalEntity` instance.\n\n    Args:\n        name: The name of the multimedia_entity.\n        id: MultiModalEntity ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        parent_ref: Entity reference.\n\n    Returns:\n        The created `MultiModalEntity` instance.\n    \"\"\"\n    return MultiModalEntity(\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        parent_ref=parent_ref,\n        name=name,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/entities/multi_modal_entity/#pixano.features.schemas.entities.multi_modal_entity.is_multi_modal_entity","title":"<code>is_multi_modal_entity(cls, strict=False)</code>","text":"<p>Check if the given class is a <code>MultiModalEntity</code> or a subclass of <code>MultiModalEntity</code>.</p> Source code in <code>pixano/features/schemas/entities/multi_modal_entity.py</code> <pre><code>def is_multi_modal_entity(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is a `MultiModalEntity` or a subclass of `MultiModalEntity`.\"\"\"\n    return issubclass_strict(cls, MultiModalEntity, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/named_entity/","title":"named_entity","text":""},{"location":"api_reference/features/schemas/entities/named_entity/#pixano.features.schemas.entities.named_entity","title":"<code>pixano.features.schemas.entities.named_entity</code>","text":""},{"location":"api_reference/features/schemas/entities/named_entity/#pixano.features.schemas.entities.named_entity.NamedEntity","title":"<code>NamedEntity(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Entity</code></p> <p>A named entity.</p> <p>A NamedEntity represents an entity with a simple name attribute.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the entity.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/named_entity/#pixano.features.schemas.entities.named_entity.create_named_entity","title":"<code>create_named_entity(name, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), parent_ref=EntityRef.none())</code>","text":"<p>Create a <code>NamedEntity</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the entity.</p> required <code>id</code> <code>str</code> <p>NamedEntity ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>parent_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>NamedEntity</code> <p>The created <code>NamedEntity</code> instance.</p> Source code in <code>pixano/features/schemas/entities/named_entity.py</code> <pre><code>def create_named_entity(\n    name: str,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    parent_ref: EntityRef = EntityRef.none(),\n) -&gt; NamedEntity:\n    \"\"\"Create a `NamedEntity` instance.\n\n    Args:\n        name: The name of the entity.\n        id: NamedEntity ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        parent_ref: Entity reference.\n\n    Returns:\n        The created `NamedEntity` instance.\n    \"\"\"\n    return NamedEntity(\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        parent_ref=parent_ref,\n        name=name,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/entities/named_entity/#pixano.features.schemas.entities.named_entity.is_named_entity","title":"<code>is_named_entity(cls, strict=False)</code>","text":"<p>Check if the given class is a <code>NamedEntity</code> or a subclass of <code>NamedEntity</code>.</p> Source code in <code>pixano/features/schemas/entities/named_entity.py</code> <pre><code>def is_named_entity(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is a `NamedEntity` or a subclass of `NamedEntity`.\"\"\"\n    return issubclass_strict(cls, NamedEntity, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/track/","title":"track","text":""},{"location":"api_reference/features/schemas/entities/track/#pixano.features.schemas.entities.track","title":"<code>pixano.features.schemas.entities.track</code>","text":""},{"location":"api_reference/features/schemas/entities/track/#pixano.features.schemas.entities.track.Track","title":"<code>Track(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>NamedEntity</code></p> <p>A <code>Track</code> entity.</p> <p>A track represents an entity that is shared among multiple view across time.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the track.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/entities/track/#pixano.features.schemas.entities.track.create_track","title":"<code>create_track(name, id='', item_ref=ItemRef.none(), view_ref=ViewRef.none(), parent_ref=EntityRef.none())</code>","text":"<p>Create a <code>Track</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the track.</p> required <code>id</code> <code>str</code> <p>Track ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>view_ref</code> <code>ViewRef</code> <p>View reference.</p> <code>none()</code> <code>parent_ref</code> <code>EntityRef</code> <p>Entity reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>Track</code> <p>The created <code>Track</code> instance.</p> Source code in <code>pixano/features/schemas/entities/track.py</code> <pre><code>def create_track(\n    name: str,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    view_ref: ViewRef = ViewRef.none(),\n    parent_ref: EntityRef = EntityRef.none(),\n) -&gt; Track:\n    \"\"\"Create a `Track` instance.\n\n    Args:\n        name: The name of the track.\n        id: Track ID.\n        item_ref: Item reference.\n        view_ref: View reference.\n        parent_ref: Entity reference.\n\n    Returns:\n        The created `Track` instance.\n    \"\"\"\n    return Track(\n        id=id,\n        item_ref=item_ref,\n        view_ref=view_ref,\n        parent_ref=parent_ref,\n        name=name,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/entities/track/#pixano.features.schemas.entities.track.is_track","title":"<code>is_track(cls, strict=False)</code>","text":"<p>Check if the given class is a <code>Track</code> or a subclass of <code>Track</code>.</p> Source code in <code>pixano/features/schemas/entities/track.py</code> <pre><code>def is_track(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is a `Track` or a subclass of `Track`.\"\"\"\n    return issubclass_strict(cls, Track, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/items/item/","title":"item","text":""},{"location":"api_reference/features/schemas/items/item/#pixano.features.schemas.items.item","title":"<code>pixano.features.schemas.items.item</code>","text":""},{"location":"api_reference/features/schemas/items/item/#pixano.features.schemas.items.item.Item","title":"<code>Item(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchema</code></p> <p>Item base class.</p> <p>Items are used to store information about an item in a dataset. It contains at least a split attribute. It also federates the information about the item's views, entities, annotations, embeddings, etc via its id.</p> <p>Attributes:</p> Name Type Description <code>split</code> <code>str</code> <p>Split of the item.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/items/item/#pixano.features.schemas.items.item.is_item","title":"<code>is_item(cls, strict=False)</code>","text":"<p>Check if a class is an <code>Item</code> or subclass of <code>Item</code>.</p> Source code in <code>pixano/features/schemas/items/item.py</code> <pre><code>def is_item(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is an `Item` or subclass of `Item`.\"\"\"\n    return issubclass_strict(cls, Item, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/views/image/","title":"image","text":""},{"location":"api_reference/features/schemas/views/image/#pixano.features.schemas.views.image","title":"<code>pixano.features.schemas.views.image</code>","text":""},{"location":"api_reference/features/schemas/views/image/#pixano.features.schemas.views.image.Image","title":"<code>Image(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>View</code></p> <p>Image view.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The image URL. Can be relative or absolute or a data URL.</p> <code>width</code> <code>int</code> <p>The image width.</p> <code>height</code> <code>int</code> <p>The image height.</p> <code>format</code> <code>str</code> <p>The image format.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/views/image/#pixano.features.schemas.views.image.Image.open","title":"<code>open(media_dir=None, output_type='base64')</code>","text":"<pre><code>open(media_dir: Path | None, output_type: Literal['base64'] = 'base64') -&gt; str\n</code></pre><pre><code>open(media_dir: Path | None, output_type: Literal['image']) -&gt; PILImage\n</code></pre> <p>Open the image.</p> Note <p>If the output type is \"base64\", the image is returned as a base64 string formatted as \"data:image/{image_format};base64,{base64}\".</p> <p>Parameters:</p> Name Type Description Default <code>media_dir</code> <code>Path | None</code> <p>Path to the media directory. If the URL is relative, it is relative to this directory.</p> <code>None</code> <code>output_type</code> <code>Literal['base64', 'image']</code> <p>The output type. Can be \"base64\" or \"image\" (PIL.Image).</p> <code>'base64'</code> <p>Returns:</p> Type Description <code>str | Image</code> <p>opened image.</p> Source code in <code>pixano/features/schemas/views/image.py</code> <pre><code>def open(\n    self,\n    media_dir: Path | None = None,\n    output_type: Literal[\"base64\", \"image\"] = \"base64\",\n) -&gt; str | PILImage:\n    \"\"\"Open the image.\n\n    Note:\n        If the output type is \"base64\", the image is returned as a base64 string formatted as\n        \"data:image/{image_format};base64,{base64}\".\n\n    Args:\n        media_dir: Path to the media directory. If the URL is relative, it is relative to this directory.\n        output_type: The output type. Can be \"base64\" or \"image\" (PIL.Image).\n\n    Returns:\n        opened image.\n    \"\"\"\n    return Image.open_url(url=self.url, media_dir=media_dir, output_type=output_type)\n</code></pre>"},{"location":"api_reference/features/schemas/views/image/#pixano.features.schemas.views.image.Image.open_url","title":"<code>open_url(url, media_dir=None, output_type='base64')</code>  <code>staticmethod</code>","text":"<pre><code>open_url(url: str, media_dir: Path | None, output_type: Literal['base64'] = 'base64') -&gt; str\n</code></pre><pre><code>open_url(url: str, media_dir: Path | None, output_type: Literal['image']) -&gt; PILImage\n</code></pre> <p>Open an image from a URL.</p> Note <p>If the output type is \"base64\", the image is returned as a base64 string formatted as \"data:image/{image_format};base64,{base64}\".</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>image url relative to media_dir or absolute.</p> required <code>media_dir</code> <code>Path | None</code> <p>path to the media directory if the URL is relative.</p> <code>None</code> <code>output_type</code> <code>Literal['base64', 'image']</code> <p>output type. Can be \"base64\" or \"image\" (PIL.Image).</p> <code>'base64'</code> <p>Returns:</p> Type Description <code>str | Image</code> <p>The opened image.</p> Source code in <code>pixano/features/schemas/views/image.py</code> <pre><code>@staticmethod\ndef open_url(\n    url: str,\n    media_dir: Path | None = None,\n    output_type: Literal[\"base64\", \"image\"] = \"base64\",\n) -&gt; str | PILImage:\n    \"\"\"Open an image from a URL.\n\n    Note:\n        If the output type is \"base64\", the image is returned as a base64 string formatted as\n        \"data:image/{image_format};base64,{base64}\".\n\n    Args:\n        url: image url relative to media_dir or absolute.\n        media_dir: path to the media directory if the URL is relative.\n        output_type: output type. Can be \"base64\" or \"image\" (PIL.Image).\n\n    Returns:\n        The opened image.\n    \"\"\"\n    if output_type not in [\"base64\", \"image\"]:\n        raise ValueError(f\"Invalid output type: {output_type}\")\n\n    # URI is incomplete\n    if urlparse(url).scheme == \"\":\n        if media_dir is None:\n            raise ValueError(\"URI is incomplete, need media directory\")\n        uri_prefix = media_dir.absolute().as_uri()\n        # URI prefix exists\n        if uri_prefix is not None:\n            parsed_uri = urlparse(uri_prefix)\n            # URI prefix is incomplete\n            if parsed_uri.scheme == \"\":\n                raise ValueError(\"URI prefix is incomplete, no scheme provided (http://, file://, ...)\")\n            if url.startswith(\"/\"):\n                url = url[1:]\n            combined_path = Path(parsed_uri.path) / url\n            parsed_uri = parsed_uri._replace(path=str(combined_path))\n            api_url = parsed_uri.geturl()\n        else:\n            # No URI prefix\n            raise ValueError(\"URI is incomplete, need URI prefix\")\n    # URI is already complete\n    else:\n        api_url = url\n\n    try:\n        with urlopen(api_url) as f:\n            im_bytes = f.read()\n    except URLError:\n        raise ValueError(f\"Error: image not found ({api_url})\")\n\n    pil_image = PIL.Image.open(io.BytesIO(im_bytes))\n\n    # Handle output types\n    if output_type == \"base64\":\n        return image_to_base64(pil_image)\n\n    return pil_image\n</code></pre>"},{"location":"api_reference/features/schemas/views/image/#pixano.features.schemas.views.image.Image.shorten_url_to_relative_path","title":"<code>shorten_url_to_relative_path(url_relative_path)</code>","text":"<p>Changes the URL of an image to be relative.</p> Note <p>This helps the creation of a dataset where Image object are created with known dimensions format and theorical path, but images are not yet accessible.</p> <p>Parameters:</p> Name Type Description Default <code>url_relative_path</code> <code>Path</code> <p>The path to convert the URL to a relative path, eg for images to be later searchable in the media_dir.</p> required <p>Returns:</p> Type Description <code>str</code> <p>shorten image url</p> Source code in <code>pixano/features/schemas/views/image.py</code> <pre><code>def shorten_url_to_relative_path(self, url_relative_path: Path) -&gt; str:\n    \"\"\"Changes the URL of an image to be relative.\n\n    Note:\n       This helps the creation of a dataset where Image object are created\n       with known dimensions format and theorical path, but images are not yet accessible.\n\n    Args:\n        url_relative_path (Path): The path to convert the URL to a relative path,\n            eg for images to be later searchable in the media_dir.\n\n    Returns:\n        str: shorten image url\n    \"\"\"\n    url = Path(self.url)\n    url_relative_path = Path(url_relative_path)\n    self.url = url.relative_to(url_relative_path).as_posix()\n    return self.url\n</code></pre>"},{"location":"api_reference/features/schemas/views/image/#pixano.features.schemas.views.image.create_image","title":"<code>create_image(url, id='', item_ref=ItemRef.none(), parent_ref=ViewRef.none(), width=None, height=None, format=None, url_relative_path=None)</code>","text":"<p>Create an <code>Image</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Path</code> <p>The image URL. If not relative, the URL is converted to a relative path using <code>url_relative_path</code>.</p> required <code>id</code> <code>str</code> <p>Image ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>parent_ref</code> <code>ViewRef</code> <p>Parent view reference.</p> <code>none()</code> <code>width</code> <code>int | None</code> <p>The image width. If None, the width is extracted from the image file.</p> <code>None</code> <code>height</code> <code>int | None</code> <p>The image height. If None, the height is extracted from the image file.</p> <code>None</code> <code>format</code> <code>str | None</code> <p>The image format. If None, the format is extracted from the image file.</p> <code>None</code> <code>url_relative_path</code> <code>Path | None</code> <p>The path to convert the URL to a relative path, eg for images to be searchable in the media_dir.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>The created <code>Image</code> instance.</p> Source code in <code>pixano/features/schemas/views/image.py</code> <pre><code>def create_image(\n    url: Path,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    parent_ref: ViewRef = ViewRef.none(),\n    width: int | None = None,\n    height: int | None = None,\n    format: str | None = None,\n    url_relative_path: Path | None = None,\n) -&gt; Image:\n    \"\"\"Create an `Image` instance.\n\n    Args:\n        url: The image URL. If not relative, the URL is converted to a relative path using `url_relative_path`.\n        id: Image ID.\n        item_ref: Item reference.\n        parent_ref: Parent view reference.\n        width: The image width. If None, the width is extracted from the image file.\n        height: The image height. If None, the height is extracted from the image file.\n        format: The image format. If None, the format is extracted from the image file.\n        url_relative_path: The path to convert the URL to a relative path,\n            eg for images to be searchable in the media_dir.\n\n    Returns:\n        The created `Image` instance.\n    \"\"\"\n    none_conditions = [width is None, height is None, format is None]\n    not_none_conditions = [width is not None, height is not None, format is not None]\n    if not all(none_conditions) and not all(not_none_conditions):\n        raise ValueError(\"width, height and format must be all defined or all None\")\n\n    url = Path(url)\n\n    if width is None:\n        img = PIL.Image.open(url)\n        width = img.width\n        height = img.height\n        format = img.format\n\n    if url_relative_path is not None:\n        url_relative_path = Path(url_relative_path)\n        url = url.relative_to(url_relative_path)\n\n    return Image(\n        id=id,\n        item_ref=item_ref,\n        parent_ref=parent_ref,\n        url=str(url.as_posix()),\n        width=width,\n        height=height,\n        format=format,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/views/image/#pixano.features.schemas.views.image.is_image","title":"<code>is_image(cls, strict=False)</code>","text":"<p>Check if the given class is <code>Image</code> or a subclass of <code>Image</code>.</p> Source code in <code>pixano/features/schemas/views/image.py</code> <pre><code>def is_image(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is `Image` or a subclass of `Image`.\"\"\"\n    return issubclass_strict(cls, Image, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/views/point_cloud/","title":"point_cloud","text":""},{"location":"api_reference/features/schemas/views/point_cloud/#pixano.features.schemas.views.point_cloud","title":"<code>pixano.features.schemas.views.point_cloud</code>","text":""},{"location":"api_reference/features/schemas/views/point_cloud/#pixano.features.schemas.views.point_cloud.PointCloud","title":"<code>PointCloud(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>View</code></p> <p>Point Cloud view.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The point cloud URL.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/views/point_cloud/#pixano.features.schemas.views.point_cloud.create_point_cloud","title":"<code>create_point_cloud(url, id='', item_ref=ItemRef.none(), parent_ref=ViewRef.none())</code>","text":"<p>Create a <code>PointCloud</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The point cloud URL.</p> required <code>id</code> <code>str</code> <p>Point cloud ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>parent_ref</code> <code>ViewRef</code> <p>Parent view reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>PointCloud</code> <p>The created <code>PointCloud</code> instance.</p> Source code in <code>pixano/features/schemas/views/point_cloud.py</code> <pre><code>def create_point_cloud(\n    url: str, id: str = \"\", item_ref: ItemRef = ItemRef.none(), parent_ref: ViewRef = ViewRef.none()\n) -&gt; PointCloud:\n    \"\"\"Create a `PointCloud` instance.\n\n    Args:\n        url: The point cloud URL.\n        id: Point cloud ID.\n        item_ref: Item reference.\n        parent_ref: Parent view reference.\n\n    Returns:\n        The created `PointCloud` instance.\n    \"\"\"\n    return PointCloud(url=url, id=id, item_ref=item_ref, parent_ref=parent_ref)\n</code></pre>"},{"location":"api_reference/features/schemas/views/point_cloud/#pixano.features.schemas.views.point_cloud.is_point_cloud","title":"<code>is_point_cloud(cls, strict=False)</code>","text":"<p>Check if the given class is a subclass of <code>PointCloud</code>.</p> Source code in <code>pixano/features/schemas/views/point_cloud.py</code> <pre><code>def is_point_cloud(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is a subclass of `PointCloud`.\"\"\"\n    return issubclass_strict(cls, PointCloud, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/views/sequence_frame/","title":"sequence_frame","text":""},{"location":"api_reference/features/schemas/views/sequence_frame/#pixano.features.schemas.views.sequence_frame","title":"<code>pixano.features.schemas.views.sequence_frame</code>","text":""},{"location":"api_reference/features/schemas/views/sequence_frame/#pixano.features.schemas.views.sequence_frame.SequenceFrame","title":"<code>SequenceFrame(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>Image</code></p> <p>Sequence Frame view.</p> <p>Attributes:</p> Name Type Description <code>timestamp</code> <code>float</code> <p>The timestamp of the frame.</p> <code>frame_index</code> <code>int</code> <p>The index of the frame in the sequence.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/views/sequence_frame/#pixano.features.schemas.views.sequence_frame.create_sequence_frame","title":"<code>create_sequence_frame(url, timestamp, frame_index, id='', item_ref=ItemRef.none(), parent_ref=ViewRef.none(), width=None, height=None, format=None, url_relative_path=None)</code>","text":"<p>Create a <code>SequenceFrame</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Path</code> <p>The frame URL. If not relative, the URL is converted to a relative path using <code>other_path</code>.</p> required <code>timestamp</code> <code>float</code> <p>The timestamp of the frame.</p> required <code>frame_index</code> <code>int</code> <p>The index of the frame in the sequence.</p> required <code>id</code> <code>str</code> <p>Point cloud ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>parent_ref</code> <code>ViewRef</code> <p>Parent view reference.</p> <code>none()</code> <code>width</code> <code>int | None</code> <p>The frame width. If None, the width is extracted from the frame file.</p> <code>None</code> <code>height</code> <code>int | None</code> <p>The frame height. If None, the height is extracted from the frame file.</p> <code>None</code> <code>format</code> <code>str | None</code> <p>The frame format. If None, the format is extracted from the frame file.</p> <code>None</code> <code>url_relative_path</code> <code>Path | None</code> <p>The path to convert the URL to a relative path.</p> <code>None</code> <p>Returns:</p> Type Description <code>SequenceFrame</code> <p>The created <code>SequenceFrame</code> instance.</p> Source code in <code>pixano/features/schemas/views/sequence_frame.py</code> <pre><code>def create_sequence_frame(\n    url: Path,\n    timestamp: float,\n    frame_index: int,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    parent_ref: ViewRef = ViewRef.none(),\n    width: int | None = None,\n    height: int | None = None,\n    format: str | None = None,\n    url_relative_path: Path | None = None,\n) -&gt; SequenceFrame:\n    \"\"\"Create a `SequenceFrame` instance.\n\n    Args:\n        url: The frame URL. If not relative, the URL is converted to a relative path using `other_path`.\n        timestamp: The timestamp of the frame.\n        frame_index: The index of the frame in the sequence.\n        id: Point cloud ID.\n        item_ref: Item reference.\n        parent_ref: Parent view reference.\n        width: The frame width. If None, the width is extracted from the frame file.\n        height: The frame height. If None, the height is extracted from the frame file.\n        format: The frame format. If None, the format is extracted from the frame file.\n        url_relative_path: The path to convert the URL to a relative path.\n\n    Returns:\n        The created `SequenceFrame` instance.\n    \"\"\"\n    image = create_image(url, id, item_ref, parent_ref, width, height, format, url_relative_path)\n    return SequenceFrame(\n        id=id,\n        item_ref=item_ref,\n        parent_ref=parent_ref,\n        url=image.url,\n        width=image.width,\n        height=image.height,\n        format=image.format,\n        timestamp=timestamp,\n        frame_index=frame_index,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/views/sequence_frame/#pixano.features.schemas.views.sequence_frame.is_sequence_frame","title":"<code>is_sequence_frame(cls, strict=False)</code>","text":"<p>Check if the given class is a subclass of <code>SequenceFrame</code>.</p> Source code in <code>pixano/features/schemas/views/sequence_frame.py</code> <pre><code>def is_sequence_frame(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is a subclass of `SequenceFrame`.\"\"\"\n    return issubclass_strict(cls, SequenceFrame, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/views/text/","title":"text","text":""},{"location":"api_reference/features/schemas/views/text/#pixano.features.schemas.views.text","title":"<code>pixano.features.schemas.views.text</code>","text":""},{"location":"api_reference/features/schemas/views/text/#pixano.features.schemas.views.text.Text","title":"<code>Text(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>View</code></p> <p>Text view.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The text content.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/views/text/#pixano.features.schemas.views.text.create_text","title":"<code>create_text(content, id='', item_ref=ItemRef.none(), parent_ref=ViewRef.none())</code>","text":"<p>Create a <code>Text</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The text content.</p> required <code>id</code> <code>str</code> <p>Text ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>parent_ref</code> <code>ViewRef</code> <p>Parent view reference.</p> <code>none()</code> <p>Returns:</p> Type Description <code>Text</code> <p>The created <code>Text</code> instance.</p> Source code in <code>pixano/features/schemas/views/text.py</code> <pre><code>def create_text(\n    content: str,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    parent_ref: ViewRef = ViewRef.none(),\n) -&gt; Text:\n    \"\"\"Create a `Text` instance.\n\n    Args:\n        content: The text content.\n        id: Text ID.\n        item_ref: Item reference.\n        parent_ref: Parent view reference.\n\n    Returns:\n        The created `Text` instance.\n    \"\"\"\n    return Text(id=id, item_ref=item_ref, parent_ref=parent_ref, content=content)\n</code></pre>"},{"location":"api_reference/features/schemas/views/text/#pixano.features.schemas.views.text.is_text","title":"<code>is_text(cls, strict=False)</code>","text":"<p>Check if the given class is <code>Text</code> or a subclass of <code>Text</code>.</p> Source code in <code>pixano/features/schemas/views/text.py</code> <pre><code>def is_text(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is `Text` or a subclass of `Text`.\"\"\"\n    return issubclass_strict(cls, Text, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/views/video/","title":"video","text":""},{"location":"api_reference/features/schemas/views/video/#pixano.features.schemas.views.video","title":"<code>pixano.features.schemas.views.video</code>","text":""},{"location":"api_reference/features/schemas/views/video/#pixano.features.schemas.views.video.Video","title":"<code>Video(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>View</code></p> <p>Video view.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The video URL.</p> <code>num_frames</code> <code>int</code> <p>The number of frames in the video.</p> <code>fps</code> <code>float</code> <p>The frames per second of the video.</p> <code>width</code> <code>int</code> <p>The video width.</p> <code>height</code> <code>int</code> <p>The video height.</p> <code>format</code> <code>str</code> <p>The video format.</p> <code>duration</code> <code>float</code> <p>The video duration.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/views/video/#pixano.features.schemas.views.video.create_video","title":"<code>create_video(url, id='', item_ref=ItemRef.none(), parent_ref=ViewRef.none(), num_frames=None, fps=None, width=None, height=None, format=None, duration=None, url_relative_path=None)</code>","text":"<p>Create a <code>Video</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Path</code> <p>The image URL. If not relative, the URL is converted to a relative path using <code>other_path</code>.</p> required <code>id</code> <code>str</code> <p>Point cloud ID.</p> <code>''</code> <code>item_ref</code> <code>ItemRef</code> <p>Item reference.</p> <code>none()</code> <code>parent_ref</code> <code>ViewRef</code> <p>Parent view reference.</p> <code>none()</code> <code>num_frames</code> <code>int | None</code> <p>The number of frames in the video. If None, the number of frames is extracted from the video file.</p> <code>None</code> <code>fps</code> <code>float | None</code> <p>The frames per second of the video. If None, the fps is extracted from the video file.</p> <code>None</code> <code>width</code> <code>int | None</code> <p>The video width. If None, the width is extracted from the video file.</p> <code>None</code> <code>height</code> <code>int | None</code> <p>The video height. If None, the height is extracted from the video file.</p> <code>None</code> <code>format</code> <code>str | None</code> <p>The video format. If None, the format is extracted from the video file.</p> <code>None</code> <code>duration</code> <code>float | None</code> <p>The video duration. If None, the duration is extracted from the video file.</p> <code>None</code> <code>url_relative_path</code> <code>Path | None</code> <p>The path to convert the URL to a relative path.</p> <code>None</code> <p>Returns:</p> Type Description <code>Video</code> <p>The created <code>Video</code> instance.</p> Source code in <code>pixano/features/schemas/views/video.py</code> <pre><code>def create_video(\n    url: Path,\n    id: str = \"\",\n    item_ref: ItemRef = ItemRef.none(),\n    parent_ref: ViewRef = ViewRef.none(),\n    num_frames: int | None = None,\n    fps: float | None = None,\n    width: int | None = None,\n    height: int | None = None,\n    format: str | None = None,\n    duration: float | None = None,\n    url_relative_path: Path | None = None,\n) -&gt; Video:\n    \"\"\"Create a `Video` instance.\n\n    Args:\n        url: The image URL. If not relative, the URL is converted to a relative path using `other_path`.\n        id: Point cloud ID.\n        item_ref: Item reference.\n        parent_ref: Parent view reference.\n        num_frames: The number of frames in the video. If None, the number of frames is\n            extracted from the video file.\n        fps: The frames per second of the video. If None, the fps is extracted from the video\n            file.\n        width: The video width. If None, the width is extracted from the video file.\n        height: The video height. If None, the height is extracted from the video file.\n        format: The video format. If None, the format is extracted from the video file.\n        duration: The video duration. If None, the duration is extracted from the video file.\n        url_relative_path: The path to convert the URL to a relative path.\n\n    Returns:\n        The created `Video` instance.\n    \"\"\"\n    none_conditions = [\n        num_frames is None,\n        fps is None,\n        width is None,\n        height is None,\n        format is None,\n        duration is None,\n    ]\n    not_none_conditions = [\n        num_frames is not None,\n        fps is not None,\n        width is not None,\n        height is not None,\n        format is not None,\n        duration is not None,\n    ]\n    if not all(none_conditions) and not all(not_none_conditions):\n        raise ValueError(\n            \"All or none of the following arguments must be provided: width, height, format, num_frames, fps, duration\"\n        )\n\n    url = Path(url)\n    if id is None:\n        id = shortuuid.uuid()\n\n    if width is None:\n        try:\n            import ffmpeg\n        except ImportError:\n            raise ImportError(\"To load video files metadata, install ffmpeg\")\n        try:\n            metadata = ffmpeg.probe(str(url.resolve()), cmd=\"ffprobe\")[\"streams\"][0]\n        except FileNotFoundError:\n            raise FileNotFoundError(\"File not found or ffprobe is not installed.\")\n        r_frame_rate = metadata[\"r_frame_rate\"].split(\"/\")\n        fps = float(r_frame_rate[0]) / float(r_frame_rate[1])\n        num_frames = int(metadata[\"nb_frames\"])\n        width = int(metadata[\"width\"])\n        height = int(metadata[\"height\"])\n        format = url.suffix[1:]\n        duration = float(metadata[\"duration\"])\n\n    if url_relative_path is not None:\n        url_relative_path = Path(url_relative_path)\n        url = url.relative_to(url_relative_path)\n\n    return Video(\n        id=id,\n        item_ref=item_ref,\n        parent_ref=parent_ref,\n        url=url.as_posix(),\n        num_frames=num_frames,\n        fps=fps,\n        width=width,\n        height=height,\n        format=format,\n        duration=duration,\n    )\n</code></pre>"},{"location":"api_reference/features/schemas/views/video/#pixano.features.schemas.views.video.is_video","title":"<code>is_video(cls, strict=False)</code>","text":"<p>Check if the given class is a <code>Video</code> or a subclass of <code>Video</code>.</p> Source code in <code>pixano/features/schemas/views/video.py</code> <pre><code>def is_video(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given class is a `Video` or a subclass of `Video`.\"\"\"\n    return issubclass_strict(cls, Video, strict)\n</code></pre>"},{"location":"api_reference/features/schemas/views/view/","title":"view","text":""},{"location":"api_reference/features/schemas/views/view/#pixano.features.schemas.views.view","title":"<code>pixano.features.schemas.views.view</code>","text":""},{"location":"api_reference/features/schemas/views/view/#pixano.features.schemas.views.view.View","title":"<code>View(created_at=None, updated_at=None, **data)</code>","text":"<p>               Bases: <code>BaseSchema</code></p> <p>View base class.</p> <p>Views are used to define a view in a dataset such as an image, a point cloud, a text. It can refer to an item and a parent view.</p> <p>Attributes:</p> Name Type Description <code>item_ref</code> <code>ItemRef</code> <p>Reference to the view's item.</p> <code>parent_ref</code> <code>ViewRef</code> <p>Reference to the view's parent view.</p> Source code in <code>pixano/features/schemas/base_schema.py</code> <pre><code>def __init__(self, /, created_at: datetime | None = None, updated_at: datetime | None = None, **data: Any):\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n        data: The data of the object validated by Pydantic.\n    \"\"\"\n    created_at, updated_at = validate_and_init_create_at_and_update_at(created_at, updated_at)\n    data.update({\"created_at\": created_at, \"updated_at\": updated_at})\n    super().__init__(**data)\n</code></pre>"},{"location":"api_reference/features/schemas/views/view/#pixano.features.schemas.views.view.View.item","title":"<code>item</code>  <code>property</code>","text":"<p>Get the view's item.</p>"},{"location":"api_reference/features/schemas/views/view/#pixano.features.schemas.views.view.View.parent","title":"<code>parent</code>  <code>property</code>","text":"<p>Get the view's parent view.</p>"},{"location":"api_reference/features/schemas/views/view/#pixano.features.schemas.views.view.is_view","title":"<code>is_view(cls, strict=False)</code>","text":"<p>Check if a class is a <code>View</code> or subclass of <code>View</code>.</p> Source code in <code>pixano/features/schemas/views/view.py</code> <pre><code>def is_view(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `View` or subclass of `View`.\"\"\"\n    return issubclass_strict(cls, View, strict)\n</code></pre>"},{"location":"api_reference/features/types/base_type/","title":"base_type","text":""},{"location":"api_reference/features/types/base_type/#pixano.features.types.base_type","title":"<code>pixano.features.types.base_type</code>","text":""},{"location":"api_reference/features/types/base_type/#pixano.features.types.base_type.BaseType","title":"<code>BaseType</code>","text":"<p>               Bases: <code>LanceModel</code></p> <p>Base class for all Pixano types.</p> <p>This class should be inherited by all Pixano types. BaseSchemas can only have a primitive type or a <code>BaseType</code> attribute.</p>"},{"location":"api_reference/features/types/base_type/#pixano.features.types.base_type.is_base_type","title":"<code>is_base_type(cls, strict=False)</code>","text":"<p>Check if a class is a <code>BaseType</code> or subclass of <code>BaseType</code>.</p> Source code in <code>pixano/features/types/base_type.py</code> <pre><code>def is_base_type(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `BaseType` or subclass of `BaseType`.\"\"\"\n    return issubclass_strict(cls, BaseType, strict)\n</code></pre>"},{"location":"api_reference/features/types/nd_array_float/","title":"nd_array_float","text":""},{"location":"api_reference/features/types/nd_array_float/#pixano.features.types.nd_array_float","title":"<code>pixano.features.types.nd_array_float</code>","text":""},{"location":"api_reference/features/types/nd_array_float/#pixano.features.types.nd_array_float.create_ndarray_float","title":"<code>create_ndarray_float(values, shape)</code>","text":"<p>Create a <code>NDArrayFloat</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[float]</code> <p>The list of floating-point values in the array.</p> required <code>shape</code> <code>list[int]</code> <p>The shape of the array, represented as a list of integers.</p> required <p>Returns:</p> Type Description <code>NDArrayFloat</code> <p>The created <code>NDArrayFloat</code> instance.</p> Source code in <code>pixano/features/types/nd_array_float.py</code> <pre><code>def create_ndarray_float(values: list[float], shape: list[int]) -&gt; NDArrayFloat:\n    \"\"\"Create a `NDArrayFloat` instance.\n\n    Args:\n        values: The list of floating-point values in the array.\n        shape: The shape of the array, represented as a list of integers.\n\n    Returns:\n        The created `NDArrayFloat` instance.\n    \"\"\"\n    return NDArrayFloat(values=values, shape=shape)\n</code></pre>"},{"location":"api_reference/features/types/nd_array_float/#pixano.features.types.nd_array_float.is_ndarray_float","title":"<code>is_ndarray_float(cls, strict=False)</code>","text":"<p>Check if a class is a <code>NDArrayFloat</code> or a subclass of <code>NDArrayFloat</code>.</p> Source code in <code>pixano/features/types/nd_array_float.py</code> <pre><code>def is_ndarray_float(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `NDArrayFloat` or a subclass of `NDArrayFloat`.\"\"\"\n    return issubclass_strict(cls, NDArrayFloat, strict)\n</code></pre>"},{"location":"api_reference/features/types/registry/","title":"registry","text":""},{"location":"api_reference/features/types/registry/#pixano.features.types.registry","title":"<code>pixano.features.types.registry</code>","text":""},{"location":"api_reference/features/types/schema_reference/","title":"schema_reference","text":""},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference","title":"<code>pixano.features.types.schema_reference</code>","text":""},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.AnnotationRef","title":"<code>AnnotationRef</code>","text":"<p>               Bases: <code>SchemaRef['Annotation']</code></p> <p>Annotation reference.</p>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.EmbeddingRef","title":"<code>EmbeddingRef</code>","text":"<p>               Bases: <code>SchemaRef['Embedding']</code></p> <p>Embedding reference.</p>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.EntityRef","title":"<code>EntityRef</code>","text":"<p>               Bases: <code>SchemaRef['Entity']</code></p> <p>Entity reference.</p>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.ItemRef","title":"<code>ItemRef</code>","text":"<p>               Bases: <code>SchemaRef['Item']</code></p> <p>Item reference.</p>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.SchemaRef","title":"<code>SchemaRef</code>","text":"<p>               Bases: <code>BaseType</code>, <code>Generic[T]</code></p> <p>A schema reference.</p> <p>A schema reference is used to reference a schema in a dataset. If an id is provided, the reference points to a specific element stored in the table associated to the schema.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the schema.</p> <code>id</code> <code>str</code> <p>The id of the schema.</p>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.SchemaRef.none","title":"<code>none()</code>  <code>classmethod</code>","text":"<p>Return a reference to no schema.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; \"SchemaRef\":\n    \"\"\"Return a reference to no schema.\"\"\"\n    return cls(name=\"\", id=\"\")\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.SchemaRef.resolve","title":"<code>resolve(dataset)</code>","text":"<p>Resolve the reference to the schema.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def resolve(self, dataset: \"Dataset\") -&gt; T:\n    \"\"\"Resolve the reference to the schema.\"\"\"\n    return dataset.resolve_ref(self)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.SourceRef","title":"<code>SourceRef</code>","text":"<p>               Bases: <code>SchemaRef['Source']</code></p> <p>Source reference.</p>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.ViewRef","title":"<code>ViewRef</code>","text":"<p>               Bases: <code>SchemaRef['View']</code></p> <p>View reference.</p>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.create_annotation_ref","title":"<code>create_annotation_ref(id, name)</code>","text":"<p>Create an annotation reference.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def create_annotation_ref(id: str, name: str) -&gt; AnnotationRef:\n    \"\"\"Create an annotation reference.\"\"\"\n    return AnnotationRef(name=name, id=id)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.create_embedding_ref","title":"<code>create_embedding_ref(id, name)</code>","text":"<p>Create an embedding reference.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def create_embedding_ref(id: str, name: str) -&gt; EmbeddingRef:\n    \"\"\"Create an embedding reference.\"\"\"\n    return EmbeddingRef(name=name, id=id)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.create_entity_ref","title":"<code>create_entity_ref(id, name)</code>","text":"<p>Create an entity reference.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def create_entity_ref(id: str, name: str) -&gt; EntityRef:\n    \"\"\"Create an entity reference.\"\"\"\n    return EntityRef(name=name, id=id)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.create_item_ref","title":"<code>create_item_ref(id, name='item')</code>","text":"<p>Create an item reference.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def create_item_ref(id: str, name: str = \"item\") -&gt; ItemRef:\n    \"\"\"Create an item reference.\"\"\"\n    return ItemRef(name=name, id=id)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.create_schema_ref","title":"<code>create_schema_ref(id, name)</code>","text":"<p>Create a schema reference.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def create_schema_ref(id: str, name: str) -&gt; SchemaRef:\n    \"\"\"Create a schema reference.\"\"\"\n    return SchemaRef(name=name, id=id)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.create_source_ref","title":"<code>create_source_ref(id)</code>","text":"<p>Create a source reference.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def create_source_ref(id: str) -&gt; SourceRef:\n    \"\"\"Create a source reference.\"\"\"\n    return SourceRef(id=id)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.create_view_ref","title":"<code>create_view_ref(id, name)</code>","text":"<p>Create a view reference.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def create_view_ref(id: str, name: str) -&gt; ViewRef:\n    \"\"\"Create a view reference.\"\"\"\n    return ViewRef(name=name, id=id)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.is_annotation_ref","title":"<code>is_annotation_ref(cls, strict=False)</code>","text":"<p>Check if a class is an <code>AnnotationRef</code> or subclass of <code>AnnotationRef</code>.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def is_annotation_ref(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is an `AnnotationRef` or subclass of `AnnotationRef`.\"\"\"\n    return issubclass_strict(cls, AnnotationRef, strict)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.is_embedding_ref","title":"<code>is_embedding_ref(cls, strict=False)</code>","text":"<p>Check if a class is an <code>EmbeddingRef</code> or subclass of <code>EmbeddingRef</code>.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def is_embedding_ref(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is an `EmbeddingRef` or subclass of `EmbeddingRef`.\"\"\"\n    return issubclass_strict(cls, EmbeddingRef, strict)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.is_entity_ref","title":"<code>is_entity_ref(cls, strict=False)</code>","text":"<p>Check if a class is an <code>EntityRef</code> or subclass of <code>EntityRef</code>.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def is_entity_ref(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is an `EntityRef` or subclass of `EntityRef`.\"\"\"\n    return issubclass_strict(cls, EntityRef, strict)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.is_item_ref","title":"<code>is_item_ref(cls, strict=False)</code>","text":"<p>Check if a class is an <code>ItemRef</code> or subclass of <code>ItemRef</code>.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def is_item_ref(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is an `ItemRef` or subclass of `ItemRef`.\"\"\"\n    return issubclass_strict(cls, ItemRef, strict)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.is_schema_ref","title":"<code>is_schema_ref(cls, strict=False)</code>","text":"<p>Check if a class is a <code>SchemaRef</code> or subclass of <code>SchemaRef</code>.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def is_schema_ref(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `SchemaRef` or subclass of `SchemaRef`.\"\"\"\n    return issubclass_strict(cls, SchemaRef, strict)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.is_source_ref","title":"<code>is_source_ref(cls, strict=False)</code>","text":"<p>Check if a class is a <code>SourceRef</code> or subclass of <code>SourceRef</code>.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def is_source_ref(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `SourceRef` or subclass of `SourceRef`.\"\"\"\n    return issubclass_strict(cls, SourceRef, strict)\n</code></pre>"},{"location":"api_reference/features/types/schema_reference/#pixano.features.types.schema_reference.is_view_ref","title":"<code>is_view_ref(cls, strict=False)</code>","text":"<p>Check if a class is a <code>ViewRef</code> or subclass of <code>ViewRef</code>.</p> Source code in <code>pixano/features/types/schema_reference.py</code> <pre><code>def is_view_ref(cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a class is a `ViewRef` or subclass of `ViewRef`.\"\"\"\n    return issubclass_strict(cls, ViewRef, strict)\n</code></pre>"},{"location":"api_reference/features/utils/boxes/","title":"boxes","text":""},{"location":"api_reference/features/utils/boxes/#pixano.features.utils.boxes","title":"<code>pixano.features.utils.boxes</code>","text":""},{"location":"api_reference/features/utils/boxes/#pixano.features.utils.boxes.denormalize_coords","title":"<code>denormalize_coords(coord, height, width, rounded_int=True)</code>","text":"<p>Denormalize coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>list[float]</code> <p>Normalized coordinates.</p> required <code>height</code> <code>int</code> <p>Height.</p> required <code>width</code> <code>int</code> <p>Width.</p> required <code>rounded_int</code> <code>bool</code> <p>True to round denormalized float to nearest integer.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>Unnormalized coordinates.</p> Source code in <code>pixano/features/utils/boxes.py</code> <pre><code>def denormalize_coords(coord: list[float], height: int, width: int, rounded_int: bool = True) -&gt; list[float]:\n    \"\"\"Denormalize coordinates.\n\n    Args:\n        coord: Normalized coordinates.\n        height: Height.\n        width: Width.\n        rounded_int: True to round denormalized float to nearest integer.\n\n    Returns:\n        Unnormalized coordinates.\n    \"\"\"\n    denorm = []\n\n    for i, c in enumerate(coord):\n        if i % 2 == 0:\n            denorm.append(round(c * width) if rounded_int else c * width)\n        else:\n            denorm.append(round(c * height) if rounded_int else c * height)\n\n    return denorm\n</code></pre>"},{"location":"api_reference/features/utils/boxes/#pixano.features.utils.boxes.mask_to_bbox","title":"<code>mask_to_bbox(mask)</code>","text":"<p>Return the smallest bounding box containing all the mask pixels.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>Mask as NumPy Array.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>Normalized xywh bounding box.</p> Source code in <code>pixano/features/utils/boxes.py</code> <pre><code>def mask_to_bbox(mask: np.ndarray) -&gt; list[float]:\n    \"\"\"Return the smallest bounding box containing all the mask pixels.\n\n    Args:\n        mask: Mask as NumPy Array.\n\n    Returns:\n        Normalized xywh bounding box.\n    \"\"\"\n    height, width = mask.shape\n    bool_mask = np.array(mask).astype(bool)\n\n    # Find all columns and rows that contain ones\n    rows = np.any(bool_mask, axis=1)\n    cols = np.any(bool_mask, axis=0)\n\n    # Find the min and max col/row index that contain ones\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    # Calculate bbox height and width\n    w = (cmax - cmin + 1) / width\n    h = (rmax - rmin + 1) / height\n\n    return [cmin / width, rmin / height, w, h]\n</code></pre>"},{"location":"api_reference/features/utils/boxes/#pixano.features.utils.boxes.normalize_coords","title":"<code>normalize_coords(coord, height, width)</code>","text":"<p>Normalize coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>list[float]</code> <p>Unnormalized coordinates.</p> required <code>height</code> <code>int</code> <p>Height.</p> required <code>width</code> <code>int</code> <p>Width.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>Normalized coordinates.</p> Source code in <code>pixano/features/utils/boxes.py</code> <pre><code>def normalize_coords(coord: list[float], height: int, width: int) -&gt; list[float]:\n    \"\"\"Normalize coordinates.\n\n    Args:\n        coord: Unnormalized coordinates.\n        height: Height.\n        width: Width.\n\n    Returns:\n        Normalized coordinates.\n    \"\"\"\n    norm = []\n\n    for i, c in enumerate(coord):\n        if i % 2 == 0:\n            norm.append(c / width)\n        else:\n            norm.append(c / height)\n\n    return norm\n</code></pre>"},{"location":"api_reference/features/utils/boxes/#pixano.features.utils.boxes.urle_to_bbox","title":"<code>urle_to_bbox(urle)</code>","text":"<p>Return the smallest bounding box containing all the mask pixels.</p> <p>Parameters:</p> Name Type Description Default <code>urle</code> <code>dict</code> <p>Mask as uncompressed RLE.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>Normalized xywh bounding box.</p> Source code in <code>pixano/features/utils/boxes.py</code> <pre><code>def urle_to_bbox(urle: dict) -&gt; list[float]:\n    \"\"\"Return the smallest bounding box containing all the mask pixels.\n\n    Args:\n        urle: Mask as uncompressed RLE.\n\n    Returns:\n        Normalized xywh bounding box.\n    \"\"\"\n    return mask_to_bbox(rle_to_mask(urle_to_rle(urle)))\n</code></pre>"},{"location":"api_reference/features/utils/boxes/#pixano.features.utils.boxes.xywh_to_xyxy","title":"<code>xywh_to_xyxy(xywh)</code>","text":"<p>Convert bounding box coordinates from xywh (using top left point as reference) to xyxy.</p> <p>Parameters:</p> Name Type Description Default <code>xywh</code> <code>list[float]</code> <p>xywh coordinates.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>xyxy coordinates.</p> Source code in <code>pixano/features/utils/boxes.py</code> <pre><code>def xywh_to_xyxy(xywh: list[float]) -&gt; list[float]:\n    \"\"\"Convert bounding box coordinates from xywh\n    (using top left point as reference) to xyxy.\n\n    Args:\n        xywh: xywh coordinates.\n\n    Returns:\n        xyxy coordinates.\n    \"\"\"\n    return [\n        float(xywh[0]),\n        float(xywh[1]),\n        float(xywh[0] + xywh[2]),\n        float(xywh[1] + xywh[3]),\n    ]\n</code></pre>"},{"location":"api_reference/features/utils/boxes/#pixano.features.utils.boxes.xyxy_to_xywh","title":"<code>xyxy_to_xywh(xyxy)</code>","text":"<p>Convert bounding box coordinates from xyxy to xywh (using top left point as reference).</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>list[float]</code> <p>xyxy coordinates.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>xywh coordinates.</p> Source code in <code>pixano/features/utils/boxes.py</code> <pre><code>def xyxy_to_xywh(xyxy: list[float]) -&gt; list[float]:\n    \"\"\"Convert bounding box coordinates from xyxy to xywh\n    (using top left point as reference).\n\n    Args:\n        xyxy: xyxy coordinates.\n\n    Returns:\n        xywh coordinates.\n    \"\"\"\n    return [\n        float(xyxy[0]),\n        float(xyxy[1]),\n        float(xyxy[2] - xyxy[0]),\n        float(xyxy[3] - xyxy[1]),\n    ]\n</code></pre>"},{"location":"api_reference/features/utils/creators/","title":"creators","text":""},{"location":"api_reference/features/utils/creators/#pixano.features.utils.creators","title":"<code>pixano.features.utils.creators</code>","text":""},{"location":"api_reference/features/utils/creators/#pixano.features.utils.creators.create_instance_of_pixano_type","title":"<code>create_instance_of_pixano_type(pix_type, **data)</code>","text":"<p>Create a pixano object.</p> Source code in <code>pixano/features/utils/creators.py</code> <pre><code>def create_instance_of_pixano_type(pix_type: type[\"BaseType\"], **data) -&gt; \"BaseType\":\n    \"\"\"Create a pixano object.\"\"\"\n    # Import here to avoid circular imports\n    from pixano.features.types import (\n        create_annotation_ref,\n        create_embedding_ref,\n        create_entity_ref,\n        create_item_ref,\n        create_ndarray_float,\n        create_schema_ref,\n        create_source_ref,\n        create_view_ref,\n        is_annotation_ref,\n        is_base_type,\n        is_embedding_ref,\n        is_entity_ref,\n        is_item_ref,\n        is_ndarray_float,\n        is_schema_ref,\n        is_source_ref,\n        is_view_ref,\n    )\n\n    if is_ndarray_float(pix_type, True):\n        return create_ndarray_float(**data)\n\n    elif is_schema_ref(pix_type, True):\n        return create_schema_ref(**data)\n\n    elif is_item_ref(pix_type, True):\n        return create_item_ref(**data)\n\n    elif is_view_ref(pix_type, True):\n        return create_view_ref(**data)\n\n    elif is_entity_ref(pix_type, True):\n        return create_entity_ref(**data)\n\n    elif is_annotation_ref(pix_type, True):\n        return create_annotation_ref(**data)\n\n    elif is_embedding_ref(pix_type, True):\n        return create_embedding_ref(**data)\n\n    elif is_source_ref(pix_type, True):\n        return create_source_ref(**data)\n\n    elif is_base_type(pix_type, False):\n        return pix_type(**data)\n\n    raise ValueError(f\"Type {pix_type} not supported.\")\n</code></pre>"},{"location":"api_reference/features/utils/creators/#pixano.features.utils.creators.create_instance_of_schema","title":"<code>create_instance_of_schema(schema, **data)</code>","text":"<p>Create a row in a Schema.</p> Source code in <code>pixano/features/utils/creators.py</code> <pre><code>def create_instance_of_schema(schema: type[\"BaseSchema\"], **data) -&gt; \"BaseSchema\":\n    \"\"\"Create a row in a Schema.\"\"\"\n    # Import here to avoid circular imports\n    from pixano.features.schemas import (\n        create_bbox,\n        create_bbox3d,\n        create_cam_calibration,\n        create_compressed_rle,\n        create_image,\n        create_keypoints,\n        create_keypoints3d,\n        create_sequence_frame,\n        create_track,\n        create_tracklet,\n        create_video,\n        is_base_schema,\n        is_bbox,\n        is_bbox3d,\n        is_cam_calibration,\n        is_compressed_rle,\n        is_image,\n        is_keypoints,\n        is_keypoints3d,\n        is_sequence_frame,\n        is_track,\n        is_tracklet,\n        is_video,\n    )\n\n    if is_image(schema, strict=True):\n        return create_image(**data)\n\n    elif is_video(schema, strict=True):\n        return create_video(**data)\n\n    elif is_sequence_frame(schema, strict=True):\n        return create_sequence_frame(**data)\n\n    elif is_tracklet(schema, strict=True):\n        return create_tracklet(**data)\n\n    elif is_track(schema, True):\n        return create_track(**data)\n\n    elif is_bbox(schema, True):\n        return create_bbox(**data)\n\n    elif is_bbox3d(schema, True):\n        return create_bbox3d(**data)\n\n    elif is_cam_calibration(schema, True):\n        return create_cam_calibration(**data)\n\n    elif is_compressed_rle(schema, True):\n        return create_compressed_rle(**data)\n\n    elif is_keypoints(schema, True):\n        return create_keypoints(**data)\n\n    elif is_keypoints3d(schema, True):\n        return create_keypoints3d(**data)\n\n    elif is_base_schema(schema, False):\n        return schema(**data)\n\n    raise ValueError(f\"Schema {schema} is not a base schema.\")\n</code></pre>"},{"location":"api_reference/features/utils/image/","title":"image","text":""},{"location":"api_reference/features/utils/image/#pixano.features.utils.image","title":"<code>pixano.features.utils.image</code>","text":""},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.base64_to_image","title":"<code>base64_to_image(base64_image)</code>","text":"<p>Decode image from base64 to Pillow.</p> <p>Expect the image to be formatted as \"data:image/{image_format};base64,{base64}\".</p> <p>Parameters:</p> Name Type Description Default <code>base64_image</code> <code>str</code> <p>Image as base64.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Pillow image.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def base64_to_image(base64_image: str) -&gt; Image.Image:\n    \"\"\"Decode image from base64 to Pillow.\n\n    Expect the image to be formatted as \"data:image/{image_format};base64,{base64}\".\n\n    Args:\n        base64_image: Image as base64.\n\n    Returns:\n        Pillow image.\n    \"\"\"\n    image_data = base64.b64decode(base64_image.split(\",\", maxsplit=1)[1].encode(\"utf-8\"))\n    return Image.open(io.BytesIO(image_data))\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.binary_to_url","title":"<code>binary_to_url(im_bytes)</code>","text":"<p>Encode image from binary to base 64 URL.</p> <p>Parameters:</p> Name Type Description Default <code>im_bytes</code> <code>bytes</code> <p>Image as binary.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Image base 64 URL.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def binary_to_url(im_bytes: bytes) -&gt; str:\n    \"\"\"Encode image from binary to base 64 URL.\n\n    Args:\n        im_bytes: Image as binary.\n\n    Returns:\n        Image base 64 URL.\n    \"\"\"\n    encoded = base64.b64encode(im_bytes).decode(\"utf-8\")\n    return f\"data:image;base64,{encoded}\"\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.depth_array_to_gray","title":"<code>depth_array_to_gray(depth, valid_start=0.2, valid_end=1, scale=1.0)</code>","text":"<p>Encode depth array to gray levels.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>ndarray</code> <p>Depth array</p> required <code>valid_start</code> <code>float</code> <p>Valid start.</p> <code>0.2</code> <code>valid_end</code> <code>float</code> <p>Valid end.</p> <code>1</code> <code>scale</code> <code>float</code> <p>Scale.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Depth array in gray levels.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def depth_array_to_gray(\n    depth: np.ndarray,\n    valid_start: float = 0.2,\n    valid_end: float = 1,\n    scale: float = 1.0,\n) -&gt; np.ndarray:\n    \"\"\"Encode depth array to gray levels.\n\n    Args:\n        depth: Depth array\n        valid_start: Valid start.\n        valid_end: Valid end.\n        scale: Scale.\n\n    Returns:\n        Depth array in gray levels.\n    \"\"\"\n    mask = depth &gt; 1\n\n    # Scale gives depth in mm\n    depth_n = depth * scale * 0.001\n\n    if mask.sum() &gt; 0:\n        depth_n[mask] -= depth_n[mask].min()\n        depth_n[mask] /= depth_n[mask].max() / (valid_end - valid_start)\n        depth_n[mask] += valid_start\n\n    depth_n *= 255\n    depth_n = cv2.applyColorMap(depth_n[:, :, np.newaxis].astype(np.uint8), cv2.COLORMAP_PLASMA)\n\n    return depth_n\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.depth_file_to_binary","title":"<code>depth_file_to_binary(depth_path)</code>","text":"<p>Encode depth file to RGB image in binary.</p> <p>Parameters:</p> Name Type Description Default <code>depth_path</code> <code>str</code> <p>Depth file path.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Depth file as RGB image in binary.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def depth_file_to_binary(depth_path: str) -&gt; bytes:\n    \"\"\"Encode depth file to RGB image in binary.\n\n    Args:\n        depth_path: Depth file path.\n\n    Returns:\n        Depth file as RGB image in binary.\n    \"\"\"\n    depth = cv2.imread(depth_path, cv2.IMREAD_ANYDEPTH).astype(np.float32)\n    depth = depth_array_to_gray(depth)\n    depth_rgb = Image.fromarray(depth)\n\n    return image_to_binary(depth_rgb)\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.encode_rle","title":"<code>encode_rle(mask, height, width)</code>","text":"<p>Encode mask from polygons / uncompressed RLE / RLE to RLE.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list[list] | dict</code> <p>Mask as polygons / uncompressed RLE / RLE.</p> required <code>height</code> <code>int</code> <p>Image height.</p> required <code>width</code> <code>int</code> <p>Image width.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Mask as RLE.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def encode_rle(mask: list[list] | dict, height: int, width: int) -&gt; dict:\n    \"\"\"Encode mask from polygons / uncompressed RLE / RLE to RLE.\n\n    Args:\n        mask: Mask as polygons / uncompressed RLE / RLE.\n        height: Image height.\n        width: Image width.\n\n    Returns:\n        Mask as RLE.\n    \"\"\"\n    if isinstance(mask, list):\n        return polygons_to_rle(mask, height, width)\n    elif isinstance(mask, dict):\n        if isinstance(mask[\"counts\"], list):\n            return urle_to_rle(mask)\n        return mask\n    raise ValueError(\"Mask must be a list of polygons or an uncompressed RLE\")\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.generate_text_image_base64","title":"<code>generate_text_image_base64(text, width=128, height=128, font_size=16)</code>","text":"<p>Generate a thumbnail displaying given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>input text</p> required <code>width</code> <code>int</code> <p>thumbnail width</p> <code>128</code> <code>height</code> <code>int</code> <p>thumbnail height</p> <code>128</code> <code>font_size</code> <code>int</code> <p>font size</p> <code>16</code> <p>Returns:</p> Type Description <code>str</code> <p>base64 image of given text.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def generate_text_image_base64(text: str, width: int = 128, height: int = 128, font_size: int = 16) -&gt; str:\n    \"\"\"Generate a thumbnail displaying given text.\n\n    Args:\n        text: input text\n        width: thumbnail width\n        height: thumbnail height\n        font_size: font size\n\n    Returns:\n        base64 image of given text.\n    \"\"\"\n    image = Image.new(\"RGB\", (width, height), \"white\")\n\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n    except IOError:\n        font = ImageFont.load_default(size=font_size)\n\n    draw = ImageDraw.Draw(image)\n\n    # Set text boundaries\n    max_text_width = width - 10  # 5px margin on both sides\n    x_start = 5  # Left margin\n    y_start = 5  # Top margin\n    line_height = font.size + 2  # Line spacing\n\n    # Manually wrap text based on actual pixel width\n    words = text.split()\n    lines = []\n    current_line = \"\"\n\n    for word in words:\n        test_line = current_line + \" \" + word if current_line else word\n        bbox = draw.textbbox((0, 0), test_line, font=font)\n        text_width = bbox[2] - bbox[0]\n\n        if text_width &lt;= max_text_width:\n            current_line = test_line  # Word fits, add it to the line\n        else:\n            lines.append(current_line)  # Save current line\n            current_line = word  # Start new line with this word\n\n    if current_line:\n        lines.append(current_line)  # Add last line\n\n    # Calculate vertical centering\n    total_text_height = len(lines) * line_height\n    y_position = y_start + (height - total_text_height) // 2\n\n    # Draw text aligned to the left\n    for line in lines:\n        draw.text((x_start, y_position), line, fill=\"black\", font=font)\n        y_position += line_height\n\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    return \"data:image/png;base64,\" + base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.get_image_thumbnail","title":"<code>get_image_thumbnail(image, size)</code>","text":"<p>Get image thumbnail.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Pillow Image.</p> required <code>size</code> <code>tuple[int, int]</code> <p>Thumbnail size.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image thumbnail as Pillow.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def get_image_thumbnail(image: Image.Image, size: tuple[int, int]) -&gt; Image.Image:\n    \"\"\"Get image thumbnail.\n\n    Args:\n        image: Pillow Image.\n        size: Thumbnail size.\n\n    Returns:\n        Image thumbnail as Pillow.\n    \"\"\"\n    if (\n        not isinstance(size, tuple)\n        or len(size) != 2\n        or not isinstance(size[0], int)\n        or not isinstance(size[1], int)\n        or size[0] &lt;= 0\n        or size[1] &lt;= 0\n    ):\n        raise ValueError(f\"Invalid thumbnail size: {size}\")\n    thumbnail = image.copy()\n    thumbnail.thumbnail(size)\n    return thumbnail\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.image_to_base64","title":"<code>image_to_base64(image, format=None)</code>","text":"<p>Encode image from Pillow to base64.</p> <p>The image is returned as a base64 string formatted as \"data:image/{image_format};base64,{base64}\".</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Pillow image.</p> required <code>format</code> <code>str | None</code> <p>Image format.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Image as base64.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def image_to_base64(image: Image.Image, format: str | None = None) -&gt; str:\n    \"\"\"Encode image from Pillow to base64.\n\n    The image is returned as a base64 string formatted as\n    \"data:image/{image_format};base64,{base64}\".\n\n    Args:\n        image: Pillow image.\n        format: Image format.\n\n    Returns:\n        Image as base64.\n    \"\"\"\n    if image.format is None and format is None:\n        raise ValueError(\"Image format is not defined\")\n\n    buffered = io.BytesIO()\n    out_format = format or image.format\n    if out_format.upper() == \"UNKNOWN\":\n        out_format = \"JPEG\"\n    image.save(buffered, format=out_format)\n\n    encoded = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\n    return f\"data:image/{out_format.lower()};base64,{encoded}\"\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.image_to_binary","title":"<code>image_to_binary(image, im_format='PNG')</code>","text":"<p>Encode an image from Pillow to binary.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Pillow image.</p> required <code>im_format</code> <code>str</code> <p>Image file extension.</p> <code>'PNG'</code> <p>Returns:</p> Type Description <code>bytes</code> <p>Image as binary.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def image_to_binary(image: Image.Image, im_format: str = \"PNG\") -&gt; bytes:\n    \"\"\"Encode an image from Pillow to binary.\n\n    Args:\n        image: Pillow image.\n        im_format: Image file extension.\n\n    Returns:\n        Image as binary.\n    \"\"\"\n    with BytesIO() as output_bytes:\n        image.save(output_bytes, im_format)\n        im_bytes = output_bytes.getvalue()\n\n    return im_bytes\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.mask_area","title":"<code>mask_area(rle)</code>","text":"<p>Compute mask area.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, list[int] | bytes]</code> <p>Mask as RLE</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mask area</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def mask_area(rle: dict[str, list[int] | bytes]) -&gt; float:\n    \"\"\"Compute mask area.\n\n    Args:\n        rle: Mask as RLE\n\n    Returns:\n        Mask area\n    \"\"\"\n    return float(mask_api.area(rle))\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.mask_to_polygons","title":"<code>mask_to_polygons(mask)</code>","text":"<p>Encode mask from NumPy array to polygons.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>Mask as NumPy array</p> required <p>Returns:</p> Type Description <code>Tuple</code> <ul> <li>Mask as polygons</li> <li>True if mask has holes</li> </ul> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def mask_to_polygons(mask: np.ndarray) -&gt; tuple[list[list], bool]:\n    \"\"\"Encode mask from NumPy array to polygons.\n\n    Args:\n        mask: Mask as NumPy array\n\n    Returns:\n        Tuple:\n            - Mask as polygons\n            - True if mask has holes\n    \"\"\"\n    # Some versions of cv2 does not support incontiguous arr\n    mask = np.ascontiguousarray(mask)\n\n    # cv2.RETR_CCOMP flag retrieves all the contours and arranges them\n    # to a 2-level hierarchy.\n    # External contours (boundary) of the object are placed in hierarchy-1.\n    # Internal contours (holes) are placed in hierarchy-2.\n    # cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\n    res = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n    hierarchy = res[-1]\n\n    # If mask is empty\n    if hierarchy is None:\n        return [], False\n\n    # Check if mask has holes\n    has_holes = (hierarchy.reshape(-1, 4)[:, 3] &gt;= 0).sum() &gt; 0\n\n    res = res[-2]\n    res = [x.flatten() for x in res]\n\n    # The coordinates from OpenCV are integers in range [0, W-1 or H-1].\n    # We add 0.5 to turn them into real-value coordinate space. A better solution\n    # would be to first +0.5 and then dilate the returned polygon by 0.5.\n    res = [x + 0.5 for x in res if len(x) &gt;= 6]\n\n    # Convert np.array to lists\n    res = [x.tolist() for x in res]\n\n    return res, has_holes\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.mask_to_rle","title":"<code>mask_to_rle(mask)</code>","text":"<p>Encode mask from Pillow or NumPy array to RLE.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Image | ndarray</code> <p>Mask as Pillow or NumPy array.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Mask as RLE.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def mask_to_rle(mask: Image.Image | np.ndarray) -&gt; dict:\n    \"\"\"Encode mask from Pillow or NumPy array to RLE.\n\n    Args:\n        mask: Mask as Pillow or NumPy array.\n\n    Returns:\n        Mask as RLE.\n    \"\"\"\n    mask_array = np.asfortranarray(mask)\n    return mask_api.encode(mask_array)\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.polygons_to_rle","title":"<code>polygons_to_rle(polygons, height, width)</code>","text":"<p>Encode mask from polygons to RLE.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>list[list]</code> <p>Mask as polygons.</p> required <code>height</code> <code>int</code> <p>Image height.</p> required <code>width</code> <code>int</code> <p>Image width.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Mask as RLE.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def polygons_to_rle(polygons: list[list], height: int, width: int) -&gt; dict:\n    \"\"\"Encode mask from polygons to RLE.\n\n    Args:\n        polygons: Mask as polygons.\n        height: Image height.\n        width: Image width.\n\n    Returns:\n        Mask as RLE.\n    \"\"\"\n    rles = mask_api.frPyObjects(polygons, height, width)\n    return mask_api.merge(rles)\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.rle_to_mask","title":"<code>rle_to_mask(rle)</code>","text":"<p>Decode mask from RLE to NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, list[int] | bytes]</code> <p>Mask as RLE.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Mask as NumPy array.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def rle_to_mask(rle: dict[str, list[int] | bytes]) -&gt; np.ndarray:\n    \"\"\"Decode mask from RLE to NumPy array.\n\n    Args:\n        rle: Mask as RLE.\n\n    Returns:\n        Mask as NumPy array.\n    \"\"\"\n    return mask_api.decode(rle)\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.rle_to_polygons","title":"<code>rle_to_polygons(rle)</code>","text":"<p>Encode mask from RLE to polygons.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, list[int] | bytes]</code> <p>Mask as RLE.</p> required <p>Returns:</p> Type Description <code>list[list]</code> <p>Mask as polygons.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def rle_to_polygons(rle: dict[str, list[int] | bytes]) -&gt; list[list]:\n    \"\"\"Encode mask from RLE to polygons.\n\n    Args:\n        rle: Mask as RLE.\n\n    Returns:\n        Mask as polygons.\n    \"\"\"\n    if \"size\" not in rle:\n        raise ValueError(\"RLE must have a size\")\n    h, w = rle[\"size\"]\n    polygons, _ = mask_to_polygons(rle_to_mask(rle))\n\n    # Normalize point coordinates\n    for p in polygons:\n        p[::2] = [x / w for x in p[::2]]\n        p[1::2] = [y / h for y in p[1::2]]\n\n    return polygons\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.rle_to_urle","title":"<code>rle_to_urle(rle)</code>","text":"<p>Encode mask from RLE to uncompressed RLE.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict[str, list[int] | bytes]</code> <p>Mask as RLE.</p> required <p>Returns:</p> Type Description <code>dict[str, list[int]]</code> <p>Mask as uncompressed RLE.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def rle_to_urle(rle: dict[str, list[int] | bytes]) -&gt; dict[str, list[int]]:\n    \"\"\"Encode mask from RLE to uncompressed RLE.\n\n    Args:\n        rle: Mask as RLE.\n\n    Returns:\n        Mask as uncompressed RLE.\n    \"\"\"\n    if \"counts\" not in rle or rle[\"counts\"] is None:\n        raise ValueError(\"RLE must have counts\")\n    mask = rle_to_mask(rle)\n    urle = {\"counts\": [], \"size\": list(mask.shape)}\n\n    for i, (value, elements) in enumerate(groupby(mask.ravel(order=\"F\"))):\n        urle[\"counts\"].append(0 if i == 0 and value == 1 else len(list(elements)))\n\n    return urle\n</code></pre>"},{"location":"api_reference/features/utils/image/#pixano.features.utils.image.urle_to_rle","title":"<code>urle_to_rle(urle)</code>","text":"<p>Encode mask from uncompressed RLE to RLE.</p> <p>Parameters:</p> Name Type Description Default <code>urle</code> <code>dict[str, list[int]]</code> <p>Mask as uncompressed RLE.</p> required <p>Returns:</p> Type Description <code>dict[str, list[int] | bytes]</code> <p>Mask as RLE.</p> Source code in <code>pixano/features/utils/image.py</code> <pre><code>def urle_to_rle(urle: dict[str, list[int]]) -&gt; dict[str, list[int] | bytes]:\n    \"\"\"Encode mask from uncompressed RLE to RLE.\n\n    Args:\n        urle: Mask as uncompressed RLE.\n\n    Returns:\n        Mask as RLE.\n    \"\"\"\n    height, width = urle[\"size\"]\n    return mask_api.frPyObjects(urle, height, width)\n</code></pre>"},{"location":"api_reference/inference/mask_generation/","title":"mask_generation","text":""},{"location":"api_reference/inference/mask_generation/#pixano.inference.mask_generation","title":"<code>pixano.inference.mask_generation</code>","text":""},{"location":"api_reference/inference/mask_generation/#pixano.inference.mask_generation.image_mask_generation","title":"<code>image_mask_generation(client, media_dir, image, entity, source, image_embedding=None, high_resolution_features=None, reset_predictor=True, bbox=None, points=None, labels=None, **client_kwargs)</code>  <code>async</code>","text":"<p>Image mask generation task.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>PixanoInferenceClient</code> <p>Pixano inference client.</p> required <code>media_dir</code> <code>Path</code> <p>Media directory.</p> required <code>image</code> <code>Image</code> <p>Image to generate mask for.</p> required <code>entity</code> <code>Entity | None</code> <p>Entity associated with the image.</p> required <code>source</code> <code>Source</code> <p>The source refering to the model.</p> required <code>image_embedding</code> <code>ViewEmbedding | NDArrayFloat | LanceVector | None</code> <p>Image embedding.</p> <code>None</code> <code>high_resolution_features</code> <code>list[ViewEmbedding] | list[NDArrayFloat] | list[LanceVector] | None</code> <p>High resolution features.</p> <code>None</code> <code>reset_predictor</code> <code>bool</code> <p>True (default) if it's a new image.</p> <code>True</code> <code>bbox</code> <code>BBox | None</code> <p>Bounding box of the object in the original image.</p> <code>None</code> <code>points</code> <code>list[list[int]] | None</code> <p>Points to generate mask for.</p> <code>None</code> <code>labels</code> <code>list[int] | None</code> <p>Labels of the points. If 0, the point is background else the point is foreground.</p> <code>None</code> <code>client_kwargs</code> <code>Any</code> <p>Additional kwargs for the client to be passed.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CompressedRLE</code> <p>tuple of the compressed RLE mask, its score, the source of the image, the image embeddings and the high</p> <code>float</code> <p>resolution features The features are returned if not provided in the arguments otherwise None is returned.</p> Source code in <code>pixano/inference/mask_generation.py</code> <pre><code>async def image_mask_generation(\n    client: PixanoInferenceClient,\n    media_dir: Path,\n    image: Image,\n    entity: Entity | None,\n    source: Source,\n    image_embedding: ViewEmbedding | NDArrayFloat | LanceVector | None = None,\n    high_resolution_features: list[ViewEmbedding] | list[NDArrayFloat] | list[LanceVector] | None = None,\n    reset_predictor: bool = True,\n    bbox: BBox | None = None,\n    points: list[list[int]] | None = None,\n    labels: list[int] | None = None,\n    **client_kwargs: Any,\n) -&gt; tuple[CompressedRLE, float, NDArrayFloat | None, list[NDArrayFloat] | None]:\n    \"\"\"Image mask generation task.\n\n    Args:\n        client: Pixano inference client.\n        media_dir: Media directory.\n        image: Image to generate mask for.\n        entity: Entity associated with the image.\n        source: The source refering to the model.\n        image_embedding: Image embedding.\n        high_resolution_features: High resolution features.\n        reset_predictor: True (default) if it's a new image.\n        bbox: Bounding box of the object in the original image.\n        points: Points to generate mask for.\n        labels: Labels of the points. If 0, the point is background else the point is foreground.\n        client_kwargs: Additional kwargs for the client to be passed.\n\n    Returns:\n        tuple of the compressed RLE mask, its score, the source of the image, the image embeddings and the high\n        resolution features The features are returned if not provided in the arguments otherwise None is returned.\n    \"\"\"\n    image_request = image.url if is_url(image.url) else image.open(media_dir, \"base64\")\n    if image_embedding is None or isinstance(image_embedding, (LanceVector, NDArrayFloat)):\n        image_embedding_request = image_embedding\n    else:\n        image_embedding_request = NDArrayFloat(values=image_embedding.vector, shape=image_embedding.shape)\n    if high_resolution_features is None:\n        high_resolution_features_request = None\n    else:\n        high_resolution_features_request = []\n        for feature in high_resolution_features:\n            if isinstance(feature, (LanceVector, NDArrayFloat)):\n                high_resolution_features_request.append(feature)\n            else:\n                high_resolution_features_request.append(NDArrayFloat(values=feature.vector, shape=feature.shape))\n\n    if points is not None:\n        points_request = [points]\n    else:\n        points_request = None\n    if labels is not None:\n        labels_request = [labels]\n    else:\n        labels_request = None\n    if bbox is not None:\n        if bbox.is_normalized:\n            bbox = bbox.denormalize(height=image.height, width=image.width)\n        bbox_request = [[int(c) for c in bbox.xyxy_coords]]\n    else:\n        bbox_request = None\n\n    # Note: As long as we don't store pixano-inference embeddings somewhere, no need to return them\n    # More, it is VERY costly (around 13 sec) as it means to shape them and transfer them via HTTP.\n    return_image_embedding = False  # reset_predictor and image_embedding is None\n\n    request = ImageMaskGenerationRequest(\n        image=image_request,\n        image_embedding=image_embedding_request,\n        high_resolution_features=high_resolution_features_request,\n        reset_predictor=reset_predictor,\n        boxes=bbox_request,\n        points=points_request,\n        labels=labels_request,\n        num_multimask_outputs=1,\n        multimask_output=False,\n        return_image_embedding=return_image_embedding,\n        model=source.name,\n    )\n\n    response: ImageMaskGenerationResponse = await client.image_mask_generation(request, **client_kwargs)\n\n    inference_metadata = jsonable_encoder(\n        {\n            \"timestamp\": response.timestamp,\n            \"processing_time\": response.processing_time,\n            **response.metadata,\n        }\n    )\n\n    mask_inference: PixanoInferenceCompressedRLE = response.data.masks[0][0]\n    mask = CompressedRLE(\n        id=shortuuid.uuid(),\n        item_ref=image.item_ref,\n        view_ref=ViewRef(name=image.table_name, id=image.id),\n        entity_ref=EntityRef(name=entity.table_name, id=entity.id) if entity else EntityRef(name=\"\", id=\"\"),\n        source_ref=SourceRef(id=source.id),\n        inference_metadata=inference_metadata,\n        **mask_inference.model_dump(),\n    )\n    score = response.data.scores.values[0]\n\n    return mask, score, response.data.image_embedding, response.data.high_resolution_features\n</code></pre>"},{"location":"api_reference/inference/mask_generation/#pixano.inference.mask_generation.video_mask_generation","title":"<code>video_mask_generation(client, media_dir, video, source, entity=None, bbox=None, points=None, labels=None, **client_kwargs)</code>  <code>async</code>","text":"<p>Image mask generation task.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>PixanoInferenceClient</code> <p>Pixano inference client.</p> required <code>media_dir</code> <code>Path</code> <p>Media directory.</p> required <code>video</code> <code>list[SequenceFrame]</code> <p>Video as list of SequenceFrame.</p> required <code>source</code> <code>Source</code> <p>The source refering to the model.</p> required <code>entity</code> <code>Entity | None</code> <p>Entity to put objects in, if provided.</p> <code>None</code> <code>bbox</code> <code>BBox | None</code> <p>Bounding box of the object in the original image.</p> <code>None</code> <code>points</code> <code>list[list[int]] | None</code> <p>Points to generate mask for.</p> <code>None</code> <code>labels</code> <code>list[int] | None</code> <p>Labels of the points. If 0, the point is background else the point is foreground.</p> <code>None</code> <code>client_kwargs</code> <code>Any</code> <p>Additional kwargs for the client to be passed.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[CompressedRLE]</code> <p>tuple of the compressed RLE mask, its score, the source of the image, the image embeddings and the high</p> <code>list[int]</code> <p>resolution features The features are returned if not provided in the arguments otherwise None is returned.</p> Source code in <code>pixano/inference/mask_generation.py</code> <pre><code>async def video_mask_generation(\n    client: PixanoInferenceClient,\n    media_dir: Path,\n    video: list[SequenceFrame],\n    source: Source,\n    entity: Entity | None = None,\n    bbox: BBox | None = None,\n    points: list[list[int]] | None = None,\n    labels: list[int] | None = None,\n    **client_kwargs: Any,\n) -&gt; tuple[list[CompressedRLE], list[int], list[int]]:\n    \"\"\"Image mask generation task.\n\n    Args:\n        client: Pixano inference client.\n        media_dir: Media directory.\n        video: Video as list of SequenceFrame.\n        source: The source refering to the model.\n        entity: Entity to put objects in, if provided.\n        bbox: Bounding box of the object in the original image.\n        points: Points to generate mask for.\n        labels: Labels of the points. If 0, the point is background else the point is foreground.\n        client_kwargs: Additional kwargs for the client to be passed.\n\n    Returns:\n        tuple of the compressed RLE mask, its score, the source of the image, the image embeddings and the high\n        resolution features The features are returned if not provided in the arguments otherwise None is returned.\n    \"\"\"\n    if not isinstance(video, list):\n        raise ValueError(\"Video format not currently supported, please use sequence frames.\")\n    video_request = [sf.url if is_url(sf.url) else sf.open(media_dir, \"base64\") for sf in video]\n\n    if points is not None:\n        points_request = [points]\n    else:\n        points_request = None\n    if labels is not None:\n        labels_request = [labels]\n    else:\n        labels_request = None\n    if bbox is not None:\n        if bbox.is_normalized:\n            bbox = bbox.denormalize(height=video[0].height, width=video[0].width)\n        bbox_request = [[int(c) for c in bbox.xyxy_coords]]\n    else:\n        bbox_request = None\n\n    request = VideoMaskGenerationRequest(\n        video=video_request,\n        objects_ids=range(len(video)),\n        frame_indexes=range(len(video)),\n        boxes=bbox_request,\n        points=points_request,\n        labels=labels_request,\n        model=source.name,\n    )\n\n    response: VideoMaskGenerationResponse = await client.video_mask_generation(request, **client_kwargs)\n\n    inference_metadata = jsonable_encoder(\n        {\n            \"timestamp\": response.timestamp,\n            \"processing_time\": response.processing_time,\n            **response.metadata,\n        }\n    )\n\n    masks: list[CompressedRLE] = []\n    objects_ids: list[int] = []\n    frame_indexes: list[int] = []\n\n    if response.status == \"SUCCESS\":\n        output: VideoMaskGenerationOutput = response.data\n        entity_ref_id = shortuuid.uuid()  # used to check masks are from same generation when no entity in input\n\n        for o_mask, obj_id, frame_idx in zip(output.masks, output.objects_ids, output.frame_indexes):\n            image = video[frame_idx]\n            mask_inference: PixanoInferenceCompressedRLE = o_mask\n            mask = CompressedRLE(\n                id=shortuuid.uuid(),\n                item_ref=image.item_ref,\n                view_ref=ViewRef(name=image.table_name, id=image.id),\n                entity_ref=EntityRef(name=entity.table_name, id=entity.id)\n                if entity\n                else EntityRef(name=\"\", id=entity_ref_id),\n                source_ref=SourceRef(id=source.id),\n                inference_metadata=inference_metadata,\n                **mask_inference.model_dump(),\n            )\n            masks.append(mask)\n            objects_ids.append(obj_id)\n            frame_indexes.append(frame_idx)\n\n    return masks, objects_ids, frame_indexes\n</code></pre>"},{"location":"api_reference/inference/text_image_conditional_generation/","title":"text_image_conditional_generation","text":""},{"location":"api_reference/inference/text_image_conditional_generation/#pixano.inference.text_image_conditional_generation","title":"<code>pixano.inference.text_image_conditional_generation</code>","text":""},{"location":"api_reference/inference/text_image_conditional_generation/#pixano.inference.text_image_conditional_generation.messages_to_prompt","title":"<code>messages_to_prompt(dataset, conversation, messages, media_dir, role_system=DEFAULT_ROLE_SYSTEM, role_user=DEFAULT_ROLE_USER, role_assistant=DEFAULT_ROLE_ASSISTANT)</code>","text":"<p>Convert a list of messages to a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The Pixano dtaset.</p> required <code>conversation</code> <code>Conversation</code> <p>The conversation entity of the messages.</p> required <code>messages</code> <code>list[Message]</code> <p>List of messages.</p> required <code>media_dir</code> <code>Path</code> <p>The directory containing the images.</p> required <code>role_system</code> <code>str</code> <p>The role to use for the system.</p> <code>DEFAULT_ROLE_SYSTEM</code> <code>role_user</code> <code>str</code> <p>The role to use for the user.</p> <code>DEFAULT_ROLE_USER</code> <code>role_assistant</code> <code>str</code> <p>The role to use for the assistant.</p> <code>DEFAULT_ROLE_ASSISTANT</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of dictionaries representing the prompt.</p> Source code in <code>pixano/inference/text_image_conditional_generation.py</code> <pre><code>def messages_to_prompt(\n    dataset: Dataset,\n    conversation: Conversation,\n    messages: list[Message],\n    media_dir: Path,\n    role_system: str = DEFAULT_ROLE_SYSTEM,\n    role_user: str = DEFAULT_ROLE_USER,\n    role_assistant: str = DEFAULT_ROLE_ASSISTANT,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert a list of messages to a prompt.\n\n    Args:\n        dataset: The Pixano dtaset.\n        conversation: The conversation entity of the messages.\n        messages: List of messages.\n        media_dir: The directory containing the images.\n        role_system: The role to use for the system.\n        role_user: The role to use for the user.\n        role_assistant: The role to use for the assistant.\n\n    Returns:\n        List of dictionaries representing the prompt.\n    \"\"\"\n    prompt = []\n    for message in messages:\n        message_prompt: dict[str, Any] = {\"content\": []}\n        match message.type:\n            case \"SYSTEM\":\n                message_prompt[\"role\"] = role_system\n            case \"QUESTION\":\n                message_prompt[\"role\"] = role_user\n            case \"ANSWER\":\n                message_prompt[\"role\"] = role_assistant\n            case _:\n                raise ValueError(f\"Unknown message type {message.type}\")\n\n        ## add images to prompt\n        tables_view = sorted(dataset.schema.groups[SchemaGroup.VIEW])\n        for table_view in tables_view:\n            images = dataset.get_data(table_view, item_ids=[conversation.item_ref.id])\n            if len(images) &gt; 0:\n                image = images[0]  # there should be only one image per view, except for video (out of scope now)\n                message_prompt[\"content\"].append(\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image.url if is_url(image.url) else image.open(media_dir, \"base64\")},\n                    }\n                )\n\n        ## add objects (bbox) to prompt\n        tables_entities = sorted(dataset.schema.groups[SchemaGroup.ENTITY])\n        tables_bbox = [k for k, v in dataset.schema.schemas.items() if is_bbox(v)]\n        if len(tables_bbox) &gt; 0:\n            table_bbox = tables_bbox[0]  # assume there is only one bbox table\n            for table_entity in tables_entities:\n                entities = dataset.get_data(table_entity, item_ids=[conversation.item_ref.id])\n                for entity in entities:\n                    if not isinstance(entity, Conversation):\n                        bboxes = dataset.get_data(\n                            table_bbox, item_ids=[conversation.item_ref.id], where=f\"entity_ref.id == '{entity.id}'\"\n                        )\n                        if len(bboxes) == 0:\n                            continue\n                        bbox = bboxes[0]  # assume only one bbox per entity\n                        bbox_text = f\"a {'normalized ' if bbox.is_normalized else ''}bounding box {bbox.coords}\"\n                        if hasattr(entity, \"name\"):\n                            bbox_text = bbox_text + f\" with the name '{entity.name}'\"\n                        message_prompt[\"content\"].append(\n                            {\n                                \"type\": \"text\",\n                                \"text\": bbox_text,\n                            }\n                        )\n\n        message_prompt[\"content\"].append({\"type\": \"text\", \"text\": message.content})\n        prompt.append(message_prompt)\n    return prompt\n</code></pre>"},{"location":"api_reference/inference/text_image_conditional_generation/#pixano.inference.text_image_conditional_generation.text_image_conditional_generation","title":"<code>text_image_conditional_generation(client, source, dataset, media_dir, messages, conversation, max_new_tokens=DEFAULT_MAX_NEW_TOKENS, temperature=DEFAULT_TEMPERATURE, role_system=DEFAULT_ROLE_SYSTEM, role_user=DEFAULT_ROLE_USER, role_assistant=DEFAULT_ROLE_ASSISTANT, **client_kwargs)</code>  <code>async</code>","text":"<p>Generate text from an image using the Pixano Inference API.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>PixanoInferenceClient</code> <p>The Pixano-Inference client to use.</p> required <code>source</code> <code>Source</code> <p>The source refering to the model.</p> required <code>dataset</code> <code>Dataset</code> <p>The Pixano dataset.</p> required <code>media_dir</code> <code>Path</code> <p>The directory containing the input media files.</p> required <code>messages</code> <code>list[Message]</code> <p>A list of Message objects representing the input messages.</p> required <code>conversation</code> <code>Conversation</code> <p>The conversation entity of the messages.</p> required <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>DEFAULT_MAX_NEW_TOKENS</code> <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>DEFAULT_TEMPERATURE</code> <code>role_system</code> <code>str</code> <p>The role of the system in the prompt.</p> <code>DEFAULT_ROLE_SYSTEM</code> <code>role_user</code> <code>str</code> <p>The role of the user in the prompt.</p> <code>DEFAULT_ROLE_USER</code> <code>role_assistant</code> <code>str</code> <p>The role of the assistant in the prompt.</p> <code>DEFAULT_ROLE_ASSISTANT</code> <code>client_kwargs</code> <code>Any</code> <p>Additional kwargs for the client to be passed.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>The response message and its source.</p> Source code in <code>pixano/inference/text_image_conditional_generation.py</code> <pre><code>async def text_image_conditional_generation(\n    client: PixanoInferenceClient,\n    source: Source,\n    dataset: Dataset,\n    media_dir: Path,\n    messages: list[Message],\n    conversation: Conversation,\n    max_new_tokens: int = DEFAULT_MAX_NEW_TOKENS,\n    temperature: float = DEFAULT_TEMPERATURE,\n    role_system: str = DEFAULT_ROLE_SYSTEM,\n    role_user: str = DEFAULT_ROLE_USER,\n    role_assistant: str = DEFAULT_ROLE_ASSISTANT,\n    **client_kwargs: Any,\n) -&gt; Message:\n    \"\"\"Generate text from an image using the Pixano Inference API.\n\n    Args:\n        client: The Pixano-Inference client to use.\n        source: The source refering to the model.\n        dataset: The Pixano dataset.\n        media_dir: The directory containing the input media files.\n        messages: A list of Message objects representing the input messages.\n        conversation: The conversation entity of the messages.\n        max_new_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use for sampling.\n        role_system: The role of the system in the prompt.\n        role_user: The role of the user in the prompt.\n        role_assistant: The role of the assistant in the prompt.\n        client_kwargs: Additional kwargs for the client to be passed.\n\n    Returns:\n        The response message and its source.\n    \"\"\"\n    prompt = messages_to_prompt(\n        dataset=dataset,\n        conversation=conversation,\n        messages=messages,\n        media_dir=media_dir,\n        role_system=role_system,\n        role_user=role_user,\n        role_assistant=role_assistant,\n    )\n    last_message = messages[-1]\n    match last_message.type:\n        case \"SYSTEM\":\n            response_type = \"QUESTION\"\n            number = last_message.number + 1\n        case \"ANSWER\":\n            response_type = \"QUESTION\"\n            number = last_message.number + 1\n        case \"QUESTION\":\n            response_type = \"ANSWER\"\n            number = last_message.number\n        case _:\n            raise ValueError(f\"Invalid last message type {last_message.type}\")\n    request = TextImageConditionalGenerationRequest(\n        model=source.name,\n        prompt=prompt,\n        images=None,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n    )\n    response: TextImageConditionalGenerationResponse = await client.text_image_conditional_generation(\n        request, **client_kwargs\n    )\n\n    inference_metadata = jsonable_encoder(\n        {\n            \"generation_config\": response.data.generation_config,\n            \"usage\": response.data.usage,\n            \"timestamp\": response.timestamp,\n            \"processing_time\": response.processing_time,\n        }\n    )\n\n    message = Message(\n        id=shortuuid.uuid(),\n        item_ref=last_message.item_ref,\n        view_ref=last_message.view_ref,\n        entity_ref=EntityRef(id=conversation.id, name=conversation.table_name),\n        source_ref=SourceRef(id=source.id),\n        type=response_type,\n        content=response.data.generated_text,\n        number=number,\n        user=source.name,\n        timestamp=response.timestamp,\n        inference_metadata=inference_metadata,\n    )\n\n    return message\n</code></pre>"},{"location":"api_reference/inference/zero_shot_detection/","title":"zero_shot_detection","text":""},{"location":"api_reference/inference/zero_shot_detection/#pixano.inference.zero_shot_detection","title":"<code>pixano.inference.zero_shot_detection</code>","text":""},{"location":"api_reference/inference/zero_shot_detection/#pixano.inference.zero_shot_detection.image_zero_shot_detection","title":"<code>image_zero_shot_detection(client, media_dir, image, entity, source, classes, box_threshold=0.5, text_threshold=0.5, **client_kwargs)</code>  <code>async</code>","text":"<p>Image zero shot task.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>PixanoInferenceClient</code> <p>Pixano inference client.</p> required <code>media_dir</code> <code>Path</code> <p>Media directory.</p> required <code>image</code> <code>Image</code> <p>Image to generate mask for.</p> required <code>entity</code> <code>Entity</code> <p>Entity associated with the image.</p> required <code>source</code> <code>Source</code> <p>The source refering to the model.</p> required <code>classes</code> <code>list[str] | str</code> <p>List of classes to detect in the image.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for detection in the image.</p> <code>0.5</code> <code>text_threshold</code> <code>float</code> <p>Text threshold for detection in the image.</p> <code>0.5</code> <code>client_kwargs</code> <code>Any</code> <p>Additional kwargs for the client to be passed.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple[BBox, Classification]]</code> <p>List of BBoxes and Classifications detected in the image with respect to classes and threshold values.</p> Source code in <code>pixano/inference/zero_shot_detection.py</code> <pre><code>async def image_zero_shot_detection(\n    client: PixanoInferenceClient,\n    media_dir: Path,\n    image: Image,\n    entity: Entity,\n    source: Source,\n    classes: list[str] | str,\n    box_threshold: float = 0.5,\n    text_threshold: float = 0.5,\n    **client_kwargs: Any,\n) -&gt; list[tuple[BBox, Classification]]:\n    \"\"\"Image zero shot task.\n\n    Args:\n        client: Pixano inference client.\n        media_dir: Media directory.\n        image: Image to generate mask for.\n        entity: Entity associated with the image.\n        source: The source refering to the model.\n        classes: List of classes to detect in the image.\n        box_threshold: Box threshold for detection in the image.\n        text_threshold: Text threshold for detection in the image.\n        client_kwargs: Additional kwargs for the client to be passed.\n\n    Returns:\n        List of BBoxes and Classifications detected in the image with respect to classes and threshold values.\n    \"\"\"\n    image_request = image.url if is_url(image.url) else image.open(media_dir, \"base64\")\n\n    request = ImageZeroShotDetectionRequest(\n        image=image_request,\n        classes=classes,\n        box_threshold=box_threshold,\n        text_threshold=text_threshold,\n        model=source.name,\n    )\n\n    response: ImageZeroShotDetectionResponse = await client.image_zero_shot_detection(request, **client_kwargs)\n\n    inference_metadata = jsonable_encoder(\n        {\n            \"timestamp\": response.timestamp,\n            \"processing_time\": response.processing_time,\n            **response.metadata,\n        }\n    )\n\n    boxes = response.data.boxes\n    scores = response.data.scores\n    detected_classes = response.data.classes\n\n    output: list[tuple[BBox, Classification]] = []\n\n    for b, s, c in zip(boxes, scores, detected_classes, strict=True):\n        view_ref = ViewRef(name=image.table_name, id=image.id)\n        entity_ref = EntityRef(name=entity.table_name, id=entity.id)\n        source_ref = SourceRef(id=source.id)\n        output.append(\n            (\n                BBox(\n                    id=shortuuid.uuid(),\n                    item_ref=image.item_ref,\n                    view_ref=view_ref,\n                    entity_ref=entity_ref,\n                    source_ref=source_ref,\n                    inference_metadata=inference_metadata,\n                    coords=b,\n                    format=\"xyxy\",\n                    is_normalized=False,\n                    confidence=s,\n                ),\n                Classification(\n                    id=shortuuid.uuid(),\n                    item_ref=image.item_ref,\n                    view_ref=view_ref,\n                    entity_ref=entity_ref,\n                    source_ref=source_ref,\n                    inference_metadata=inference_metadata,\n                    labels=[c],\n                    confidences=[s],\n                ),\n            )\n        )\n\n    return output\n</code></pre>"},{"location":"api_reference/utils/python/","title":"python","text":""},{"location":"api_reference/utils/python/#pixano.utils.python","title":"<code>pixano.utils.python</code>","text":""},{"location":"api_reference/utils/python/#pixano.utils.python.estimate_folder_size","title":"<code>estimate_folder_size(folder_path)</code>","text":"<p>Estimate a folder size and return it as a human-readable string.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>Path</code> <p>Folder path.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Folder size as a human-readable string.</p> Source code in <code>pixano/utils/python.py</code> <pre><code>def estimate_folder_size(folder_path: Path) -&gt; str:\n    \"\"\"Estimate a folder size and return it as a human-readable string.\n\n    Args:\n        folder_path: Folder path.\n\n    Returns:\n        Folder size as a human-readable string.\n    \"\"\"\n    # Estimate size\n    total_size = 0.0\n    for dirpath, _, filenames in os.walk(folder_path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            # skip if it is symbolic link\n            if not os.path.islink(fp):\n                total_size += os.path.getsize(fp)\n\n    # Format size\n    i = 0\n    suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    while total_size &gt;= 1024 and i &lt; len(suffixes) - 1:\n        total_size /= 1024.0\n        i += 1\n    f = (f\"{total_size:.2f}\").rstrip(\"0\").rstrip(\".\")\n    readable_size = f\"{f} {suffixes[i]}\"\n\n    return readable_size\n</code></pre>"},{"location":"api_reference/utils/python/#pixano.utils.python.fn_sort_dict","title":"<code>fn_sort_dict(dict_, order_by, descending)</code>","text":"<p>Function to sort a dictionary by multiple keys in different orders.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>dict[str, Any]</code> <p>Dictionary to sort.</p> required <code>order_by</code> <code>list[str]</code> <p>List of keys to sort by.</p> required <code>descending</code> <code>list[bool]</code> <p>List of booleans indicating the order for each key.</p> required Source code in <code>pixano/utils/python.py</code> <pre><code>def fn_sort_dict(dict_: dict[str, Any], order_by: list[str], descending: list[bool]) -&gt; tuple[Any, ...]:\n    \"\"\"Function to sort a dictionary by multiple keys in different orders.\n\n    Args:\n        dict_: Dictionary to sort.\n        order_by: List of keys to sort by.\n        descending: List of booleans indicating the order for each key.\n    \"\"\"\n    key: list[Any] = []\n    for col, desc in zip(order_by, descending, strict=True):\n        value = dict_.get(col)\n        if desc:\n            if isinstance(value, (int, float)):\n                key.append(-value)\n            elif isinstance(value, str):\n                key.append(\"\".join(chr(255 - ord(c)) for c in value))\n            elif value is bool:\n                key.append(not value)\n            elif value is None:\n                key.append(None)\n            else:\n                raise ValueError(\n                    f\"Cannot sort by {type(value)} in descending order. \"\n                    \"Please use open an issue if you need this feature.\"\n                )\n        else:\n            key.append(value)\n    return tuple(key)\n</code></pre>"},{"location":"api_reference/utils/python/#pixano.utils.python.get_super_type_from_dict","title":"<code>get_super_type_from_dict(sub_type, dict_types)</code>","text":"<p>Get the first super type in a dictionary of types for the given type.</p> <p>Parameters:</p> Name Type Description Default <code>sub_type</code> <code>type</code> <p>Sub type to find the super type for.</p> required <code>dict_types</code> <code>dict[str, type]</code> <p>Dictionary of types.</p> required <p>Returns:</p> Type Description <code>type | None</code> <p>Super type if found, None otherwise.</p> Source code in <code>pixano/utils/python.py</code> <pre><code>def get_super_type_from_dict(sub_type: type, dict_types: dict[str, type]) -&gt; type | None:\n    \"\"\"Get the first super type in a dictionary of types for the given type.\n\n    Args:\n        sub_type: Sub type to find the super type for.\n        dict_types: Dictionary of types.\n\n    Returns:\n        Super type if found, None otherwise.\n    \"\"\"\n    if sub_type in dict_types.values():\n        return sub_type\n\n    sup_type = None\n    for dict_type in dict_types.values():\n        if issubclass(sub_type, dict_type):\n            sup_type = dict_type\n            break\n\n    if sup_type is None:\n        return None\n\n    found_type = True\n    while found_type:\n        found_type = False\n        for dict_type in dict_types.values():\n            if issubclass(sub_type, dict_type) and issubclass(dict_type, sup_type) and dict_type is not sup_type:\n                sup_type = dict_type\n                found_type = True\n                break\n\n    return sup_type\n</code></pre>"},{"location":"api_reference/utils/python/#pixano.utils.python.natural_key","title":"<code>natural_key(string)</code>","text":"<p>Return key for string natural sort.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>Input string.</p> required <p>Returns:</p> Type Description <code>list</code> <p>Sort key.</p> Source code in <code>pixano/utils/python.py</code> <pre><code>def natural_key(string: str) -&gt; list:\n    \"\"\"Return key for string natural sort.\n\n    Args:\n        string: Input string.\n\n    Returns:\n        Sort key.\n    \"\"\"\n    return [int(s) if s.isdecimal() else s for s in re.split(r\"(\\d+)\", string)]\n</code></pre>"},{"location":"api_reference/utils/python/#pixano.utils.python.to_sql_list","title":"<code>to_sql_list(ids)</code>","text":"<p>Convert a list of IDs to a SQL-friendly string.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>str | Sequence[str] | set[str]</code> <p>List of IDs.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL-friendly string of IDs.</p> Source code in <code>pixano/utils/python.py</code> <pre><code>def to_sql_list(ids: str | Sequence[str] | set[str]) -&gt; str:\n    \"\"\"Convert a list of IDs to a SQL-friendly string.\n\n    Args:\n        ids: List of IDs.\n\n    Returns:\n        SQL-friendly string of IDs.\n    \"\"\"\n    if isinstance(ids, str):\n        return f\"('{ids}')\"\n    elif len(ids) == 0:\n        raise ValueError(\"IDs must not be empty.\")\n    else:\n        for id in ids:\n            if not isinstance(id, str):\n                raise ValueError(\"IDs must be strings.\")\n    ids = list(dict.fromkeys(ids))  # Keep order and remove duplicates\n    if len(ids) == 1:\n        return f\"('{ids.pop()}')\"\n    return str(tuple(ids))\n</code></pre>"},{"location":"api_reference/utils/python/#pixano.utils.python.unique_list","title":"<code>unique_list(sequence)</code>","text":"<p>Select unique elements in a list while keeping order.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>Sequence[Any]</code> <p>Input sequence.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of unique elements.</p> Source code in <code>pixano/utils/python.py</code> <pre><code>def unique_list(sequence: Sequence[Any]) -&gt; list[Any]:\n    \"\"\"Select unique elements in a list while keeping order.\n\n    Args:\n        sequence: Input sequence.\n\n    Returns:\n        List of unique elements.\n    \"\"\"\n    return list(OrderedDict.fromkeys(sequence))\n</code></pre>"},{"location":"api_reference/utils/validation/","title":"validation","text":""},{"location":"api_reference/utils/validation/#pixano.utils.validation","title":"<code>pixano.utils.validation</code>","text":""},{"location":"api_reference/utils/validation/#pixano.utils.validation.issubclass_strict","title":"<code>issubclass_strict(obj, cls, strict=False)</code>","text":"<p>Check if the given object is of the given class type or a subclass of the given class type.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>type</code> <p>The object to check.</p> required <code>cls</code> <code>type</code> <p>The class to compare against.</p> required <code>strict</code> <code>bool</code> <p>If True, the object must be of the given class type.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the object is of the given class type or a subclass of the given class type.</p> Source code in <code>pixano/utils/validation.py</code> <pre><code>def issubclass_strict(obj: type, cls: type, strict: bool = False) -&gt; bool:\n    \"\"\"Check if the given object is of the given class type or a subclass of the given class type.\n\n    Args:\n        obj: The object to check.\n        cls: The class to compare against.\n        strict: If True, the object must be of the given class type.\n\n    Returns:\n        True if the object is of the given class type or a subclass of the given class type.\n    \"\"\"\n    if strict:\n        return obj == cls\n    return issubclass(obj, cls)\n</code></pre>"},{"location":"api_reference/utils/validation/#pixano.utils.validation.validate_and_init_create_at_and_update_at","title":"<code>validate_and_init_create_at_and_update_at(created_at, updated_at)</code>","text":"<p>Validate and initialize created_at and updated_at.</p> <p>The validation and initialization of created_at and updated_at is done as follows: - If created_at is None, it is set to the current date and time. - If updated_at is None, it is set to created_at. - If updated_at is not None and created_at is None, a ValueError is raised. - If updated_at is not None and created_at is not None, updated_at should be greater than created_at. - If created_at and updated_at are provided as strings, they are converted to datetime objects from ISO format.</p> <p>Parameters:</p> Name Type Description Default <code>created_at</code> <code>datetime | str | None</code> <p>The creation date of the object.</p> required <code>updated_at</code> <code>datetime | str | None</code> <p>The last modification date of the object.</p> required <p>Returns:</p> Type Description <code>tuple[datetime, datetime]</code> <p>A tuple containing the created_at and updated_at.</p> Source code in <code>pixano/utils/validation.py</code> <pre><code>def validate_and_init_create_at_and_update_at(\n    created_at: datetime | str | None, updated_at: datetime | str | None\n) -&gt; tuple[datetime, datetime]:\n    \"\"\"Validate and initialize created_at and updated_at.\n\n    The validation and initialization of created_at and updated_at is done as follows:\n    - If created_at is None, it is set to the current date and time.\n    - If updated_at is None, it is set to created_at.\n    - If updated_at is not None and created_at is None, a ValueError is raised.\n    - If updated_at is not None and created_at is not None, updated_at should be greater than created_at.\n    - If created_at and updated_at are provided as strings, they are converted to datetime objects from ISO format.\n\n    Args:\n        created_at: The creation date of the object.\n        updated_at: The last modification date of the object.\n\n    Returns:\n        A tuple containing the created_at and updated_at.\n    \"\"\"\n    if created_at is not None and not isinstance(created_at, datetime):\n        if isinstance(created_at, str):\n            created_at = datetime.fromisoformat(created_at)\n        else:\n            raise ValueError(\"created_at should be a datetime object or None.\")\n    if updated_at is not None and not isinstance(updated_at, datetime):\n        if isinstance(updated_at, str):\n            updated_at = datetime.fromisoformat(updated_at)\n        else:\n            raise ValueError(\"updated_at should be a datetime object or None.\")\n    if updated_at is not None and created_at is None:\n        raise ValueError(\"created_at should be set if updated_at is set.\")\n    elif created_at is not None:\n        if updated_at is not None:\n            if not isinstance(updated_at, datetime):\n                raise ValueError(\"updated_at should be a datetime object.\")\n            elif updated_at &lt; created_at:\n                raise ValueError(\"updated_at should be greater than created_at.\")\n        else:\n            updated_at = created_at\n    elif created_at is None:\n        created_at = datetime.now()\n        updated_at = created_at\n    return created_at, updated_at\n</code></pre>"},{"location":"getting_started/","title":"Getting started with Pixano","text":"<p>Pixano is an open-source data-centric AI tool for exploring and annotating computer vision datasets. It combines a Python/FastAPI backend with a Svelte/TypeScript frontend and stores data in the fast, columnar Lance format.</p> <ul> <li> Quickstart</li> </ul> <p>Import a dataset with pre-annotations and launch the app in minutes.</p> <p> Get started</p> <ul> <li> Installation</li> </ul> <p>All the ways to install Pixano: pip, Docker, or from source.</p> <p> Install Pixano</p> <ul> <li> Key concepts</li> </ul> <p>Understand the data model: items, views, entities, annotations, and more.</p> <p> Learn concepts</p> <ul> <li> Launching the app</li> </ul> <p>Start Pixano locally, from a notebook, with Docker, or connected to S3.</p> <p> Launch Pixano</p> <ul> <li> Using the app</li> </ul> <p>Navigate the web UI: browse datasets, annotate, and use AI-assisted tools.</p> <p> Explore the UI</p> <p>Once you are comfortable with the basics, head over to the Tutorials for deeper walkthroughs.</p>"},{"location":"getting_started/installing_pixano/","title":"Installation","text":""},{"location":"getting_started/installing_pixano/#pip-recommended","title":"pip (recommended)","text":"<p>We recommend installing Pixano in a virtual environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate   # Windows: .venv\\Scripts\\activate\npip install pixano\n</code></pre>"},{"location":"getting_started/installing_pixano/#docker","title":"Docker","text":"<p>Pixano is available on Docker Hub. Pull the stable version (latest release):</p> <pre><code>docker pull pixano/pixano:stable\n</code></pre> <p>See Launching the app for how to run the container.</p>"},{"location":"getting_started/installing_pixano/#from-source","title":"From source","text":"<p>Clone the repository and install with uv:</p> <pre><code>git clone https://github.com/pixano/pixano.git\ncd pixano\nuv sync\n</code></pre> <p>When running from source, prefix every command with <code>uv run</code> (e.g. <code>uv run pixano server run ./my_data</code>).</p>"},{"location":"getting_started/key_concepts/","title":"Key concepts","text":""},{"location":"getting_started/key_concepts/#what-pixano-does","title":"What Pixano does","text":"<p>Pixano provides two main capabilities:</p> <ol> <li>Dataset management in a fast, columnar data format based on the Lance format and the LanceDB vector database.</li> <li>An annotation web platform powered by AI.</li> </ol>"},{"location":"getting_started/key_concepts/#architecture","title":"Architecture","text":"<p>Pixano has three layers:</p> <ul> <li>The backend manages datasets: creation, insertions, deletions, updates, and statistical computations.</li> <li>The REST API handles HTTP requests and translates them into backend operations.</li> <li>The UI allows users to visualize data and interact with it to add, update, or delete annotations and entities.</li> </ul>"},{"location":"getting_started/key_concepts/#schema-groups","title":"Schema groups","text":"<p>Every dataset is composed of tables grouped by purpose. Pixano defines six schema groups:</p> Group Description Example Item One row per dataset entry (metadata, split, ...) <code>Item</code> View Media attached to an item <code>Image</code>, <code>Video</code> Entity Objects or tracks within an item <code>Entity</code> Annotation Labels attached to entities <code>BBox</code>, <code>KeyPoints</code>, <code>CompressedRLE</code> Embedding Vectors for semantic search <code>Embedding</code> Source Provenance of annotations/predictions <code>Source</code> <p>Each group has a base class (e.g. <code>Item</code>, <code>View</code>, <code>Entity</code>, <code>Annotation</code>) that you can subclass to add domain-specific fields:</p> <pre><code>from pixano.features import Entity\n\nclass DetectedObject(Entity):\n    category: str = \"\"\n</code></pre>"},{"location":"getting_started/key_concepts/#library-media-and-models","title":"Library, media, and models","text":"<p>Pixano organizes data around three directories:</p> <ul> <li>Library -- contains all datasets (each dataset is a LanceDB database). The path is provided at launch time.</li> <li>Media -- holds images, videos, and other files referenced by views. Also provided at launch time.</li> <li>Models -- optional directory for model files used by AI-assisted features (e.g. SAM for interactive segmentation).</li> </ul>"},{"location":"getting_started/key_concepts/#on-disk-layout","title":"On-disk layout","text":"<p>Each dataset is a directory inside the library with this structure:</p> <pre><code>&lt;dataset&gt;/\n  info.json               # name, description, workspace type\n  schema.json             # tables, fields, relations, groups\n  features_values.json    # value constraints for fields (optional)\n  stats.json              # dataset statistics (optional)\n  preview.png             # preview image (optional)\n  db/                     # LanceDB tables (one per schema)\n</code></pre> <p>Media files live in the separate media directory, and views reference them by relative URL.</p>"},{"location":"getting_started/key_concepts/#common-python-operations","title":"Common Python operations","text":"<pre><code>from pathlib import Path\nfrom pixano.datasets import Dataset\n\n# Open a dataset\nds = Dataset(Path(\"./library/my_dataset\"), media_dir=Path(\"./media\"))\n\n# Read items\nitems = ds.get_dataset_items(limit=20, skip=0)\n\n# Read a specific table\nimages = ds.get_data(\"image\", limit=5)\n\n# Add annotations\nds.add_data(\"bboxes\", [bbox1, bbox2])\n\n# Semantic search (requires precomputed embeddings)\ntop_items, distances, ids = ds.semantic_search(\"car on road\", \"image_embedding\", limit=50)\n</code></pre> <p>For a full walkthrough, see the Build and query a dataset tutorial.</p>"},{"location":"getting_started/launching_the_app/","title":"Launching Pixano","text":""},{"location":"getting_started/launching_the_app/#from-a-terminal-locally","title":"From a terminal: locally","text":"<p>You can start the Pixano app with the following command:</p> <pre><code>pixano server run your_data_dir/\n</code></pre> <p>This expects the data directory to contain <code>library/</code>, <code>media/</code>, and optionally <code>models/</code> subdirectories. Use <code>pixano init</code> to create this structure automatically (see the Quickstart).</p> <p>You will then be provided with a URL to open in your browser to use the app.</p>"},{"location":"getting_started/launching_the_app/#from-a-terminal-s3-experimental","title":"From a terminal: S3 (Experimental)","text":"<p>You can connect to an S3-compatible storage by providing S3 paths and credentials as options:</p> <ul> <li><code>--aws-endpoint</code>: S3 endpoint URL, use 'AWS' if not provided.</li> <li><code>--aws-region</code>: S3 region name, not always required for private storages.</li> <li><code>--aws-access-key</code>: S3 AWS access key.</li> <li><code>--aws-secret-key</code>: S3 AWS secret key.</li> </ul> <p>So the command becomes:</p> <pre><code>pixano server run ./my_data \\\n--aws-endpoint=\"https://your-aws-endpoint.com\" \\\n--aws-region=\"\" \\\n--aws-access-key=\"your_access_key\" \\\n--aws-secret-key=\"your_secret_key\"\n</code></pre>"},{"location":"getting_started/launching_the_app/#from-a-notebook","title":"From a notebook","text":"<p>If you are in a Jupyter or Google Colab notebook, you can start the app by running the following cells:</p> <pre><code>from pixano.app import App\napp = App(\"your_data_dir/\")\n</code></pre> <p>You can then use the apps directly from the notebook in another cell with:</p> <pre><code>app.display()\n</code></pre>"},{"location":"getting_started/launching_the_app/#from-docker","title":"From Docker","text":"<p>To launch the app you have to mount a data directory that contains <code>library/</code>, <code>media/</code>, and optionally <code>models/</code> subdirectories.</p> <p>Here is an example:</p> <pre><code>docker run -d \\\n    --name pixano \\\n    -p 7492:7492 \\\n    -v ./my_data:/app/data \\\n    pixano/pixano:stable\n</code></pre>"},{"location":"getting_started/quickstart/","title":"Quickstart: Import a Dataset with Pre-annotations","text":"<p>This guide walks you through importing an annotated dataset into Pixano in three steps: init your data directory, import your dataset, and serve the application. By the end you will have a running Pixano instance displaying images with bounding-box pre-annotations. See Installing Pixano for setup instructions.</p>"},{"location":"getting_started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ installed</li> <li>Pixano installed:</li> </ul> <pre><code>pip install pixano\n</code></pre> Running from source with uv <p>Clone the repository and install with uv:</p> <pre><code>git clone https://github.com/pixano/pixano.git\ncd pixano\nuv sync\n</code></pre> <p>When running from source, prefix every command with <code>uv run</code> (e.g. <code>uv run pixano init ./my_data</code>).</p>"},{"location":"getting_started/quickstart/#step-1-initialize-your-data-directory","title":"Step 1 \u2014 Initialize Your Data Directory","text":"<p>Create the directory structure Pixano needs to store datasets, media files, and models:</p> <pre><code>pixano init ./my_data\n</code></pre> <p>This creates the following tree:</p> <pre><code>my_data/\n  library/   # LanceDB dataset storage\n  media/     # Imported media files (images, videos)\n  models/    # Model files for inference\n</code></pre>"},{"location":"getting_started/quickstart/#step-2-prepare-and-import-your-dataset","title":"Step 2 \u2014 Prepare and Import Your Dataset","text":"<p>Pixano imports datasets from a source directory organized by splits (e.g. <code>train/</code>, <code>val/</code>). Each split folder contains the media files and an optional <code>metadata.jsonl</code> file with pre-annotations.</p>"},{"location":"getting_started/quickstart/#object-detection","title":"Object Detection","text":"<p>You have images with bounding-box annotations and want to import them into Pixano.</p> <p>Organize your source folder</p> <pre><code>street_objects/\n  train/\n    img_001.jpg\n    img_002.jpg\n    img_003.jpg\n    metadata.jsonl\n  val/\n    img_004.jpg\n    img_005.jpg\n</code></pre> <p>Two rules:</p> <ul> <li>Each subfolder is a split.</li> <li>A <code>metadata.jsonl</code> file inside a split folder provides pre-annotations for that split. Splits without one (like <code>val/</code> above) import images with no pre-annotations.</li> </ul> <p>Define your schema</p> <p>The base <code>Entity</code> class has no domain-specific attributes, so you need a custom schema to attach fields like <code>category</code> to your objects. Save this file as <code>schema.py</code>:</p> <pre><code># schema.py\nfrom pixano.features import Entity\nfrom pixano.datasets.workspaces import DefaultImageDatasetItem\n\n\nclass DetectedObject(Entity):\n    category: str = \"\"\n\n\nclass StreetObjectsDatasetItem(DefaultImageDatasetItem):\n    objects: list[DetectedObject]  # overrides default Entity table\n    image_source: str = \"\"         # item-level metadata\n</code></pre> <p>Extending <code>Entity</code> lets you add custom attributes to each detected object. Extending <code>DefaultImageDatasetItem</code> gives you the standard image tables out of the box \u2014 you only need to override what you want to customize.</p> What's in the default schema? <p><code>DefaultImageDatasetItem</code> provides the following attributes. Fields you don't override are included automatically.</p> Attribute Type Stored in <code>image</code> <code>Image</code> <code>image</code> table (view) <code>objects</code> <code>list[Entity]</code> <code>objects</code> table (entities) <code>bboxes</code> <code>list[BBox]</code> <code>bboxes</code> table (annotations) <code>masks</code> <code>list[CompressedRLE]</code> <code>masks</code> table (annotations) <code>keypoints</code> <code>list[KeyPoints]</code> <code>keypoints</code> table (annotations) <p>In the example above, <code>objects</code> is overridden with <code>list[DetectedObject]</code> to add the <code>category</code> field. The <code>image_source</code> field does not correspond to any schema group, so it is stored as item-level metadata in the <code>item</code> table.</p> <p>Write the metadata.jsonl</p> <p>Create a <code>metadata.jsonl</code> file in each split folder that has pre-annotations. Each line is a JSON object describing one image:</p> <p><code>street_objects/train/metadata.jsonl</code></p> <pre><code>{\"image\": \"img_001.jpg\", \"image_source\": \"dashcam\", \"objects\": {\"bboxes\": [[0.12, 0.25, 0.15, 0.30], [0.55, 0.10, 0.20, 0.45]], \"category\": [\"car\", \"pedestrian\"]}}\n{\"image\": \"img_002.jpg\", \"image_source\": \"dashcam\", \"objects\": {\"bboxes\": [[0.30, 0.40, 0.08, 0.12]], \"category\": [\"bicycle\"]}}\n{\"image\": \"img_003.jpg\", \"image_source\": \"drone\"}\n</code></pre> <p>Format rules:</p> <ul> <li><code>image</code> matches the view field name in the schema and points to the image file in the same folder.</li> <li><code>image_source</code> is stored as item-level metadata (it does not match any schema table name).</li> <li><code>objects</code> matches the entity field name in the schema. Its value is a dict containing entity attributes and annotation data.</li> <li><code>bboxes</code> inside <code>objects</code> matches the annotation table name. Each bounding box is <code>[x, y, w, h]</code>.</li> <li>Entity attributes (<code>category</code>) are parallel arrays \u2014 one value per bounding box.</li> <li>Items without <code>objects</code> (like <code>img_003.jpg</code>) have no pre-annotations.</li> <li>Coordinates are auto-detected as normalized when all values fall within [0, 1].</li> </ul> Format details <p>The <code>metadata.jsonl</code> format mirrors the schema structure:</p> <ul> <li>Top-level keys that match a view field (e.g. <code>image</code>) are treated as view references \u2014 the value is the filename relative to the split folder.</li> <li>Top-level keys that match an entity field (e.g. <code>objects</code>) contain a nested dict. Keys inside this dict that match annotation table names (e.g. <code>bboxes</code>) are parsed as annotation data. All other keys are treated as entity attributes.</li> <li>Top-level keys that don't match any view or entity field (e.g. <code>image_source</code>) are stored as item-level metadata.</li> <li>Entity attributes and annotation arrays must have the same length \u2014 each index corresponds to one object.</li> </ul> <p>Run the import</p> <pre><code>pixano data import ./my_data ./street_objects \\\n    --name \"Street Objects\" \\\n    --schema ./schema.py:StreetObjectsDatasetItem\n</code></pre> <p>Common options:</p> Option Description <code>--name</code> Dataset name. Defaults to the source directory name. <code>--type</code> Dataset type: <code>image</code> (default), <code>video</code>, or <code>vqa</code>. <code>--mode</code> <code>create</code> (default, fails if exists), <code>overwrite</code>, or <code>add</code> (append). <code>--schema</code> Custom schema as <code>path/to/file.py:ClassName</code>. Uses the default schema if omitted. Alternative: build the dataset with Python <p>You can also build the dataset programmatically:</p> <pre><code>from pathlib import Path\nfrom pixano.datasets import DatasetInfo\nfrom pixano.datasets.builders import ImageFolderBuilder\n\nbuilder = ImageFolderBuilder(\n    media_dir=Path(\"./my_data/media\"),\n    library_dir=Path(\"./my_data/library\"),\n    dataset_item=StreetObjectsDatasetItem,\n    info=DatasetInfo(\n        name=\"Street Objects\",\n        description=\"Street scene images with object detection pre-annotations\",\n    ),\n    dataset_path=\"street_objects\",\n)\n\ndataset = builder.build(mode=\"create\")\nprint(f\"Dataset built: {dataset.num_rows} items\")\n</code></pre>"},{"location":"getting_started/quickstart/#try-it-pascal-voc-2007","title":"Try it: Pascal VOC 2007","text":"<p>The repository includes a ready-to-run example that downloads a sample of the Pascal VOC 2007 dataset and produces a Pixano-compatible folder. Use it to test the full import workflow with real data.</p> <p>1. Generate the sample folder</p> <pre><code>pip install pillow                        # required by the script\npython examples/voc/generate_sample.py ./voc_sample --num-samples 50\n</code></pre> <p>This downloads VOC 2007, samples 50 images per split, converts the XML annotations to <code>metadata.jsonl</code> files with normalized bounding boxes, and writes everything to <code>./voc_sample/</code>.</p> <p>The resulting folder looks like:</p> <pre><code>voc_sample/\n  train/\n    000032.jpg\n    000045.jpg\n    ...\n    metadata.jsonl\n  validation/\n    000007.jpg\n    000019.jpg\n    ...\n    metadata.jsonl\n</code></pre> <p>Each line in <code>metadata.jsonl</code> follows the format described above:</p> <pre><code>{\"image\": \"000032.jpg\", \"objects\": {\"bboxes\": [[0.078, 0.090, 0.756, 0.792], ...], \"category\": [\"aeroplane\", ...], \"is_difficult\": [false, ...]}}\n</code></pre> <p>2. Review the schema</p> <p>The example schema is at <code>examples/voc/schema.py</code>:</p> <pre><code>from pixano.datasets.workspaces import DefaultImageDatasetItem\nfrom pixano.features import Entity\n\n\nclass VOCObject(Entity):\n    category: str = \"\"\n    is_difficult: bool = False\n\n\nclass VOCDatasetItem(DefaultImageDatasetItem):\n    objects: list[VOCObject]\n</code></pre> <p>Same pattern as the street-objects example \u2014 a custom <code>Entity</code> subclass with domain-specific attributes, plugged into <code>DefaultImageDatasetItem</code>.</p> <p>3. Import and serve</p> <pre><code>pixano init ./my_data\npixano data import ./my_data ./voc_sample \\\n    --name \"VOC 2007 Sample\" \\\n    --schema examples/voc/schema.py:VOCDatasetItem\npixano server run ./my_data\n</code></pre> <p>Open <code>http://127.0.0.1:7492</code> to browse the imported VOC images and bounding boxes.</p>"},{"location":"getting_started/quickstart/#visual-question-answering","title":"Visual Question Answering","text":"<p>You have images with question-answer pairs and want to import them into Pixano.</p> <p>Organize your source folder</p> <pre><code>vqa_data/\n  train/\n    image_001.jpg\n    image_002.jpg\n    metadata.jsonl\n  val/\n    image_003.jpg\n    metadata.jsonl\n</code></pre> <p>Same rules as object detection: each subfolder is a split, and a <code>metadata.jsonl</code> provides the annotations.</p> <p>Write the metadata.jsonl</p> <p>For VQA datasets, each line in <code>metadata.jsonl</code> describes one image and its associated question-answer conversations:</p> <p><code>vqa_data/train/metadata.jsonl</code></p> <pre><code>{\"image\": \"image_001.jpg\", \"conversations\": [{\"question\": {\"content\": \"What color is the car?\", \"question_type\": \"OPEN\"}, \"responses\": [{\"content\": \"red\", \"user\": \"annotator_0\"}, {\"content\": \"dark red\", \"user\": \"annotator_1\"}]}, {\"question\": {\"content\": \"How many people are visible?\", \"question_type\": \"OPEN\"}, \"responses\": [{\"content\": \"3\", \"user\": \"annotator_0\"}]}]}\n{\"image\": \"image_002.jpg\", \"conversations\": [{\"question\": {\"content\": \"Is it raining?\", \"question_type\": \"OPEN\"}, \"responses\": [{\"content\": \"no\", \"user\": \"annotator_0\"}]}]}\n</code></pre> <p>Format rules:</p> <ul> <li><code>image</code> points to the image file in the same folder.</li> <li><code>conversations</code> is a list of question-answer exchanges for the image.</li> <li>Each conversation has a <code>question</code> with <code>content</code> (the question text) and <code>question_type</code> (<code>\"OPEN\"</code> for free-form answers).</li> <li><code>responses</code> is a list of answers. Each response has <code>content</code> (the answer text) and an optional <code>user</code> field to distinguish annotators.</li> <li>An image can have multiple conversations (multiple questions about the same image).</li> </ul> Multiple-choice questions <p>For multiple-choice questions, set <code>question_type</code> to <code>\"multi_choice\"</code> and add a <code>choices</code> list:</p> <pre><code>{\"question\": {\"content\": \"What is in the image?\", \"question_type\": \"multi_choice\", \"choices\": [\"a cat\", \"a dog\", \"a bird\", \"a fish\"]}, \"responses\": [{\"content\": \"a cat\"}]}\n</code></pre> <p>Run the import</p> <pre><code>pixano data import ./my_data ./vqa_data \\\n    --name \"VQA Data\" \\\n    --type vqa\n</code></pre> <p>The <code>--type vqa</code> flag tells Pixano to use the VQA folder builder, which parses the <code>conversations</code> format. No custom schema is required \u2014 the default <code>DefaultVQADatasetItem</code> schema handles images, conversations, messages, objects, bounding boxes, masks, and keypoints.</p> Alternative: build the dataset with Python <p>You can also build the dataset programmatically:</p> <pre><code>from pathlib import Path\nfrom pixano.datasets import DatasetInfo\nfrom pixano.datasets.builders import VQAFolderBuilder\nfrom pixano.datasets.workspaces import DefaultVQADatasetItem\n\nbuilder = VQAFolderBuilder(\n    media_dir=Path(\"./my_data/media\"),\n    library_dir=Path(\"./my_data/library\"),\n    dataset_item=DefaultVQADatasetItem,\n    info=DatasetInfo(\n        name=\"VQA Data\",\n        description=\"VQA dataset with question-answer pairs\",\n    ),\n    dataset_path=\"vqa_data\",\n)\n\ndataset = builder.build(mode=\"create\")\nprint(f\"Dataset built: {dataset.num_rows} items\")\n</code></pre>"},{"location":"getting_started/quickstart/#try-it-vqav2","title":"Try it: VQAv2","text":"<p>The repository includes a ready-to-run example that downloads a sample of a small VQAv2 subset from HuggingFace and produces a Pixano-compatible folder. It also demonstrates how to combine VQA with object annotation using a custom schema. Use it to test the full VQA import workflow with real data.</p> <p>1. Generate the sample folder</p> <pre><code>pip install datasets pillow              # required by the script\npython examples/vqav2/generate_sample.py ./vqav2_sample --num-samples 50\n</code></pre> <p>This downloads a small VQAv2 subset, samples 50 images with their questions, and writes everything to <code>./vqav2_sample/</code>.</p> <p>The resulting folder looks like:</p> <pre><code>vqav2_sample/\n  validation/\n    000000.jpg\n    000001.jpg\n    ...\n    metadata.jsonl\n</code></pre> <p>Each line in <code>metadata.jsonl</code> describes one image with its question and answer:</p> <pre><code>{\n  \"image\": \"000000.jpg\",\n  \"conversations\": [\n    {\n      \"question\": {\n        \"content\": \"Where are the kids riding?\",\n        \"question_type\": \"OPEN\"\n      },\n      \"responses\": [{ \"content\": \"carnival ride\" }]\n    }\n  ]\n}\n</code></pre> <p>2. Review the schema</p> <p>The example uses a custom schema at <code>examples/vqav2/schema.py</code> to add object annotation support alongside the VQA task:</p> <pre><code>from pixano.datasets.workspaces import DefaultVQADatasetItem\nfrom pixano.features import Entity\n\n\nclass ObjectEntity(Entity):\n    \"\"\"Custom entity for object annotation with category and occlusion info.\"\"\"\n\n    category: str = \"\"\n    subcategory: str = \"\"\n    is_occluded: bool = False\n\n\nclass VQAv2DatasetItem(DefaultVQADatasetItem):\n    \"\"\"Dataset item for VQAv2 with object annotation support.\"\"\"\n\n    objects: list[ObjectEntity]\n</code></pre> <p>Same pattern as the VOC example \u2014 a custom <code>Entity</code> subclass with domain-specific attributes, plugged into <code>DefaultVQADatasetItem</code>.</p> <p>3. Import and serve</p> <pre><code>pixano init ./my_data\npixano data import ./my_data ./vqav2_sample \\\n    --name \"VQAv2 Sample\" \\\n    --type vqa \\\n    --schema examples/vqav2/schema.py:VQAv2DatasetItem\npixano server run ./my_data\n</code></pre> <p>Open <code>http://127.0.0.1:7492</code> to browse the imported VQA images, questions, and answers.</p>"},{"location":"getting_started/quickstart/#step-3-launch-the-server","title":"Step 3 \u2014 Launch the Server","text":"<pre><code>pixano server run ./my_data\n</code></pre> <p>Open <code>http://127.0.0.1:7492</code> in your browser to explore and annotate your dataset.</p> <ul> <li><code>--host</code> \u2014 bind address (e.g. <code>0.0.0.0</code> for network access). Default: <code>127.0.0.1</code>.</li> <li><code>--port</code> \u2014 port number. Default: <code>7492</code>.</li> </ul>"},{"location":"getting_started/quickstart/#explore-your-data","title":"Explore Your Data","text":"<p>The web UI lets you browse items, view annotations, and start annotating. You can also interact with your data programmatically.</p> Explore the REST API <p>While the server is running, interactive API documentation is available at:</p> <ul> <li>Swagger UI: <code>http://127.0.0.1:7492/docs</code></li> <li>ReDoc: <code>http://127.0.0.1:7492/redoc</code></li> </ul> <p>List all datasets and find the dataset ID:</p> <pre><code>curl -s http://localhost:7492/datasets/info | python -m json.tool\n</code></pre> <p>The response includes an <code>id</code> field for each dataset. Use this ID in subsequent requests (referred to as <code>&lt;DATASET_ID&gt;</code> below).</p> <p>Get items (first 3):</p> <pre><code>curl -s 'http://localhost:7492/items/&lt;DATASET_ID&gt;?limit=3' | python -m json.tool\n</code></pre> <p>Get image views:</p> <pre><code>curl -s 'http://localhost:7492/views/&lt;DATASET_ID&gt;/image?limit=2' | python -m json.tool\n</code></pre> <p>Get entities (objects with category):</p> <pre><code>curl -s 'http://localhost:7492/entities/&lt;DATASET_ID&gt;/objects?limit=5' | python -m json.tool\n</code></pre> <p>Get bounding boxes for a specific item:</p> <pre><code>curl -s 'http://localhost:7492/annotations/&lt;DATASET_ID&gt;/bboxes?item_ids=&lt;ITEM_ID&gt;' | python -m json.tool\n</code></pre> <p>Browse dataset (same endpoint the UI uses):</p> <pre><code>curl -s 'http://localhost:7492/browser/&lt;DATASET_ID&gt;?limit=10' | python -m json.tool\n</code></pre> Verify with the Python API <p>Load the dataset and inspect items to confirm the import worked:</p> <pre><code>from pathlib import Path\nfrom pixano.datasets import Dataset\n\ndataset = Dataset(Path(\"./my_data/library/street_objects\"), media_dir=Path(\"./my_data/media\"))\nitems = dataset.get_dataset_items(limit=3)\nfor item in items:\n    print(f\"Item {item.id} | split={item.split}\")\n</code></pre>"},{"location":"getting_started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Build and query a dataset \u2014 deeper dive into dataset creation and querying</li> <li>Pre-annotation \u2014 more pre-annotation formats and workflows</li> <li>Semantic search \u2014 search your dataset using embeddings</li> </ul>"},{"location":"getting_started/using_the_app/","title":"Using Pixano","text":""},{"location":"getting_started/using_the_app/#home-page","title":"Home page","text":"<p>From the app home page, you will be greeted with a list of all the Pixano format datasets found in the directory you provided.</p> <p></p>"},{"location":"getting_started/using_the_app/#header","title":"Header","text":"<p>On the header is displayed the number of datasets and total number of items in datasets. A field allows to filter datasets by contained text.</p>"},{"location":"getting_started/using_the_app/#dataset-card","title":"Dataset Card","text":"<p>Each dataset card display dataset name, number of items, an image (currently user-defined), and the dataset kind (Image, Video, VQA or EntityLinking), if defined.</p> <p>When hovering over a dataset card, a tooltip will display this dataset specific information: name, description, maximum number of views, total number of entities, as well as a count of each different annotations.</p> <p>Clicking on a dataset card will lead to the selected dataset page.</p>"},{"location":"getting_started/using_the_app/#dataset-page","title":"Dataset page","text":"<p>On the dataset page, you will see a list of all the items it contains, by pages of 20 items.</p> <p></p>"},{"location":"getting_started/using_the_app/#header_1","title":"Header","text":"<p>On the header is displayed the dataset name. Pixano logo allows to go back to Pixano home page. \"Dashboard\" and \"Dataset\" buttons allows to switch between Dashboard and Dataset view. Datasets are always open in \"Dataset\" mode first.</p>"},{"location":"getting_started/using_the_app/#column-filter","title":"Column filter","text":"<p>You can filter table by choosing a column and a value. the right button clear the filter.</p> <p>Note: \"Free mode\" allows to type your filter, as a basic SQL WHERE clause. Only listed columns are availables. Strings must be enclosed in single quotes in free mode.</p> <p>Free mode example: <code>id LIKE '%009' AND split = 'train2017'</code> will filter rows where id ends with 009 and split equals 'train2017'.</p>"},{"location":"getting_started/using_the_app/#semantic-search","title":"Semantic search","text":"<p>If you have computed semantic embeddings, semantic search will be displayed. You can select semantic embeddings (stored in a table) and type some text to get your dataset items sorted by semantic distance to this text (closer first).</p>"},{"location":"getting_started/using_the_app/#item-table","title":"Item table","text":"<p>Table header display columns name. By default, the \"images\" columns are first, then dataset items metadatas, then computed counts columns (with a leading '#'). Except images columns, you can order items of any column, ascending or descending, by clicking on it.</p> <p>This ordering is disabled after a semantic search.</p> <p>The \"gear\" icon open a panel to select which columns to display, and reorder them. By default all available columns are displayed.</p>"},{"location":"getting_started/using_the_app/#navigation","title":"Navigation","text":"<p>Navigation buttons at bottom allows to move through the pages of your dataset.</p> <p>Clicking on any item lead to Item page.</p>"},{"location":"getting_started/using_the_app/#dashboard-page","title":"Dashboard page","text":"<p>From the dataset page, you can go to the dashboard page, which contains more information about your datasets and also displays all the computed statistics available.</p>"},{"location":"getting_started/using_the_app/#item-page-common","title":"Item page - Common","text":"<p>When opening an item, the item media will be displayed in the center on the screen. For multi-view datasets, the images will be tiled.</p> <p>On the right, a two-tab panel, Object or Scene, is displayed.</p> <p>For Video datasets, there will be a timeline display of tracks.</p> <p>For VQA and EntityLinking datasets, there will be a text panel on the left.</p> <p>In the following sub-sections we will cover only common subjects for all kind of dataset. Specifics are in their respectives sections.</p> <p>You can resize panels by dragging the thin border pink line between them.</p>"},{"location":"getting_started/using_the_app/#header_2","title":"Header","text":"<p>The \"Home\" icon return to home page.</p> <p>The circled left arrow go back to dataset explorer page, at the current item's page.</p> <p>Dataset item id is displayed next.</p> <p>Left and right arrows allows to go to previous/next item, in the current selection and order. Shortcuts, detailed in tooltip, are available too.</p> <p>Next, is displayed the index of current item in the full selection, ordered.</p> <p>The toolbar is at the center.</p> <p>\"Save\" button on the right. It has a pulsing dot when there is changes to save.</p>"},{"location":"getting_started/using_the_app/#scene-panel","title":"Scene panel","text":"<p>The scene panel will display all the scene features, like the item split, or any other feature created when importing your dataset, as well as metadata information on all the images in the item.</p> <p>You can edit the scene features and then click the save changes button to write them to the dataset.</p> <p>There is also a set of image filters and settings. They allow to disable / enable image smoothing, equalize the image histogram, and tune brightness, contrast, or RGB channels.</p>"},{"location":"getting_started/using_the_app/#object-panel","title":"Object panel","text":"<p>The objects panel will display all the item objects.</p> <p>On the header line just under tabs, a global visibility icon can hide/show all objects. On the right, an \"options\" icon, and the number of objects is displayed. If some filter apply, this the number of filtered object / total number of objects.</p>"},{"location":"getting_started/using_the_app/#options","title":"Options","text":"<p>By clicking on \"Options\" icon, the option panel opens.</p> <p></p> <p>Here you can set a threshold for bounding boxes confidence.</p> <p>In video mode, disable / enable interpolation of bounding boxes and keypoints.</p> <p>Filters are available. You can add new filter wit 'AND\" or \"OR\" buttons, and clear the filters with the \"clear\" icon. For each filter, you have to select a table, a field, an operator, and a value. Note that \"AND\" has precedence over \"OR\". Click on \"Filter\" button to apply.</p>"},{"location":"getting_started/using_the_app/#object-card","title":"Object card","text":"<p>For each object in the dataset item, an object card is displayed.</p> <p>An \"Eye\" icon allows to toggle visibility.</p> <p>A colored dot show the color used to display this object. Clicking on it select it. In Video mode, it will change current frame to the first visible instance.</p> <p>Next is the 'name' (in fact the first available feature amongst \"name\", \"category\", \"category_name\"), as well as the object id (parenthized if there is a 'name').</p> <p>A \"Trash\" icon, when opened or on hover, allows to delete the object. It requires another click within 3 seconds to confirm.</p> <p>In Video mode, there is a \"Hide track\"/\"Show track\" icon. It toggle the track display in Video Inspector</p> <p>The right most \"chevron\" icon open the details.</p> <p>When opened, there is at most 4 sub-sections: Features, Objects, Thumbnails and Text spans. Some are displayed only if relevant.</p> <ul> <li> <p>\"Features\" display the object features. Clicking on the \"Edit\" icon allows to edit them.   Each features is displayed with it's table name in brackets (in some cases features are from different sub-objects).</p> </li> <li> <p>\"Object\" display the sub-objects, somehow the shapes belonging to that object, or entity.   For video, there can be another layer as shapes are under tracklets. Under tracklet, interpolated shapes do not offer interaction here as this is not real objects of your dataset. See Track Item for other interactions on tracks and tracklets.</p> </li> </ul> <p>For each sub object, here also an \"Eye\" icon to toggle visibility, then an icon that represent the kind of sub-object (bounding box, mask, keypoints, text, or tracklet). Hovering on it show the actual class. Next is it's id. On the right, an \"Edit\" icon, if relevant, allows to modify the shape. A \"Link\" icon allows to relink this shape in another object (entity). And finally a \"Trash\" icon to delete it.</p> <ul> <li> <p>\"Thumbnails\" shows a crop of the object, if relevant. Note: It is not meant to be the exact bounding box, if any, but as a quick look on the object.</p> </li> <li> <p>\"Text spans\" is only for EntityLinking datasets, and show a table associating text span features to the actual text span (the highlighted text).</p> </li> </ul>"},{"location":"getting_started/using_the_app/#toolbar","title":"Toolbar","text":""},{"location":"getting_started/using_the_app/#pan-tool","title":"Pan tool","text":"<p>With the pan tool selected, you can move the image around. This is especially useful for multi-view datasets for organizing multiple images.</p> <p>Moving the images is still possible while any other tools is selected by using your mouse middle click. You can also zoom in and out of an image with the mouse wheel, and double click an image to bring it in front of the others.</p> <p>Note: if you want to select image or an object under another object, you can select the front one and hide it with visibility icon on right panel. Then you can go through the hidden one.</p>"},{"location":"getting_started/using_the_app/#creation-tools","title":"Creation tools","text":"<p>Each of theses tools allows creation of a new shape.</p> <p>There is 3 manual creation tools, and the interactive segmentation tool.</p> <p>For each tool, when the shape is drawn, you will be prompted to enter values for your object features depending on your dataset, and to confirm the object.</p> <p></p> <p>The dropbox \"Select parent Entity\" let choose if the object is a new object, or if it belongs to an existing one.</p> <p>Below, each relevant feature for this shape is listed, with an input to enter value. If Features Values are defined, choices may be proposed.</p>"},{"location":"getting_started/using_the_app/#bounding-box-tool","title":"Bounding box tool","text":"<p>With the bounding box tool, you can create a bounding box object by click and dragging a rectangle over the image.</p>"},{"location":"getting_started/using_the_app/#polygon-tool","title":"Polygon tool","text":"<p>With the polygon tool, you can create a segmentation mask manually by adding points with the granularity of your choice.</p>"},{"location":"getting_started/using_the_app/#keypoints-tool","title":"Keypoints tool","text":"<p>With the keypoints tool, you can create a keypoints. First select one of the keypoints templates, then drag a box to roughly place the template. Now you can place each keypoint by dragging them.</p> <p>Once created, by editing keypoints shape, you can change a keypoint status (visible, hidden, invisible) by clicking on it.</p>"},{"location":"getting_started/using_the_app/#smart-segmentation-tool","title":"Smart segmentation tool","text":"<p>With Pixano, you can segment with smart segmentation tool like SAM (Segment Anything Model).</p> <p>The first time you click on the \"magicwand\" icon, the Smart Model Selection modal will open, to configure the segmentation model to use. See more information on Smart Model Selection for configuration.</p> <p>With the positive and negative points, you can inform the interactive segmentation tool on which part of the image you are trying to segment, and it will generate the mask.</p> <p>When relevant, you can also use the rectangle tool to select the thing you want to segment.</p>"},{"location":"getting_started/using_the_app/#associate-tool","title":"Associate tool","text":"<p>In Video mode only, a \"associate\" icon allows to merge tracks.</p> <p></p> <p>In this mode, select a first object in any view, then other objects to associate. Clicking again on a object deselect it. Object that cannot be selected, because of current selection, are greyed out. You can move in the timeline, select in other view or other kind of shapes. Validate when done. All selected objects are merged in the first one of the selected list.</p>"},{"location":"getting_started/using_the_app/#item-page-video-dataset","title":"Item page - Video Dataset","text":"<p>In this mode, a specific \"associate\" tool is available in the toolbar.</p>"},{"location":"getting_started/using_the_app/#video-inspector","title":"Video Inspector","text":"<p>Video Inspector panel is displayed in the bottom.</p> <p></p> <p>Here you have the timeline, with each track, and a control panel at the bottom.</p>"},{"location":"getting_started/using_the_app/#timeline","title":"Timeline","text":"<p>The time axis, in seconds. You can change (click or drag) the current time of the video.</p>"},{"location":"getting_started/using_the_app/#track-item","title":"Track item","text":"<p>Upon each track is displayed a colored dot and 'name' (id) (same as in Object Card)</p> <p>Then, all tracklets are shown as lines. Horizontal level correspond to each view, for multiview dataset. If the track, or tracklet, is selected, and the timeline zoom is enough (depends on frame density), keyframes are shown too. Keyframes are \"real\" shapes. On frames between keyframes, inside a traclet, the shape is interpolated (bounding boxes and keypoints).</p> <p>A left-click on a tracklet change the current time.</p> <p>A right click on a tracklet select it and open the tracklet popup menu. Depending on context, different options are available:</p> <ul> <li>Add point: Replace interpolated shape by a \"real\" shape.</li> <li>Split: Cut the tracklet in two tracklets, between previous and next keypoints.</li> <li>Glue left/right: Glue tracklet to the left or right tracklet.</li> <li>Relink: change tracklet owner (move or merge it in another track)</li> <li>Delete: Delete tracklet, and all that it contains.</li> </ul> <p>A right click on a keyframe open the keyframe popup menu. Depending on context, different options are available:</p> <ul> <li>Remove item: Remove the item.</li> <li>Edit item: Edit the item.</li> </ul> <p>Note: There may be different shapes under the same keyframe (ex: a bounding box and a keypoints). Use Object Card for more precise control.</p>"},{"location":"getting_started/using_the_app/#control-panel","title":"Control panel","text":"<p>\"Play\", \"Step backward\" and \"Step forward\" icons allows the corresponding actions. There are keyboard shortcuts described in tooltip for theses. \"Play\" will read the video to the end once.</p> <p>The time (format: minutes:second.millisecond) is diplayed next, then the current frame index in parenthesis. On the right, a slider allows to zoom the timeline.</p>"},{"location":"getting_started/using_the_app/#item-page-vqa-dataset","title":"Item page - VQA Dataset","text":"<p>On the left is the VQA panel list.</p> <p></p> <p>On the top, the VQA model settings.</p> <p>Under, a list of question and answers.</p>"},{"location":"getting_started/using_the_app/#vqa-model-settings","title":"VQA model settings","text":"<p>The \"sparkling star\" icon, allows to connect a Pixano Inference model provider. It is red if Pixano Inference is not connected, orange if connected but no suitable model found, and green if a suitable model is ready. The dropdown list let choose the model, the \"+\" icon allows to instantiate a new model in Pixano Inference provider. See Select Smart Model. The \"gear\" icon allows to set some model prompt settings and model temperature.</p>"},{"location":"getting_started/using_the_app/#questions-and-answers","title":"Questions and answers","text":"<p>The \"Add question\" button open the QA Editor, where you can choose a question type and type the question, or generate it if a model is up and ready.</p> <p>For each question, there is a \"check\" icon, red if no answer, green if an answer is present. Then question number, and on the right another \"sparling star\" icon to generate the answer. You also can manually enter an answer, and correct it. Multiple answers are allowed.</p>"},{"location":"getting_started/using_the_app/#item-page-entitylinking-dataset","title":"Item page - EntityLinking Dataset","text":"<p>An the left is the entity linking text panel.</p> <p>Text spans are background colored, using the object color. Text span can be clicked to select the object.</p> <p>In the text area, if you select some text and click \"Tag Selected Text\" button, the creation panel will open to validate this new text span.</p>"},{"location":"getting_started/using_the_app/#select-smart-model","title":"Select Smart Model","text":"<p>Here you can choose a model for segmentation. You can choose either a local model (which requires precomputed embeddings and a local model, see Interactive segmentation), or a segmentation model provided by Pixano Inference.</p> <p>The \"sparkling star\" icon, allows to connect a Pixano Inference model provider. It is red if Pixano Inference is not connected, orange if connected but no suitable model found, and green if a suitable model is ready.</p> <p>\"Add a model\" button open the Instantiate model page (for either segmentation or VQA, depending on context)</p> <p>For more information on the settings to provide, please refer to Pixano Inference.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In this section, we provide guides to cover Pixano functionalities.</p> <ul> <li> Build and query a dataset</li> </ul> <p>Build a Pixano dataset and query it using the Python API.</p> <p> Dataset tutorial</p> <ul> <li> Semantic search</li> </ul> <p>Use models like OpenCLIP to browse your data efficiently with semantic search.</p> <p> Semantic search</p> <ul> <li> Interactive segmentation</li> </ul> <p>Use models like SAM to annotate your data efficiently with interactive segmentation.</p> <p> Interactive segmentation</p> <ul> <li> Pre-annotation</li> </ul> <p>Use detection models like YOLO or GroundingDINO to pre-annotate your data.</p> <p> Pre-annotation</p>"},{"location":"tutorials/dataset/","title":"Build and query a dataset","text":"<p>New to Pixano?</p> <p>If you just want to get a dataset running quickly, start with the Quickstart guide.</p>"},{"location":"tutorials/dataset/#context","title":"Context","text":"<p>In this tutorial, we will build a library consisting of one dataset from a folder dataset stored in the <code>./assets/health_images/</code> folder with a unique subfolder <code>all/</code> which will later be considered as a split.</p> <p>It contains 10 images of human parts from several image sources (MRI, microscope, and high-resolution photos). A <code>metadata.jsonl</code> also provides annotations, bounding boxes and keypoints, associated to these images.</p>"},{"location":"tutorials/dataset/#build-your-dataset","title":"Build your dataset","text":""},{"location":"tutorials/dataset/#describe-your-dataset","title":"Describe your dataset","text":"<p>To create a Pixano <code>Dataset</code>, a <code>DatasetBuilder</code> needs a pythonic description of the dataset item based on a Pydantic's BaseModel.</p> <p>To do so, Pixano provides the <code>pixano.datasets.DatasetItem</code> class that already has the attributes <code>id</code> and <code>split</code> to uniquely identify an item and categorize the items respectively.</p> <p>To define a custom <code>DatasetItem</code>, simply create a sublcass of <code>DatasetItem</code> that define all the features of your dataset.</p> <p>In our case we have:</p> <ul> <li><code>image_type</code>: a string metadata.</li> <li><code>image</code>: the unique view of the item that is an Image.</li> <li><code>objects</code>: the entites of an item. Think of it as a common identifier for multiple annotations.</li> <li><code>bboxes</code>: the bounding boxes of an item.</li> <li><code>masks</code>: the masks of an item.</li> <li><code>keypoints</code>: the keypoints of an item.</li> </ul> <p>This is how it can be defined in code:</p> <pre><code>from pixano.datasets import DatasetItem\nfrom pixano.features import BBox, CompressedRLE, Entity, Image, KeyPoints\n\nclass EntityWithCategory(Entity):\n    category: str\n\nclass HealthDatasetItem(DatasetItem):\n    image: Image\n    objects: list[EntityWithCategory]\n    bboxes: list[BBox]\n    masks: list[CompressedRLE]\n    keypoints: list[KeyPoints]\n    image_type: str\n</code></pre> <p>Notice that when multiple elements are attached to an item we use the <code>list</code> type.</p> <p>Another possibility is to use a predefined <code>DatasetItem</code> for image datasets:</p> <pre><code>class DefaultImageDatasetItem(DatasetItem):\n    \"\"\"Default Image DatasetItem Schema.\"\"\"\n\n    image: Image\n    objects: list[Entity]\n    bboxes: list[BBox]\n    masks: list[CompressedRLE]\n    keypoints: list[KeyPoints]\n</code></pre> <p>This is the one used by <code>ImageFolderBuilder</code> if no schema is provided.</p> <p>We can override it to add or modify attributes, as shown in the following example, which is functionally equivalent:</p> <pre><code>class EntityWithCategory(Entity):\n    category: str\n\nclass HealthDatasetItem(DefaultImageDatasetItem):\n    objects: list[EntityWithCategory]\n    image_type: str\n</code></pre>"},{"location":"tutorials/dataset/#use-one-of-the-provided-datasetbuilder","title":"Use one of the provided DatasetBuilder","text":"<p>The Health dataset follows a folder structure that can be handled by Pixano:</p> <pre><code>root_folder/\n    split1/\n        view0.ext\n        view1.ext\n        ...\n        viewN.ext\n        metadata.jsonl\n    split2/\n        ...\n</code></pre> <p>Therefore the <code>pixano.datasets.builders.ImageFolderBuilder</code> can be used to construct the Pixano dataset as follows:</p> <pre><code>from pathlib import Path\nfrom pixano.datasets import DatasetInfo\nfrom pixano.datasets.builders import ImageFolderBuilder\n\n\nbuilder = ImageFolderBuilder(\n    media_dir=Path(\"./assets\"),\n    library_dir=Path(\"./pixano_library\"),\n    dataset_item=HealthDatasetItem,\n    info=DatasetInfo(\n        name=\"Health Images\",\n        description=\"A dataset of health images\",\n    ),\n    dataset_path=\"/health_images\"\n)\n\ndataset = builder.build(mode=\"create\")\n</code></pre> <p><code>media_dir</code> and <code>library_dir</code> should be the same you provide to launch Pixano.</p> <p>CLI alternative</p> <p>You can also import a dataset from the command line. The CLI copies your source directory into the data directory automatically:</p> <pre><code>pixano data import ./my_data ./health_images \\\n    --name \"Health Images\" --schema ./schema.py:HealthDatasetItem\n</code></pre> <p>See the quickstart guide for more details.</p>"},{"location":"tutorials/dataset/#write-your-own-datasetbuilder","title":"Write your own DatasetBuilder","text":"<p>While it is convenient to use built-in dataset builders, Pixano do not cover all your use-cases. Fortunatly it is possible to design your own builder by subclassing the <code>pixano.datasets.builders.DatasetBuilder</code> class and implementing the <code>generate_data</code> method.</p> <p>Here is roughly the code for a custom <code>HealthFolderBuilder</code>. It is a simplified version of FolderBuilder.</p> <pre><code>from collections import defaultdict\nfrom pathlib import Path\nfrom typing import Iterator\n\nfrom pixano.datasets.dataset_info import DatasetInfo\nfrom pixano.features import (\n    Annotation,\n    BaseSchema,\n    Entity,\n    Image,\n    SourceKind,\n)\n\n\nclass TestFolderBuilder(ImageFolderBuilder):\n\n    def generate_data(\n        self,\n    ) -&gt; Iterator[dict[str, BaseSchema | list[BaseSchema]]]:\n        self.source_id = self.add_source(\"Builder\", SourceKind.OTHER)\n        for split in self.source_dir.glob(\"*\"):\n            if not split.is_dir() or split.name.startswith(\".\"):\n                continue\n            try:\n                dataset_pieces = self._read_metadata(split / self.METADATA_FILENAME)\n            except FileNotFoundError:\n                raise ValueError(f\"Metadata not found in {str(split)}\")\n\n            for i, dataset_piece in enumerate(dataset_pieces):\n                # split metadata in different kind\n                item_metadata = {}\n                view_metadata = {}\n                obj_metadata = {}\n                for k in dataset_piece.keys():\n                    if k in self.views_schema:\n                        view_metadata.update({k: dataset_piece.get(k, None)})\n                    elif k in self.entities_schema:\n                        obj_metadata.update({k: dataset_piece.get(k, None)})\n                    else:\n                        item_metadata.update({k: dataset_piece.get(k, None)})\n\n                # create item\n                item = self._create_item(split.name, **item_metadata)\n\n                # create view\n                views_data: list[tuple[str, Image]] = []\n                for view_name, im in view_metadata.items():\n                    # in case split is or is not in given filename\n                    view_file = self.source_dir / Path(im) if split.name == Path(im).parts[0] else split / Path(im)\n                    if view_file.is_file() and view_file.suffix in self.EXTENSIONS:\n                        view = self._create_view(item, view_file, Image)\n                        views_data.append((view_name, view))\n\n                # create entities and annotations\n                all_entities_data: dict[str, list[Entity]] = defaultdict(list)\n                all_annotations_data: dict[str, list[Annotation]] = defaultdict(list)\n                for k, v in obj_metadata.items():\n                    if k in self.entities_schema and v is not None:\n                        entity_name = k\n                        raw_entities_data = v\n                        entity_schema = self.entities_schema.get(entity_name)\n                        if entity_schema is not None:\n                            entities_data, annotations_data = self._create_objects_entities(\n                                item, views_data, entity_name, entity_schema, raw_entities_data\n                            )\n\n                            for name, entities in entities_data.items():\n                                all_entities_data[name].extend(entities)\n\n                            for name, annotations in annotations_data.items():\n                                all_annotations_data[name].extend(annotations)\n\n                yield {self.item_schema_name: item}\n                for view_name, view in views_data:\n                    yield {view_name: view}\n\n                if all_entities_data is None:\n                    continue\n\n                yield all_entities_data\n                yield all_annotations_data\n\n\nbuilder = TestFolderBuilder(\n    media_dir=media_dir,\n    library_dir=library_dir,\n    dataset_item=CustomDatasetItem,\n    info=DatasetInfo(\n        name=dataset_name,\n        description=dataset_description\n    ),\n    dataset_path=dataset_dirname,\n)\n\nbuilder.build(mode=\"overwrite\")\n</code></pre> <p>We recommend that you take a look at the implementation of the class <code>pixano.datasets.builders.FolderBaseBuilder</code> for the complete code to understand how to construct your own builder.</p> <p>Notice that the <code>generate_data</code> is a generator of dictionaries whose keys are the names of the tables to fill and the values one example or a list of <code>pixano.features.BaseSchema</code> to fill these tables. The builder flushes data by chunks of a size configured for every table with the argument <code>flush_every_n_samples</code> in the <code>build</code> method. This offers a trade-off between speed and memory footprint.</p>"},{"location":"tutorials/dataset/#query-your-dataset","title":"Query your dataset","text":""},{"location":"tutorials/dataset/#use-the-python-api","title":"Use the Python API","text":"<p>To interact with your dataset you can use CRUD (create, read, update, and delete) operations either on a table level or on a complete dataset item with the following methods:</p> operation table dataset item create add_data add_dataset_items read get_data get_dataset_items update update_data update_dataset_items delete delete_data delete_dataset_items <p>Here is an example to read the first two image views:</p> <pre><code>from pixano.datasets import Dataset\n\ndataset = Dataset( # Load the dataset\n    Path(\"./pixano_library/health_dataset\"), media_dir=Path(\"./assets/\")\n)\n\ndataset.get_data(\"image\", limit=2, skip=0) # Fetch images\n\n&gt;&gt;&gt; [Image(id='QmmM6LhwB4uMMCRUjgXejY', created_at=datetime.datetime(2024, 11, 7, 10, 14, 5, 364059), updated_at=datetime.datetime(2024, 11, 7, 10, 14, 5, 364059), item_ref=ItemRef(name='item', id='iyA4tHmGeHPP4N6diSuUXi'), parent_ref=ViewRef(name='', id=''), url='/health_images/all/microcope-red_blood_cells.jpg', width=2560, height=1920, format='JPEG'), Image(id='SNgW9Zk68g7mBW2CuHQQKZ', created_at=datetime.datetime(2024, 11, 7, 10, 14, 5, 367359), updated_at=datetime.datetime(2024, 11, 7, 10, 14, 5, 367359), item_ref=ItemRef(name='item', id='VkBcn4bhgt2RRJNWWjvDN5'), parent_ref=ViewRef(name='', id=''), url='/health_images/all/microscope-peau.jpg', width=896, height=550, format='JPEG')]\n</code></pre> <p>And here is an example to read the dataset item whose id is <code>'iyA4tHmGeHPP4N6diSuUXi'</code>. Beware that because they are generated randomly you won't have the same id in your dataset.</p> <pre><code>dataset.get_dataset_items(\"iyA4tHmGeHPP4N6diSuUXi\")\n\n&gt;&gt;&gt; DatasetItem(id='iyA4tHmGeHPP4N6diSuUXi', split='all', created_at=datetime.datetime(2024, 11, 7, 10, 14, 5, 363831), updated_at=datetime.datetime(2024, 11, 7, 10, 14, 5, 363831), image=Image(id='QmmM6LhwB4uMMCRUjgXejY', created_at=datetime.datetime(2024, 11, 7, 10, 14, 5, 364059), updated_at=datetime.datetime(2024, 11, 7, 10, 14, 5, 364059), item_ref=ItemRef(name='item', id='iyA4tHmGeHPP4N6diSuUXi'), parent_ref=ViewRef(name='', id=''), url='/health_images/all/microcope-red_blood_cells.jpg', width=2560, height=1920, format='JPEG'), objects=[], bbox=[], keypoints=[], image_type='microcope')\n</code></pre>"},{"location":"tutorials/dataset/#use-the-rest-api","title":"Use the REST API","text":"<p>To use the REST API, we first need to launch the Pixano's app.</p> <pre><code>pixano server run ./my_data --host localhost --port 7492\n</code></pre> <p>Then we can call the endpoints of the REST API. To have the complete list of endpoints, we can have take a look at the Swagger docs http://localhost:7492/docs or look at the relevant documentation.</p> <p>To fetch the first two image views here is the call:</p> <pre><code>curl -X 'GET' \\\n  'http://localhost:7492/views/health_images/image/?limit=2&amp;skip=0' \\\n  -H 'accept: application/json'\n\n&gt;&gt;&gt; [\n  {\n    \"id\": \"QmmM6LhwB4uMMCRUjgXejY\",\n    \"created_at\": \"2024-11-07T10:14:05.364059\",\n    \"updated_at\": \"2024-11-07T10:14:05.364059\",\n    \"table_info\": {\n      \"name\": \"image\",\n      \"group\": \"views\",\n      \"base_schema\": \"Image\"\n    },\n    \"data\": {\n      \"item_ref\": {\n        \"name\": \"item\",\n        \"id\": \"iyA4tHmGeHPP4N6diSuUXi\"\n      },\n      \"parent_ref\": {\n        \"name\": \"\",\n        \"id\": \"\"\n      },\n      \"url\": \"/health_images/all/microcope-red_blood_cells.jpg\",\n      \"width\": 2560,\n      \"height\": 1920,\n      \"format\": \"JPEG\"\n    }\n  },\n  {\n    \"id\": \"SNgW9Zk68g7mBW2CuHQQKZ\",\n    \"created_at\": \"2024-11-07T10:14:05.367359\",\n    \"updated_at\": \"2024-11-07T10:14:05.367359\",\n    \"table_info\": {\n      \"name\": \"image\",\n      \"group\": \"views\",\n      \"base_schema\": \"Image\"\n    },\n    \"data\": {\n      \"item_ref\": {\n        \"name\": \"item\",\n        \"id\": \"VkBcn4bhgt2RRJNWWjvDN5\"\n      },\n      \"parent_ref\": {\n        \"name\": \"\",\n        \"id\": \"\"\n      },\n      \"url\": \"/health_images/all/microscope-peau.jpg\",\n      \"width\": 896,\n      \"height\": 550,\n      \"format\": \"JPEG\"\n    }\n  }\n]\n</code></pre>"},{"location":"tutorials/interactive_segmentation/","title":"Interactive segmentation","text":""},{"location":"tutorials/interactive_segmentation/#context","title":"Context","text":"<p>SAM (Segment Anything Model) is an open-source model proposed by Meta to perform mask segmentation from boxes, keypoints and/or original masks.</p> <p>Pixano's web app integrates SAM to quickly annotate your images. It first requires to pre-compute the embeddings of the images.</p> <p>This tutorial will help you unlock this feature.</p>"},{"location":"tutorials/interactive_segmentation/#create-the-image-embeddings","title":"Create the image embeddings","text":""},{"location":"tutorials/interactive_segmentation/#install-the-requirements","title":"Install the requirements","text":"<ol> <li>Pip dependencies</li> </ol> <p>Install the official SAM repo, <code>onnx</code> to export the model and <code>transformers</code> to get the image embeddings.</p> <pre><code>pip install git+https://github.com/facebookresearch/segment-anything.git\npip install onnx transformers\n</code></pre> <ol> <li>Download the model and export it to ONNX format.</li> </ol> <pre><code>git clone https://github.com/facebookresearch/segment-anything.git\n\ncd segment-anything\n\nwget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n\npython segment-anything/scripts/export_onnx_model.py \\\n    --checkpoint sam_vit_h_4b8939.pth \\\n    --model-type vit_h \\\n    --output sam_h.onnx\n\ncp sam_h.onnx /path/to/pixano/models/\n# Defaults is models/ under the library\n</code></pre>"},{"location":"tutorials/interactive_segmentation/#create-the-embeddings","title":"Create the embeddings","text":"<p>We will use the Health Images dataset defined in the Build and query a dataset tutorial.</p> <ol> <li>Load the model and the dataset.</li> </ol> <pre><code>import torch\nfrom transformers import SamModel, SamProcessor\nfrom pixano.datasets import Dataset\nfrom pixano.features import Image\nfrom pathlib import Path\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device=device)\nprocessor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n\ndataset = Dataset(\n    Path(\"./pixano_library/health_dataset\"),\n    media_dir=Path(\"./assets/\")\n)\n\nimages: list[Image] = dataset.get_data(\"image\")\nnum_images  = len(images)\n\nprint(num_images)\n\n&gt;&gt;&gt; 11\n</code></pre> <ol> <li>Create the SAM embeddings table.</li> </ol> <pre><code>from pixano.features import ViewEmbedding\nfrom pixano.datasets.dataset_schema import SchemaRelation\nfrom lancedb.pydantic import Vector\n\nclass SAMViewEmbedding(ViewEmbedding):\n    vector: Vector(1048576)\n\nsam_table = dataset.create_table(\n    name=\"sam_embedding\",\n    schema=SAMViewEmbedding,\n    relation_item=SchemaRelation.ONE_TO_ONE,\n    mode=\"overwrite\"\n)\n</code></pre> <ol> <li>Compute the embeddings</li> </ol> <pre><code>import shortuuid\nfrom pixano.features import ViewRef\n\nembeddings = []\nfor i, image in enumerate(images):\n    pil_image = image.open( # Load the actual image\n            media_dir=dataset.media_dir,\n            output_type=\"image\"\n        ).convert(\"RGB\")\n    with torch.inference_mode():\n        # Compute the embeddings\n        inputs = processor(pil_image, return_tensors=\"pt\").to(device=device)\n        output = model.get_image_embeddings(inputs[\"pixel_values\"])\n    # Validate the output\n    embedding = dataset.schema.schemas[\"sam_embedding\"](\n        id=shortuuid.uuid(),\n        item_ref=image.item_ref,\n        view_ref=ViewRef(id=image.id, name=image.table_name),\n        vector=output.flatten().tolist(),\n        shape=output.squeeze().shape,\n    )\n    embeddings.append(embedding)\n\n# Flush to the table\ndataset.add_data(\"sam_embedding\", embeddings)\n</code></pre>"},{"location":"tutorials/interactive_segmentation/#use-the-interactive-segmentation","title":"Use the interactive segmentation","text":""},{"location":"tutorials/interactive_segmentation/#with-the-app","title":"With the app","text":"<p>Now you are all set to use SAM, check the \"Smart segmentation tool\" section of the using the app guide!</p>"},{"location":"tutorials/pre_annotation/","title":"Pre Annotation","text":""},{"location":"tutorials/pre_annotation/#context","title":"Context","text":"<p>It is common to use a object detection model to pre-annotate a dataset.</p> <p>This tutorial will help you unlock this feature.</p>"},{"location":"tutorials/pre_annotation/#shared-utility","title":"Shared utility","text":"<p>Both examples below use the same helper function to create a Pixano <code>BBox</code> and its associated <code>Entity</code>. It assumes your dataset has been created with the <code>EntityWithCategory</code> custom Entity and one of the defaults provided in <code>pixano.datasets.workspaces</code>.</p> <p>You can customize it to match your own dataset.</p> <pre><code>from pixano.features import BBox, Entity\nimport shortuuid\n\nclass EntityWithCategory(Entity):\n    category: str\n\ndef create_pixano_bbox_entity(\n    pix_image, bbox_coords, score, category, *,\n    is_normalized=True, source_id=\"source\",\n):\n    view_ref = {\"id\": pix_image.id, \"name\": \"image\"}\n    entity = EntityWithCategory(\n        id=shortuuid.uuid(),\n        item_ref=pix_image.item_ref,\n        view_ref=view_ref,\n        category=category,\n    )\n    bbox = BBox(\n        id=shortuuid.uuid(),\n        item_ref=pix_image.item_ref,\n        view_ref=view_ref,\n        entity_ref={\"id\": entity.id, \"name\": \"objects\"},\n        confidence=score,\n        coords=bbox_coords,\n        is_normalized=is_normalized,\n        format=\"xyxy\",\n        source_ref={\"id\": source_id, \"name\": \"source\"},\n    )\n\n    return entity, bbox\n</code></pre>"},{"location":"tutorials/pre_annotation/#using-yolov11-from-ultralytics","title":"Using YOLOv11 from Ultralytics","text":"<p>In your environnement, install ultralytics</p> <pre><code>pip install ultralytics\n</code></pre> <p>Set your paths</p> <pre><code>from pathlib import Path\n\nlibrary = Path(\"/path_to_pixano_library_dir\")\nmedia = Path(\"/path_to_pixano_media_dir\")\ndataset_dirname = \"pixano_dataset_directory_name\"  # as in library directory\n</code></pre> <p>Load your Pixano dataset</p> <pre><code>from pixano.datasets import Dataset\nds = Dataset(library / dataset_dirname, media_dir=media)\n\n# Add YOLO source\nif not ds.get_data(\"source\", ids=\"src_yolo\"):\n    ds.add_data(\"source\", [Source(id=\"src_yolo\", name=\"yolo11n\", kind=\"model\")])\n</code></pre> <p>Load YOLOv11 model.</p> <pre><code>from ultralytics import YOLO\n\n# Load a COCO-pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n</code></pre> <p>Pre-annotate</p> <pre><code>from tqdm.auto import tqdm\n\nnew_entities = []\nnew_bboxes = []\n\nimages = ds.get_data(\"image\")\n\nfor image in tqdm(images):\n    results = model.predict(media / image.url, verbose=False)\n    for res in results:\n        for bbox, score, category in zip(\n            res.boxes.xyxyn.tolist(),\n            res.boxes.conf.tolist(),\n            res.boxes.cls.tolist(),\n        ):\n            entity, pix_bbox = create_pixano_bbox_entity(\n                image, bbox, score, res.names[category],\n                is_normalized=True, source_id=\"src_yolo\",\n            )\n            new_entities.append(entity)\n            new_bboxes.append(pix_bbox)\n\nds.add_data(\"objects\", new_entities)\nds.add_data(\"bboxes\", new_bboxes)\n\nprint(\"Done\")\n</code></pre>"},{"location":"tutorials/pre_annotation/#using-grounding-dino-from-idea-research-with-pixano-inference","title":"Using Grounding DINO from IDEA-Research with Pixano Inference","text":"<p>We can use Pixano Inference client as a convenient way to access models, as long as they have a registered inference provider supported by Pixano Inference.</p> <p>Set your paths</p> <pre><code>from pathlib import Path\n\nlibrary = Path(\"/path_to_pixano_library_dir\")\nmedia = Path(\"/path_to_pixano_media_dir\")\ndataset_dirname = \"pixano_dataset_directory_name\"  # as in library directory\n</code></pre> <p>Load your Pixano dataset</p> <pre><code>from pixano.datasets import Dataset\nds = Dataset(library / dataset_dirname, media_dir=media)\n\n# Add GroundingDINO source\nif not ds.get_data(\"source\", ids=\"src_gdino\"):\n    ds.add_data(\"source\", [Source(id=\"src_gdino\", name=\"GroundingDINO\", kind=\"model\")])\n</code></pre> <p>Load Grounding DINO model with Pixano Inference from transformers provider (HuggingFace)</p> <pre><code>from pixano_inference.providers.transformers import TransformersProvider\nfrom pixano_inference.tasks import ImageTask\nimport torch\n\nprovider = TransformersProvider()\nmodel = provider.load_model(\n    \"dino\",\n    ImageTask.ZERO_SHOT_DETECTION.value,\n    torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\",\n    \"IDEA-Research/grounding-dino-base\"\n)\n</code></pre> <p>Pre-annotate. We ask Grounding DINO to detect objects of classes [\"person\", \"car\", \"motorcycle\"].</p> <pre><code>from pixano_inference.pydantic import ImageZeroShotDetectionOutput\nfrom pixano_inference.utils.media import convert_string_to_image\nfrom tqdm.auto import tqdm\n\nnew_entities = []\nnew_bboxes = []\n\nimages = ds.get_data(\"image\")\n\nfor image in tqdm(images):\n    image_zero_shot_detection_out: ImageZeroShotDetectionOutput = (\n        model.image_zero_shot_detection(\n            image=convert_string_to_image(media / image.url),\n            classes=[\"person\", \"car\", \"motorcycle\"],\n            box_threshold=0.3,\n            text_threshold=0.2,\n        )\n    )\n    for bbox, score, category in zip(\n        image_zero_shot_detection_out.boxes,\n        image_zero_shot_detection_out.scores,\n        image_zero_shot_detection_out.classes,\n    ):\n        entity, pix_bbox = create_pixano_bbox_entity(\n            image, bbox, score, category,\n            is_normalized=False, source_id=\"src_gdino\",\n        )\n        new_entities.append(entity)\n        new_bboxes.append(pix_bbox)\n\nds.add_data(\"objects\", new_entities)\nds.add_data(\"bboxes\", new_bboxes)\n\nprint(\"Done\")\n</code></pre>"},{"location":"tutorials/semantic_search/","title":"Semantic search","text":""},{"location":"tutorials/semantic_search/#context","title":"Context","text":"<p>The Pixano's app support semantic search which allows a user to search for a view based on a text content.</p> <p>To do that, Pixano requires that the embeddings of the views are pre-computed with the model used for semantic search, using the embeddings functions from LanceDB.</p> <p>In this tutorial, we will go through the process of semantic search using the OpenCLIP model.</p>"},{"location":"tutorials/semantic_search/#pre-compute-the-embeddings","title":"Pre-compute the embeddings","text":"<p>First, we need to pre-compute the embeddings using LanceDB and Pixano. We will use the Health Images dataset defined in the Build and query a dataset tutorial.</p> <ol> <li>Install OpenCLIP</li> </ol> <p>For this tutorial, we will use OpenCLIP embedding function and therefore need it to be installed.</p> <pre><code>pip install open-clip-torch\n</code></pre> <ol> <li>Create the Image View Embedding table:</li> </ol> <pre><code>from pathlib import Path\nfrom pixano.datasets import Dataset\nfrom pixano.datasets.dataset_schema import SchemaRelation\nfrom pixano.features import ViewEmbedding\n\nlancedb_embedding_fn = \"open-clip\"\ntable_name = \"emb_open-clip\"\n\ndataset = Dataset( # Load the dataset\n    Path(\"./pixano_library/health_dataset\"), media_dir=Path(\"./assets/\")\n)\nembedding_schema = ViewEmbedding.create_schema( # Create the ViewEmbeddingSchema\n    embedding_fn=lancedb_embedding_fn, # For LanceDB\n    table_name=table_name,\n    dataset=dataset\n)\ndataset.create_table( # Create the table\n    name=table_name,\n    schema=embedding_schema,\n    relation_item=SchemaRelation.ONE_TO_ONE, # Only one view per item\n    mode=\"create\"\n)\n\n&gt;&gt;&gt; LanceTable(connection=LanceDBConnection(.../pixano/docs/tutorials/pixano_library/health_dataset/db), name=\"emb_open-clip\")\n</code></pre> <p>The <code>pixano.features.ViewEmbedding</code>'s method <code>create_schema</code> create a schema that contains a LanceDB embedding function compatible with Pixano.</p> <ol> <li>Compute the embeddings</li> </ol> <p>To compute the embeddings, Pixano needs to access the references to the views. Then, based on these information, it can use the LanceDB embedding function on the views.</p> <pre><code>import shortuuid\nfrom datetime import datetime\n\nviews = dataset.get_data(table_name=\"image\") # Get all views from the dataset's table \"image\".\n\ndata = [] # List of dictionnary of ViewEmbedding's model dump without the vector field.\nfor view in views:\n    data.append(\n        {\n            \"id\": shortuuid.uuid(),\n            \"created_at\": datetime.now(),\n            \"updated_at\": datetime.now(),\n            \"item_ref\": {\n                \"id\": view.item_ref.id,\n                \"name\": view.item_ref.name,\n            },\n            \"view_ref\": { # Reference to the table \"image\"\n                \"id\": view.id,\n                \"name\": \"image\",\n            },\n        }\n    )\n\ndataset.compute_view_embeddings(table_name=table_name, data=data)\n</code></pre>"},{"location":"tutorials/semantic_search/#use-the-semantic-search","title":"Use the semantic search","text":""},{"location":"tutorials/semantic_search/#with-the-app","title":"With the app","text":"<p>Now you are all set to use the semantic search, follow the using the app guide!</p> <p>The semantic search bar will appear on the dataset page.</p>"},{"location":"tutorials/semantic_search/#with-the-api","title":"With the API","text":""},{"location":"tutorials/semantic_search/#python-api","title":"Python API","text":"<p>To perform semantic search, simply call the dataset's <code>semantic_search</code> method with the text query. It will return the items with the closest view semantically to the query and the distance.</p> <pre><code>query = \"microscope\"\n\ndataset.semantic_search(query, table_name, limit=5, skip=0)\n\n&gt;&gt;&gt;(\n    [\n        Item(id='bFPEPGYaSmJGPakiaPctfF', ..., split='all', image_type='microscope'),\n        Item(id='VkBcn4bhgt2RRJNWWjvDN5', ..., split='all', image_type='microscope'),\n        Item(id='5XFPk5qwFL5xWhLHzGXLeV', ..., split='all', image_type='microscope'),\n        Item(id='A7BXLwmXWRAypbqxd5KqzK', ..., split='all', image_type='peau'),\n        Item(id='iyA4tHmGeHPP4N6diSuUXi', ..., split='all', image_type='microcope')\n    ],\n    [1.5589371919631958, 1.5611850023269653, 1.5707159042358398, 1.5987659692764282, 1.605470895767212]\n)\n</code></pre>"},{"location":"tutorials/semantic_search/#rest-api","title":"REST API","text":"<p>You first need to launch the Pixano's app to interact with the REST API.</p> <p>Then you can curl the REST API to navigate through your items:</p> <pre><code>curl -X 'GET' \\\n  'http://localhost:7492/browser/health_images?limit=5&amp;skip=0&amp;query=microscope&amp;embedding_table=emb_open-clip' \\\n  -H 'accept: application/json'\n\n&gt;&gt;&gt; {\n  \"id\": \"health_images\",\n  \"name\": \"Health Images\",\n  \"table_data\": {\n    \"columns\": [\n      {\n        \"name\": \"image\",\n        \"type\": \"image\"\n      },\n      {\n        \"name\": \"id\",\n        \"type\": \"str\"\n      },\n      {\n        \"name\": \"created_at\",\n        \"type\": \"datetime\"\n      },\n      {\n        \"name\": \"updated_at\",\n        \"type\": \"datetime\"\n      },\n      {\n        \"name\": \"split\",\n        \"type\": \"str\"\n      },\n      {\n        \"name\": \"image_type\",\n        \"type\": \"str\"\n      },\n      {\n        \"name\": \"distance\",\n        \"type\": \"float\"\n      }\n    ],\n    \"rows\": [\n      {\n        \"image\": \"\",\n        \"id\": \"bFPEPGYaSmJGPakiaPctfF\",\n        \"created_at\": \"2024-11-07T10:14:05.396010\",\n        \"updated_at\": \"2024-11-07T10:14:05.396010\",\n        \"split\": \"all\",\n        \"image_type\": \"microscope\",\n        \"distance\": 1.5589371919631958\n      },\n      {\n        \"image\": \"\",\n        \"id\": \"VkBcn4bhgt2RRJNWWjvDN5\",\n        \"created_at\": \"2024-11-07T10:14:05.367159\",\n        \"updated_at\": \"2024-11-07T10:14:05.367159\",\n        \"split\": \"all\",\n        \"image_type\": \"microscope\",\n        \"distance\": 1.5611850023269653\n      },\n      {\n        \"image\": \"\",\n        \"id\": \"5XFPk5qwFL5xWhLHzGXLeV\",\n        \"created_at\": \"2024-11-07T10:14:05.402816\",\n        \"updated_at\": \"2024-11-07T10:14:05.402816\",\n        \"split\": \"all\",\n        \"image_type\": \"microscope\",\n        \"distance\": 1.5707159042358398\n      },\n      {\n        \"image\": \"\",\n        \"id\": \"A7BXLwmXWRAypbqxd5KqzK\",\n        \"created_at\": \"2024-11-07T10:14:05.409434\",\n        \"updated_at\": \"2024-11-07T10:14:05.409434\",\n        \"split\": \"all\",\n        \"image_type\": \"peau\",\n        \"distance\": 1.5987659692764282\n      },\n      {\n        \"image\": \"\",\n        \"id\": \"iyA4tHmGeHPP4N6diSuUXi\",\n        \"created_at\": \"2024-11-07T10:14:05.363831\",\n        \"updated_at\": \"2024-11-07T10:14:05.363831\",\n        \"split\": \"all\",\n        \"image_type\": \"microcope\",\n        \"distance\": 1.605470895767212\n      }\n    ]\n  },\n  \"pagination\": {\n    \"current_page\": 0,\n    \"page_size\": 5,\n    \"total_size\": 11\n  },\n  \"semantic_search\": [\n    \"emb_open-clip\"\n  ]\n}\n</code></pre>"}]}