{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Pixano Documentation!","text":"<p>Pixano is a library of data-centric AI building blocks for computer vision applications.</p> <p>Get started</p> <p>Check the API reference</p>"},{"location":"code/","title":"Pixano API reference","text":"<p>Here you will find the documentation for all of our Python API.</p> <ul> <li>The analytics module contains useful functions for computing statistics on a dataset.</li> <li>The apps module contains the Pixano Explorer and Annotator PixanoApps.</li> <li>The core module contains the Pixano Datasets and custom data types.</li> <li>The data module contains the Pixano DataLoaders for importing and exporting datasets.</li> <li>The models module contains the Pixano InferenceModel for inference generation and embedding precomputing.</li> <li>The transforms module contains many useful functions for working with images, bounding boxes, and labels.</li> </ul>"},{"location":"code/analytics/feature_statistics/","title":"feature_statistics","text":""},{"location":"code/analytics/feature_statistics/#analytics.feature_statistics","title":"<code>analytics.feature_statistics</code>","text":""},{"location":"code/analytics/feature_statistics/#analytics.feature_statistics.categorical_stats","title":"<code>categorical_stats(df, split, field_name)</code>","text":"<p>Compute feature categorical statistics</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Input DataFrame</p> required <code>split</code> <code>str</code> <p>DataFrame split</p> required <code>field_name</code> <code>str</code> <p>Selected field</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: Feature statistics</p> Source code in <code>pixano/analytics/feature_statistics.py</code> <pre><code>def categorical_stats(df: pd.DataFrame, split: str, field_name: str) -&gt; list[dict]:\n\"\"\"Compute feature categorical statistics\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        split (str): DataFrame split\n        field_name (str): Selected field\n\n    Returns:\n        list[dict]: Feature statistics\n    \"\"\"\n\n    counts = df.value_counts(subset=field_name)\n    return [{field_name: k, \"counts\": v, \"split\": split} for k, v in counts.items()]\n</code></pre>"},{"location":"code/analytics/feature_statistics/#analytics.feature_statistics.compute_additional_data","title":"<code>compute_additional_data(data_table)</code>","text":"<p>Convert Table to DataFrame and add resolution and aspect ratio</p> <p>Parameters:</p> Name Type Description Default <code>data_table</code> <code>pa.Table</code> <p>Input Table</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: DataFrame with added resolution and aspect ratio</p> Source code in <code>pixano/analytics/feature_statistics.py</code> <pre><code>def compute_additional_data(data_table: pa.Table) -&gt; pd.DataFrame:\n\"\"\"Convert Table to DataFrame and add resolution and aspect ratio\n\n    Args:\n        data_table (pa.Table): Input Table\n\n    Returns:\n        pd.DataFrame: DataFrame with added resolution and aspect ratio\n    \"\"\"\n\n    # Take a subset of table without image columns (which can't be converted to pandas)\n    if not all(p in data_table.column_names for p in [\"width\", \"height\"]):\n        return None\n    data = data_table.select([\"width\", \"height\"]).to_pandas()\n\n    # Compute additional data\n    data[\"resolution\"] = data.apply(\n        lambda x: str(x[\"width\"]) + \"x\" + str(x[\"height\"]), axis=1\n    )\n    data[\"aspect_ratio\"] = data.apply(\n        lambda x: str(Fraction(x[\"width\"], x[\"height\"])).replace(\"/\", \":\"), axis=1\n    )\n\n    return data\n</code></pre>"},{"location":"code/analytics/feature_statistics/#analytics.feature_statistics.compute_stats","title":"<code>compute_stats(df, split, feature)</code>","text":"<p>Compute feature statistics</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Input DataFrame</p> required <code>split</code> <code>str</code> <p>DataFrame split</p> required <code>feature</code> <code>dict</code> <p>Selected feature</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: Feature statistics</p> Source code in <code>pixano/analytics/feature_statistics.py</code> <pre><code>def compute_stats(df: pd.DataFrame, split: str, feature: dict) -&gt; list[dict]:\n\"\"\"Compute feature statistics\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        split (str): DataFrame split\n        feature (dict): Selected feature\n\n    Returns:\n        list[dict]: Feature statistics\n    \"\"\"\n\n    # Categorical\n    if feature[\"type\"] == \"categorical\":\n        return categorical_stats(df, split, feature[\"name\"])\n    # Numerical\n    elif feature[\"type\"] == \"numerical\":\n        return numerical_stats(df, split, feature[\"name\"], feature.get(\"range\", None))\n    # Else\n    else:\n        return []\n</code></pre>"},{"location":"code/analytics/feature_statistics/#analytics.feature_statistics.numerical_stats","title":"<code>numerical_stats(df, split, field_name, field_range=None)</code>","text":"<p>Compute feature numerical statistics</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Input DataFrame</p> required <code>split</code> <code>str</code> <p>DataFrame split</p> required <code>field_name</code> <code>str</code> <p>Selected field</p> required <code>field_range</code> <code>list[float]</code> <p>Selected field range. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: Feature statistics</p> Source code in <code>pixano/analytics/feature_statistics.py</code> <pre><code>def numerical_stats(\n    df: pd.DataFrame, split: str, field_name: str, field_range: list[float] = None\n) -&gt; list[dict]:\n\"\"\"Compute feature numerical statistics\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        split (str): DataFrame split\n        field_name (str): Selected field\n        field_range (list[float], optional): Selected field range. Defaults to None.\n\n    Returns:\n        list[dict]: Feature statistics\n    \"\"\"\n\n    counts, bins = np.histogram(df[field_name], range=field_range)\n    return [\n        {\n            \"bin_start\": float(bins[i]),\n            \"bin_end\": float(bins[i + 1]),\n            \"counts\": int(counts[i]),\n            \"split\": split,\n        }\n        for i in range(len(counts))\n    ]\n</code></pre>"},{"location":"code/analytics/feature_statistics/#analytics.feature_statistics.objects_tableToDF","title":"<code>objects_tableToDF(data_table, field)</code>","text":"<p>Convert a field from the objects column to a DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>data_table</code> <code>pa.Table</code> <p>Table with an objects column</p> required <code>field</code> <code>str</code> <p>Selected field from the objects column</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: Selected field as DataFrame</p> Source code in <code>pixano/analytics/feature_statistics.py</code> <pre><code>def objects_tableToDF(data_table: pa.Table, field: str) -&gt; pd.DataFrame:\n\"\"\"Convert a field from the objects column to a DataFrame\n\n    Args:\n        data_table (pa.Table): Table with an objects column\n        field (str): Selected field from the objects column\n\n    Returns:\n        pd.DataFrame: Selected field as DataFrame\n    \"\"\"\n\n    try:\n        df_objs = data_table.select([\"objects\"]).to_pandas()\n        sel = [{field: d[field]} for objs in df_objs[\"objects\"] for d in objs]\n        return pd.DataFrame.from_dict(sel)\n    except Exception:\n        print(\"ERROR: Unable to convert table Pandas DataFrame\")\n        return None\n</code></pre>"},{"location":"code/apps/annotator/","title":"annotator","text":""},{"location":"code/apps/annotator/#apps.annotator.serve","title":"<code>apps.annotator.serve</code>","text":""},{"location":"code/apps/annotator/#apps.annotator.serve.AnnotatorApp","title":"<code>AnnotatorApp(library_dir, host='127.0.0.1', port=0)</code>","text":"<p>         Bases: <code>PixanoApp</code></p> <p>Pixano Annotator App</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>uvicorn.Config</code> <p>App config</p> <code>server</code> <code>uvicorn.Server</code> <p>App server</p> <code>task_function</code> <code>typing.Callable</code> <p>Run task function for running environment</p> <code>display_function</code> <code>typing.Callable</code> <p>Display function for running environment</p> <p>Parameters:</p> Name Type Description Default <code>library_dir</code> <code>str</code> <p>Dataset library directory</p> required <code>host</code> <code>str</code> <p>App host. Defaults to \"127.0.0.1\".</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>App port. Defaults to 0.</p> <code>0</code> Source code in <code>pixano/apps/annotator/serve.py</code> <pre><code>def __init__(\n    self,\n    library_dir: str,\n    host: str = \"127.0.0.1\",\n    port: int = 0,\n) -&gt; None:\n\"\"\"Initialize and run Pixano Annotator app\n\n    Args:\n        library_dir (str): Dataset library directory\n        host (str, optional): App host. Defaults to \"127.0.0.1\".\n        port (int, optional): App port. Defaults to 0.\n    \"\"\"\n\n    super().__init__(library_dir, ASSETS_PATH, TEMPLATE_PATH, host, port)\n</code></pre>"},{"location":"code/apps/annotator/#apps.annotator.serve.main","title":"<code>main(library_dir, host, port)</code>","text":"<p>Launch Pixano Annotator</p> <p>LIBRARY_DIR: Dataset library directory</p> Source code in <code>pixano/apps/annotator/serve.py</code> <pre><code>@click.command(context_settings={\"auto_envvar_prefix\": \"UVICORN\"})\n@click.argument(\n    \"library_dir\",\n    type=str,\n)\n@click.option(\n    \"--host\",\n    type=str,\n    default=\"127.0.0.1\",\n    help=\"Bind socket to this host.\",\n    show_default=True,\n)\n@click.option(\n    \"--port\",\n    type=int,\n    default=0,\n    help=\"Bind socket to this port.\",\n    show_default=True,\n)\ndef main(library_dir: str, host: str, port: int):\n\"\"\"Launch Pixano Annotator\n\n    LIBRARY_DIR: Dataset library directory\n    \"\"\"\n    AnnotatorApp(library_dir, host, port)\n</code></pre>"},{"location":"code/apps/explorer/","title":"explorer","text":""},{"location":"code/apps/explorer/#apps.explorer.serve","title":"<code>apps.explorer.serve</code>","text":""},{"location":"code/apps/explorer/#apps.explorer.serve.ExplorerApp","title":"<code>ExplorerApp(library_dir, host='127.0.0.1', port=0)</code>","text":"<p>         Bases: <code>PixanoApp</code></p> <p>Pixano Explorer App</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>uvicorn.Config</code> <p>App config</p> <code>server</code> <code>uvicorn.Server</code> <p>App server</p> <code>task_function</code> <code>typing.Callable</code> <p>Run task function for running environment</p> <code>display_function</code> <code>typing.Callable</code> <p>Display function for running environment</p> <p>Parameters:</p> Name Type Description Default <code>library_dir</code> <code>str</code> <p>Dataset library directory</p> required <code>host</code> <code>str</code> <p>App host. Defaults to \"127.0.0.1\".</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>App port. Defaults to 0.</p> <code>0</code> Source code in <code>pixano/apps/explorer/serve.py</code> <pre><code>def __init__(\n    self,\n    library_dir: str,\n    host: str = \"127.0.0.1\",\n    port: int = 0,\n) -&gt; None:\n\"\"\"Initialize and run Pixano Explorer app\n\n    Args:\n        library_dir (str): Dataset library directory\n        host (str, optional): App host. Defaults to \"127.0.0.1\".\n        port (int, optional): App port. Defaults to 0.\n    \"\"\"\n\n    super().__init__(library_dir, ASSETS_PATH, TEMPLATE_PATH, host, port)\n</code></pre>"},{"location":"code/apps/explorer/#apps.explorer.serve.main","title":"<code>main(library_dir, host, port)</code>","text":"<p>Launch Pixano Explorer</p> <p>LIBRARY_DIR: Dataset library directory</p> Source code in <code>pixano/apps/explorer/serve.py</code> <pre><code>@click.command(context_settings={\"auto_envvar_prefix\": \"UVICORN\"})\n@click.argument(\n    \"library_dir\",\n    type=str,\n)\n@click.option(\n    \"--host\",\n    type=str,\n    default=\"127.0.0.1\",\n    help=\"Bind socket to this host.\",\n    show_default=True,\n)\n@click.option(\n    \"--port\",\n    type=int,\n    default=0,\n    help=\"Bind socket to this port.\",\n    show_default=True,\n)\ndef main(library_dir: str, host: str, port: int):\n\"\"\"Launch Pixano Explorer\n\n    LIBRARY_DIR: Dataset library directory\n    \"\"\"\n    ExplorerApp(library_dir, host, port)\n</code></pre>"},{"location":"code/apps/core/db_utils/","title":"db_utils","text":""},{"location":"code/apps/core/db_utils/#apps.core.db_utils","title":"<code>apps.core.db_utils</code>","text":""},{"location":"code/apps/core/db_utils/#apps.core.db_utils.Feature","title":"<code>Feature</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Feature</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Feature name</p> <code>dtype</code> <code>str</code> <p>Feature dtype</p> <code>value</code> <code>Any</code> <p>Feature value</p>"},{"location":"code/apps/core/db_utils/#apps.core.db_utils.get_item_details","title":"<code>get_item_details(dataset, item_id, media_dir, inf_datasets=[])</code>","text":"<p>Get item details</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ds.Dataset</code> <p>Dataset</p> required <code>item_id</code> <code>str</code> <p>Selected item ID</p> required <code>media_dir</code> <code>Path</code> <p>Dataset media directory</p> required <code>inf_datasets</code> <code>list[ds.Dataset]</code> <p>List of inference datasets. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>ImageDetails features for UI</p> Source code in <code>pixano/apps/core/db_utils.py</code> <pre><code>def get_item_details(\n    dataset: ds.Dataset,\n    item_id: str,\n    media_dir: Path,\n    inf_datasets: list[ds.Dataset] = [],\n) -&gt; dict:\n\"\"\"Get item details\n\n    Args:\n        dataset (ds.Dataset): Dataset\n        item_id (str): Selected item ID\n        media_dir (Path): Dataset media directory\n        inf_datasets (list[ds.Dataset], optional): List of inference datasets. Defaults to [].\n\n    Returns:\n        dict: ImageDetails features for UI\n    \"\"\"\n\n    # Get item\n    scanner = dataset.scanner(filter=ds.field(\"id\").isin([item_id]))\n    item = scanner.to_table().to_pylist()[0]\n\n    # Get item inference objects\n    for inf_ds in inf_datasets:\n        inf_scanner = inf_ds.scanner(filter=ds.field(\"id\").isin([item_id]))\n        inf_item = inf_scanner.to_table().to_pylist()[0]\n        if inf_item is not None:\n            item[\"objects\"].extend(inf_item[\"objects\"])\n\n    # Create features\n    features = {\n        \"id\": item[\"id\"],\n        \"filename\": None,\n        \"width\": None,\n        \"height\": None,\n        \"categoryStats\": [],\n        \"views\": {},\n    }\n\n    # Category statistics\n    cat_ids = [\n        obj[\"category_id\"] for obj in item[\"objects\"] if obj[\"category_id\"] is not None\n    ]\n    cat_names = [\n        obj[\"category_name\"]\n        for obj in item[\"objects\"]\n        if obj[\"category_id\"] is not None\n    ]\n    cat, index, count = np.unique(cat_ids, return_index=True, return_counts=True)\n    # Add to features\n    features[\"categoryStats\"] = [\n        {\n            \"id\": int(cat[i]),\n            \"name\": str(cat_names[index[i]]),\n            \"count\": int(count[i]),\n        }\n        for i in range(len(cat))\n        if cat[i] is not None\n    ]\n\n    # Views\n    for field in dataset.schema:\n        if arrow_types.is_image_type(field.type):\n            # Image\n            image = item[field.name]\n            image.uri_prefix = media_dir.absolute().as_uri()\n\n            # Objects IDs\n            ids = [obj[\"id\"] for obj in item[\"objects\"] if obj[\"view_id\"] == field.name]\n            # Categories\n            cats = [\n                {\"id\": obj[\"category_id\"], \"name\": obj[\"category_name\"]}\n                for obj in item[\"objects\"]\n                if obj[\"view_id\"] == field.name\n                and obj[\"category_id\"] is not None\n                and obj[\"category_name\"] is not None\n            ]\n            # Bounding boxes\n            boxes = [\n                transforms.format_bbox(\n                    obj[\"bbox\"],\n                    obj[\"bbox_confidence\"] is not None,\n                    obj[\"bbox_confidence\"],\n                )\n                for obj in item[\"objects\"]\n                if obj[\"view_id\"] == field.name and obj[\"bbox\"] is not None\n            ]\n            # Masks\n            masks = [\n                transforms.rle_to_urle(obj[\"mask\"])\n                for obj in item[\"objects\"]\n                if obj[\"view_id\"] == field.name and obj[\"mask\"] is not None\n            ]\n            # Add to features\n            features[\"views\"][field.name] = {\n                \"image\": image.url,\n                \"objects\": {\n                    \"id\": ids,\n                    \"category\": cats,\n                    \"boundingBox\": boxes,\n                    \"segmentation\": masks,\n                },\n            }\n\n    return features\n</code></pre>"},{"location":"code/apps/core/db_utils/#apps.core.db_utils.get_item_view_embedding","title":"<code>get_item_view_embedding(emb_ds, item_id, view)</code>","text":"<p>Get item embedding for a view</p> <p>Parameters:</p> Name Type Description Default <code>emb_ds</code> <code>ds.Dataset</code> <p>Embedding dataset</p> required <code>item_id</code> <code>str</code> <p>Item ID</p> required <code>view</code> <code>str</code> <p>Item embedding view</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Embedding in base 64</p> Source code in <code>pixano/apps/core/db_utils.py</code> <pre><code>def get_item_view_embedding(emb_ds: ds.Dataset, item_id: str, view: str) -&gt; bytes:\n\"\"\"Get item embedding for a view\n\n    Args:\n        emb_ds (ds.Dataset): Embedding dataset\n        item_id (str): Item ID\n        view (str): Item embedding view\n\n    Returns:\n        bytes: Embedding in base 64\n    \"\"\"\n\n    # Get item\n    emb_scanner = emb_ds.scanner(filter=ds.field(\"id\").isin([item_id]))\n    emb_item = emb_scanner.to_table().to_pylist()[0]\n    return emb_item[f\"{view}_embedding\"]\n</code></pre>"},{"location":"code/apps/core/db_utils/#apps.core.db_utils.get_items","title":"<code>get_items(dataset, params=None)</code>","text":"<p>Get items</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>pa.Dataset</code> <p>Dataset</p> required <code>params</code> <code>AbstractParams</code> <p>FastAPI params for pagination. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>AbstractPage</code> <code>AbstractPage</code> <p>List of Features for UI (DatasetExplorer)</p> Source code in <code>pixano/apps/core/db_utils.py</code> <pre><code>def get_items(dataset: ds.Dataset, params: AbstractParams = None) -&gt; AbstractPage:\n\"\"\"Get items\n\n    Args:\n        dataset (pa.Dataset): Dataset\n        params (AbstractParams, optional): FastAPI params for pagination. Defaults to None.\n\n    Returns:\n        AbstractPage: List of Features for UI (DatasetExplorer)\n    \"\"\"\n\n    # Get page parameters\n    params = resolve_params(params)\n    raw_params = params.to_raw_params()\n    total = dataset.count_rows()\n\n    # Get page items\n    start = raw_params.offset\n    stop = min(raw_params.offset + raw_params.limit, total)\n    if start &gt;= stop:\n        return None\n    items_table = dataset.take(range(start, stop))\n\n    def _create_features(row: list) -&gt; list[Feature]:\n\"\"\"Create features based on field types\n\n        Args:\n            row (list): Input row\n\n        Returns:\n            list[Feature]: Row as list of features\n        \"\"\"\n\n        features = []\n\n        # Iterate on fields\n        for field in dataset.schema:\n            # Number fields\n            if arrow_types.is_number(field.type):\n                features.append(\n                    Feature(name=field.name, dtype=\"number\", value=row[field.name])\n                )\n            # Image fields\n            elif arrow_types.is_image_type(field.type):\n                thumbnail = row[field.name].preview_url\n                features.append(\n                    Feature(name=field.name, dtype=\"image\", value=thumbnail)\n                )\n            # String fields\n            elif pa.types.is_string(field.type):\n                features.append(\n                    Feature(name=field.name, dtype=\"text\", value=row[field.name])\n                )\n\n        return features\n\n    # Create items features\n    items = [_create_features(e) for e in items_table.to_pylist()]\n\n    return create_page(items, total=total, params=params)\n</code></pre>"},{"location":"code/apps/core/db_utils/#apps.core.db_utils.update_annotations","title":"<code>update_annotations(dataset_dir, item_id, annotations)</code>","text":"<p>Update dataset annotations</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>Path</code> <p>Dataset directory</p> required <code>item_id</code> <code>str</code> <p>Item ID</p> required <code>annotations</code> <code>list[arrow_types.ObjectAnnotation]</code> <p>Item annotations</p> required Source code in <code>pixano/apps/core/db_utils.py</code> <pre><code>def update_annotations(\n    dataset_dir: Path,\n    item_id: str,\n    annotations: list[arrow_types.ObjectAnnotation],\n):\n\"\"\"Update dataset annotations\n\n    Args:\n        dataset_dir (Path): Dataset directory\n        item_id (str): Item ID\n        annotations (list[arrow_types.ObjectAnnotation]): Item annotations\n    \"\"\"\n\n    # Convert URLE to RLE and add bounding box\n    # TODO: Find a fix for image datasets\n    item_anns = [o.dict() for o in annotations]\n    for ann in item_anns:\n        ann[\"bbox\"] = urle_to_bbox(ann[\"mask\"])\n        ann[\"bbox_source\"] = ann[\"mask_source\"]\n        ann[\"mask\"] = urle_to_rle(ann[\"mask\"])\n\n    # Dataset files\n    files = (dataset_dir / \"db\").glob(\"**/*.parquet\")\n    files = sorted(files, key=lambda x: natural_key(x.name))\n\n    # Iterate on dataset files\n    for file in files:\n        # Look for updated item\n        table = pq.read_table(file)\n        filter = table.filter(pc.field(\"id\").isin([item_id]))\n        item = filter.to_pylist()\n\n        # If item found\n        if item != []:\n            # Read table without item\n            updated_table = table.filter(~pc.field(\"id\").isin([item_id])).to_pydict()\n\n            # Add item with updated annotations\n            item[0][\"objects\"] = item_anns\n            for field in table.schema:\n                updated_table[field.name].append(item[0][field.name])\n\n            # Sort table fields according to IDs\n            for field in table.schema:\n                if field.name != \"id\":\n                    updated_table[field.name] = [\n                        x\n                        for _, x in sorted(\n                            zip(updated_table[\"id\"], updated_table[field.name]),\n                            key=lambda pair: natural_key(pair[0]),\n                        )\n                    ]\n            # Sort table IDs\n            updated_table[\"id\"] = sorted(updated_table[\"id\"], key=natural_key)\n\n            # Convert ExtensionTypes\n            arrays = []\n            for field in table.schema:\n                # Convert image types to dict before PyArrow conversion\n                # TODO: find a better way\n                if arrow_types.is_image_type(field.type):\n                    updated_table[field.name] = [\n                        i.to_dict() for i in updated_table[field.name]\n                    ]\n                arrays.append(\n                    arrow_types.convert_field(\n                        field_name=field.name,\n                        field_type=field.type,\n                        field_data=updated_table[field.name],\n                    )\n                )\n\n            # Save updated table\n            pq.write_table(\n                pa.Table.from_arrays(arrays, schema=table.schema),\n                file,\n            )\n            return\n</code></pre>"},{"location":"code/apps/core/main/","title":"main","text":""},{"location":"code/apps/core/main/#apps.core.main","title":"<code>apps.core.main</code>","text":""},{"location":"code/apps/core/main/#apps.core.main.Settings","title":"<code>Settings</code>","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Dataset library settings</p> <p>Attributes:</p> Name Type Description <code>data_dir</code> <code>Path</code> <p>Dataset library directory</p>"},{"location":"code/apps/core/main/#apps.core.main.create_app","title":"<code>create_app(settings)</code>","text":"<p>Run Pixano app</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Settings</code> <p>Dataset Library</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>404, Dataset is not found</p> <code>HTTPException</code> <p>404, Dataset stats are not found</p> <code>HTTPException</code> <p>404, Dataset is not found</p> <code>HTTPException</code> <p>404, Dataset is not found</p> <code>HTTPException</code> <p>404, Embedding dataset is not found</p> <p>Returns:</p> Name Type Description <code>FastAPI</code> <code>FastAPI</code> <p>Explorer app</p> Source code in <code>pixano/apps/core/main.py</code> <pre><code>def create_app(settings: Settings) -&gt; FastAPI:\n\"\"\"Run Pixano app\n\n    Args:\n        settings (Settings): Dataset Library\n\n    Raises:\n        HTTPException: 404, Dataset is not found\n        HTTPException: 404, Dataset stats are not found\n        HTTPException: 404, Dataset is not found\n        HTTPException: 404, Dataset is not found\n        HTTPException: 404, Embedding dataset is not found\n\n    Returns:\n        FastAPI: Explorer app\n    \"\"\"\n    app = FastAPI()\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Check if library exists\n    if not settings.data_dir.exists():\n        raise FileNotFoundError(\n            f\"Dataset library '{settings.data_dir.absolute()}' not found\"\n        )\n\n    # Create models folder\n    model_dir = settings.data_dir / \"models\"\n    model_dir.mkdir(exist_ok=True)\n    app.mount(\"/models\", StaticFiles(directory=model_dir), name=\"models\")\n\n    @app.get(\"/datasets\", response_model=list[DatasetInfo])\n    async def get_datasets_list():\n        return load_library(settings)\n\n    @app.get(\"/datasets/{ds_id}/items\", response_model=Page[db_utils.Features])\n    async def get_dataset_items(ds_id, params: Params = Depends()):\n        # Load dataset\n        ds = load_dataset(ds_id, settings)\n        if ds is None:\n            raise HTTPException(status_code=404, detail=\"Dataset not found\")\n        # Return dataset items\n        res = db_utils.get_items(ds.load(), params)\n        if res is None:\n            raise HTTPException(status_code=404, detail=\"Data not found\")\n        else:\n            return res\n\n    @app.get(\"/datasets/{ds_id}/stats\")\n    async def get_dataset_stats(ds_id):\n        # Load dataset stats\n        stats = load_dataset_stats(ds_id, settings)\n        if stats is None:\n            raise HTTPException(status_code=404, detail=\"Stats not found\")\n        # Return dataset stats\n        return stats\n\n    @app.get(\"/datasets/{ds_id}/items/{item_id}\")\n    async def get_dataset_item_details(ds_id: str, item_id: str):\n        # Load dataset\n        ds = load_dataset(ds_id, settings)\n        if ds is None:\n            raise HTTPException(status_code=404, detail=\"Dataset not found\")\n\n        # Load inference datasets\n        inf_datasets = []\n        for inf_json in sorted(list(ds.path.glob(\"db_infer_*/infer.json\"))):\n            inf_datasets.append(InferenceDataset(inf_json.parent).load())\n\n        # Return item details\n        return db_utils.get_item_details(ds.load(), item_id, ds.media_dir, inf_datasets)\n\n    @app.post(\"/datasets/{ds_id}/items/{item_id}/{view}/embedding\")\n    async def get_dataset_item_view_embedding(ds_id: str, item_id: str, view: str):\n        # Load dataset\n        ds = load_dataset(ds_id, settings)\n        if ds is None:\n            raise HTTPException(status_code=404, detail=\"Dataset not found\")\n\n        # Load embedding dataset (currently selecting latest one)\n        emb_ds = None\n        for emb_json in sorted(list(ds.path.glob(\"db_embed_*/embed.json\"))):\n            emb_ds = EmbeddingDataset(emb_json.parent).load()\n        if emb_ds is None:\n            raise HTTPException(status_code=404, detail=\"Embedding dataset not found\")\n\n        # Return item embedding\n        return Response(content=db_utils.get_item_view_embedding(emb_ds, item_id, view))\n\n    @app.post(\n        \"/datasets/{ds_id}/items/{item_id}/annotations\",\n        response_model=list[arrow_types.ObjectAnnotation],\n    )\n    async def post_dataset_item_annotations(\n        ds_id: str,\n        item_id: str,\n        annotations: list[arrow_types.ObjectAnnotation],\n    ):\n        # Load dataset\n        ds = load_dataset(ds_id, settings)\n        if ds is None:\n            raise HTTPException(status_code=404, detail=\"Dataset not found\")\n\n        # Update dataset annotations\n        db_utils.update_annotations(ds.path, item_id, annotations)\n\n        return Response()\n\n    add_pagination(app)\n\n    return app\n</code></pre>"},{"location":"code/apps/core/main/#apps.core.main.load_dataset","title":"<code>load_dataset(ds_id, settings)</code>","text":"<p>Load dataset based on its ID</p> <p>Parameters:</p> Name Type Description Default <code>ds_id</code> <code>str</code> <p>Dataset ID</p> required <code>settings</code> <code>Settings</code> <p>Dataset library</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset</p> Source code in <code>pixano/apps/core/main.py</code> <pre><code>def load_dataset(ds_id: str, settings: Settings) -&gt; Dataset:\n\"\"\"Load dataset based on its ID\n\n    Args:\n        ds_id (str): Dataset ID\n        settings (Settings): Dataset library\n\n    Returns:\n        Dataset: Dataset\n    \"\"\"\n\n    for spec in settings.data_dir.glob(\"*/spec.json\"):\n        info = DatasetInfo.parse_file(spec)\n        if ds_id == info.id:\n            return Dataset(spec.parent)\n</code></pre>"},{"location":"code/apps/core/main/#apps.core.main.load_dataset_stats","title":"<code>load_dataset_stats(ds_id, settings)</code>","text":"<p>Load dataset stats based on its ID</p> <p>Parameters:</p> Name Type Description Default <code>ds_id</code> <code>str</code> <p>Dataset ID</p> required <code>settings</code> <code>Settings</code> <p>Dataset Library</p> required <p>Returns:</p> Type Description <code>dict</code> <p>list[dict]: Dataset stats</p> Source code in <code>pixano/apps/core/main.py</code> <pre><code>def load_dataset_stats(ds_id: str, settings: Settings) -&gt; dict:\n\"\"\"Load dataset stats based on its ID\n\n    Args:\n        ds_id (str): Dataset ID\n        settings (Settings): Dataset Library\n\n    Returns:\n        list[dict]: Dataset stats\n    \"\"\"\n\n    ds = load_dataset(ds_id, settings)\n    if ds is not None:\n        stats_file = ds.path / \"db_feature_statistics.json\"\n        if stats_file.is_file():\n            with open(stats_file, \"r\") as f:\n                return json.load(f)\n</code></pre>"},{"location":"code/apps/core/main/#apps.core.main.load_library","title":"<code>load_library(settings)</code>","text":"<p>Load all dataset info files in library</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Settings</code> <p>Dataset library settings</p> required <p>Returns:</p> Type Description <code>list[DatasetInfo]</code> <p>list[DatasetInfo]: Dataset info files</p> Source code in <code>pixano/apps/core/main.py</code> <pre><code>def load_library(settings: Settings) -&gt; list[DatasetInfo]:\n\"\"\"Load all dataset info files in library\n\n    Args:\n        settings (Settings): Dataset library settings\n\n    Returns:\n        list[DatasetInfo]: Dataset info files\n    \"\"\"\n\n    infos = []\n    for spec in sorted(settings.data_dir.glob(\"*/spec.json\")):\n        # Load dataset info\n        info = DatasetInfo.parse_file(spec)\n        # Load thumbnail\n        preview_path = spec.parent / \"preview.png\"\n        if preview_path.is_file():\n            im = arrow_types.Image(uri=preview_path.absolute().as_uri())\n            info.preview = im.url\n\n        # Load categories\n        info.categories = getattr(info, \"categories\", [])\n        if info.categories is None:\n            info.categories = []\n        # Save dataset info\n        infos.append(info)\n    return infos\n</code></pre>"},{"location":"code/apps/core/serve/","title":"serve","text":""},{"location":"code/apps/core/serve/#apps.core.serve","title":"<code>apps.core.serve</code>","text":""},{"location":"code/apps/core/serve/#apps.core.serve.PixanoApp","title":"<code>PixanoApp(library_dir, assets_path, template_path, host='127.0.0.1', port=8000)</code>","text":"<p>Pixano App</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>uvicorn.Config</code> <p>App config</p> <code>server</code> <code>uvicorn.Server</code> <p>App server</p> <code>task_function</code> <code>typing.Callable</code> <p>Run task function for running environment</p> <code>display_function</code> <code>typing.Callable</code> <p>Display function for running environment</p> <p>Parameters:</p> Name Type Description Default <code>library_dir</code> <code>str</code> <p>Dataset library directory</p> required <code>host</code> <code>str</code> <p>App host. Defaults to \"127.0.0.1\".</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>App port. Defaults to 8000.</p> <code>8000</code> Source code in <code>pixano/apps/core/serve.py</code> <pre><code>def __init__(\n    self,\n    library_dir: str,\n    assets_path: str,\n    template_path: str,\n    host: str = \"127.0.0.1\",\n    port: int = 8000,\n):\n\"\"\"Initialize and run Pixano app\n\n    Args:\n        library_dir (str): Dataset library directory\n        host (str, optional): App host. Defaults to \"127.0.0.1\".\n        port (int, optional): App port. Defaults to 8000.\n    \"\"\"\n\n    # Create app\n    templates = Jinja2Templates(directory=template_path)\n    settings = Settings(data_dir=Path(library_dir))\n    app = create_app(settings)\n\n    @app.get(\"/\", response_class=HTMLResponse)\n    def main(request: fastapi.Request):\n        return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n    app.mount(\"/assets\", StaticFiles(directory=assets_path), name=\"assets\")\n    self.config = uvicorn.Config(app, host=host, port=port)\n    self.server = uvicorn.Server(self.config)\n\n    # Get environmennt\n    self.task_function = {\n        \"colab\": asyncio.get_event_loop().create_task,\n        \"ipython\": asyncio.get_event_loop().create_task,\n        \"none\": asyncio.run,\n    }[get_env()]\n    self.display_function = {\n        \"colab\": display_colab,\n        \"ipython\": display_ipython,\n        \"none\": display_cli,\n    }[get_env()]\n\n    # Serve app\n    self.task_function(self.server.serve())\n</code></pre>"},{"location":"code/apps/core/serve/#apps.core.serve.PixanoApp.display","title":"<code>display(height=1000)</code>","text":"<p>Display Pixano app</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Frame height. Defaults to 1000.</p> <code>1000</code> Source code in <code>pixano/apps/core/serve.py</code> <pre><code>def display(self, height: int = 1000) -&gt; None:\n\"\"\"Display Pixano app\n\n    Args:\n        height (int, optional): Frame height. Defaults to 1000.\n    \"\"\"\n\n    # Wait for app to be online\n    while not self.server.started:\n        self.task_function(asyncio.wait(0.1))\n\n    # Display app\n    for server in self.server.servers:\n        for socket in server.sockets:\n            address = socket.getsockname()\n            self.display_function(\n                url=f\"http://{address[0]}\", port=address[1], height=height\n            )\n</code></pre>"},{"location":"code/apps/core/serve/#apps.core.serve.display_cli","title":"<code>display_cli(url, port, height)</code>","text":"<p>Display a Pixano app inside a command line interface</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Pixano app URL</p> required <code>port</code> <code>int</code> <p>Pixano app port</p> required <code>height</code> <code>int</code> <p>Frame height</p> required Source code in <code>pixano/apps/core/serve.py</code> <pre><code>def display_cli(url: str, port: int, height: int):\n\"\"\"Display a Pixano app inside a command line interface\n\n    Args:\n        url (str): Pixano app URL\n        port (int): Pixano app port\n        height (int): Frame height\n    \"\"\"\n\n    print(f\"Please visit {url}:{port} in a web browser.\")\n</code></pre>"},{"location":"code/apps/core/serve/#apps.core.serve.display_colab","title":"<code>display_colab(url, port, height)</code>","text":"<p>Display a Pixano app inside a Google Colab</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Pixano app URL</p> required <code>port</code> <code>int</code> <p>Pixano app port</p> required <code>height</code> <code>int</code> <p>Frame height</p> required Source code in <code>pixano/apps/core/serve.py</code> <pre><code>def display_colab(url: str, port: int, height: int):\n\"\"\"Display a Pixano app inside a Google Colab\n\n    Args:\n        url (str): Pixano app URL\n        port (int): Pixano app port\n        height (int): Frame height\n    \"\"\"\n\n    # Define frame template\n    shell = \"\"\"\n        (() =&gt; {\n            const url = new URL(%URL%);\n            const port = %PORT%;\n            if (port) {\n                url.port = port;\n            }\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '%HEIGHT%');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    \"\"\"\n\n    # Replace variables in template\n    replacements = [\n        (\"%HEIGHT%\", \"%d\" % height),\n        (\"%PORT%\", \"%d\" % port),\n        (\"%URL%\", json.dumps(url)),\n    ]\n    for k, v in replacements:\n        shell = shell.replace(k, v)\n\n    # Display frame\n    script = IPython.display.Javascript(shell)\n    IPython.display.display(script)\n</code></pre>"},{"location":"code/apps/core/serve/#apps.core.serve.display_ipython","title":"<code>display_ipython(url, port, height)</code>","text":"<p>Display a Pixano app inside a Jupyter notebook</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Pixano app URL</p> required <code>port</code> <code>int</code> <p>Pixano app port</p> required <code>height</code> <code>int</code> <p>Frame height</p> required Source code in <code>pixano/apps/core/serve.py</code> <pre><code>def display_ipython(url: str, port: int, height: int):\n\"\"\"Display a Pixano app inside a Jupyter notebook\n\n    Args:\n        url (str): Pixano app URL\n        port (int): Pixano app port\n        height (int): Frame height\n    \"\"\"\n\n    # Define frame template\n    shell = \"\"\"\n        &lt;iframe id=\"%HTML_ID%\" width=\"100%\" height=\"%HEIGHT%\" frameborder=\"0\"&gt;\n        &lt;/iframe&gt;\n        &lt;script&gt;\n            (function() {\n                const frame = document.getElementById(%JSON_ID%);\n                const url = new URL(%URL%, window.location);\n                const port = %PORT%;\n                if (port) {\n                    url.port = port;\n                }\n                frame.src = url;\n            })();\n        &lt;/script&gt;\n    \"\"\"\n\n    # Replace variables in template\n    frame_id = f\"frame-{shortuuid.uuid()}\"\n    replacements = [\n        (\"%HTML_ID%\", html.escape(frame_id, quote=True)),\n        (\"%JSON_ID%\", json.dumps(frame_id)),\n        (\"%HEIGHT%\", \"%d\" % height),\n        (\"%PORT%\", \"%d\" % port),\n        (\"%URL%\", json.dumps(url)),\n    ]\n    for k, v in replacements:\n        shell = shell.replace(k, v)\n\n    # Display frame\n    iframe = IPython.display.HTML(shell)\n    IPython.display.display(iframe)\n</code></pre>"},{"location":"code/apps/core/serve/#apps.core.serve.get_env","title":"<code>get_env()</code>","text":"<p>Get current running environment</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Running environment</p> Source code in <code>pixano/apps/core/serve.py</code> <pre><code>def get_env() -&gt; str:\n\"\"\"Get current running environment\n\n    Returns:\n        str: Running environment\n    \"\"\"\n\n    # If Google colab import succeeds\n    try:\n        import google.colab\n        import IPython\n    except ImportError:\n        pass\n    else:\n        if IPython.get_ipython() is not None:\n            return \"colab\"\n\n    # If IPython import succeeds\n    try:\n        import IPython\n    except ImportError:\n        pass\n    else:\n        ipython = IPython.get_ipython()\n        if ipython is not None and ipython.has_trait(\"kernel\"):\n            return \"ipython\"\n\n    # Else\n    return \"none\"\n</code></pre>"},{"location":"code/core/arrow_types/","title":"arrow_types","text":""},{"location":"code/core/arrow_types/#core.arrow_types","title":"<code>core.arrow_types</code>","text":""},{"location":"code/core/arrow_types/#core.arrow_types.BBoxType","title":"<code>BBoxType()</code>","text":"<p>         Bases: <code>pa.ExtensionType</code></p> <p>Bounding box type as PyArrow list of PyArrow float32</p> Source code in <code>pixano/core/arrow_types/features.py</code> <pre><code>def __init__(self):\n    super(BBoxType, self).__init__(pa.list_(pa.float32(), list_size=4), \"bbox\")\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.CompressedRLEType","title":"<code>CompressedRLEType()</code>","text":"<p>         Bases: <code>pa.ExtensionType</code></p> <p>Segmentation mask type as PyArrow StructType</p> Source code in <code>pixano/core/arrow_types/image.py</code> <pre><code>def __init__(self):\n    super(CompressedRLEType, self).__init__(\n        pa.struct(\n            [\n                pa.field(\"size\", pa.list_(pa.int32())),\n                pa.field(\"counts\", pa.binary()),\n            ]\n        ),\n        \"mask[rle]\",\n    )\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.Embedding","title":"<code>Embedding</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Embedding class</p> <p>Attributes:</p> Name Type Description <code>embedding</code> <code>bytes</code> <p>Embedding as binary</p>"},{"location":"code/core/arrow_types/#core.arrow_types.EmbeddingType","title":"<code>EmbeddingType()</code>","text":"<p>         Bases: <code>pa.ExtensionType</code></p> <p>Embedding type as PyArrow binary</p> Source code in <code>pixano/core/arrow_types/features.py</code> <pre><code>def __init__(self):\n    super(EmbeddingType, self).__init__(pa.binary(), \"embedding\")\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.Image","title":"<code>Image(uri, bytes=None, preview_bytes=None, uri_prefix=None)</code>","text":"<p>Image type using URI or bytes</p> <p>Attributes:</p> Name Type Description <code>_uri</code> <code>str</code> <p>Image URI</p> <code>_bytes</code> <code>bytes</code> <p>Image bytes</p> <code>_preview_bytes</code> <code>bytes</code> <p>Image preview bytes</p> <code>uri_prefix</code> <code>str</code> <p>URI prefix for relative URIs</p> <p>Attributes:</p> Name Type Description <code>uri</code> <code>str</code> <p>Image URI</p> <code>bytes</code> <code>bytes</code> <p>Image bytes. Defaults to None.</p> <code>preview_bytes</code> <code>bytes</code> <p>Image preview bytes. Defaults to None.</p> <code>uri_prefix</code> <code>str</code> <p>URI prefix for relative URIs. Defaults to None.</p> Source code in <code>pixano/core/arrow_types/image.py</code> <pre><code>def __init__(\n    self,\n    uri: str,\n    bytes: bytes = None,\n    preview_bytes: bytes = None,\n    uri_prefix: str = None,\n):\n\"\"\"Initialize image from URI or bytes\n\n    Attributes:\n        uri (str): Image URI\n        bytes (bytes, optional): Image bytes. Defaults to None.\n        preview_bytes (bytes, optional): Image preview bytes. Defaults to None.\n        uri_prefix (str, optional): URI prefix for relative URIs. Defaults to None.\n    \"\"\"\n\n    self._uri = uri\n    self._bytes = bytes\n    self._preview_bytes = preview_bytes\n    self.uri_prefix = uri_prefix\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.bytes","title":"<code>bytes: bytes</code>  <code>property</code>","text":"<p>Return image bytes</p> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Image bytes</p>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.preview_bytes","title":"<code>preview_bytes: bytes</code>  <code>property</code>","text":"<p>Return image preview bytes</p> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Image bytes</p>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.preview_url","title":"<code>preview_url: str</code>  <code>property</code>","text":"<p>Return image preview URL</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Image preview URL</p>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.size","title":"<code>size: list[int]</code>  <code>property</code>","text":"<p>Return image size</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: Image size</p>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.uri","title":"<code>uri: str</code>  <code>property</code>","text":"<p>Return image URI</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Image URI</p>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.url","title":"<code>url: str</code>  <code>property</code>","text":"<p>Return image URL</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Image URL</p>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.as_cv2","title":"<code>as_cv2()</code>","text":"<p>Open image as OpenCV</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: Image as OpenCV</p> Source code in <code>pixano/core/arrow_types/image.py</code> <pre><code>def as_cv2(self) -&gt; np.ndarray:\n\"\"\"Open image as OpenCV\n\n    Returns:\n        np.ndarray: Image as OpenCV\n    \"\"\"\n\n    im_arr = np.frombuffer(self.open().read(), dtype=np.uint8)\n    return cv2.imdecode(im_arr, cv2.IMREAD_COLOR)\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.as_pillow","title":"<code>as_pillow()</code>","text":"<p>Open image as Pillow</p> <p>Returns:</p> Type Description <code>PILImage.Image</code> <p>PIL.Image.Image: Image as Pillow</p> Source code in <code>pixano/core/arrow_types/image.py</code> <pre><code>def as_pillow(self) -&gt; PILImage.Image:\n\"\"\"Open image as Pillow\n\n    Returns:\n        PIL.Image.Image: Image as Pillow\n    \"\"\"\n\n    return PILImage.open(self.open()).convert(\"RGB\")\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.display","title":"<code>display(preview=False)</code>","text":"<p>Display image</p> <p>Parameters:</p> Name Type Description Default <code>preview</code> <code>bool</code> <p>True to display image preview instead of full image. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>IPyImage</code> <p>IPython.core.display.Image: Image as IPython Display</p> Source code in <code>pixano/core/arrow_types/image.py</code> <pre><code>def display(self, preview=False) -&gt; IPyImage:\n\"\"\"Display image\n\n    Args:\n        preview (bool, optional): True to display image preview instead of full image. Defaults to False.\n\n    Returns:\n        IPython.core.display.Image: Image as IPython Display\n    \"\"\"\n\n    im_bytes = self._preview_bytes if preview else self.bytes\n    return IPyImage(url=binary_to_url(im_bytes), format=IPyImage(im_bytes).format)\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.open","title":"<code>open()</code>","text":"<p>Open image</p> <p>Returns:</p> Name Type Description <code>IO</code> <code>IO</code> <p>Opened image</p> Source code in <code>pixano/core/arrow_types/image.py</code> <pre><code>def open(self) -&gt; IO:\n\"\"\"Open image\n\n    Returns:\n        IO: Opened image\n    \"\"\"\n\n    return urlopen(self.uri)\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.image.Image.to_dict","title":"<code>to_dict()</code>","text":"<p>Return image attributes as dict</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Image attributes</p> Source code in <code>pixano/core/arrow_types/image.py</code> <pre><code>def to_dict(self) -&gt; dict:\n\"\"\"Return image attributes as dict\n\n    Returns:\n        dict: Image attributes\n    \"\"\"\n\n    return {\n        \"uri\": self._uri,\n        \"bytes\": self._bytes,\n        \"preview_bytes\": self._preview_bytes,\n    }\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.ImageType","title":"<code>ImageType()</code>","text":"<p>         Bases: <code>pa.ExtensionType</code></p> <p>Externalized image type containing the URI string in UTF-8</p> Source code in <code>pixano/core/arrow_types/image.py</code> <pre><code>def __init__(self):\n    super(ImageType, self).__init__(\n        pa.struct(\n            [\n                pa.field(\"uri\", pa.utf8()),\n                pa.field(\"bytes\", pa.binary()),\n                pa.field(\"preview_bytes\", pa.binary()),\n            ]\n        ),\n        \"Image\",\n    )\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.ObjectAnnotation","title":"<code>ObjectAnnotation</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>ObjectAnnotation class to contain all annotation data</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Annotation unique ID</p> <code>view_id</code> <code>str</code> <p>View ID (e.g. 'image', 'cam_2')</p> <code>bbox</code> <code>list[float]</code> <p>Bounding box coordinates in xywh format (using top left point as reference)</p> <code>bbox_source</code> <code>str</code> <p>Bounding box source</p> <code>bbox_confidence</code> <code>float</code> <p>Bounding box confidence</p> <code>is_group_of</code> <code>bool</code> <p>is_group_of</p> <code>is_difficult</code> <code>bool</code> <p>is_difficult</p> <code>is_truncated</code> <code>bool</code> <p>is_truncated</p> <code>mask</code> <code>dict[str, bytes]</code> <p>Mask</p> <code>mask_source</code> <code>str</code> <p>Mask source</p> <code>area</code> <code>float</code> <p>area</p> <code>pose</code> <code>dict[str, list[float]]</code> <p>Pose</p> <code>category_id</code> <code>int</code> <p>Category ID</p> <code>category_name</code> <code>str</code> <p>Category name</p> <code>identity</code> <code>str</code> <p>Identity</p>"},{"location":"code/core/arrow_types/#core.arrow_types.ObjectAnnotationType","title":"<code>ObjectAnnotationType()</code>","text":"<p>ObjectAnnotation type as PyArrow StructType</p> <p>Returns:</p> Type Description <code>pa.StructType</code> <p>pa.StructType: ObjectAnnotation StructType</p> Source code in <code>pixano/core/arrow_types/features.py</code> <pre><code>def ObjectAnnotationType() -&gt; pa.StructType:\n\"\"\"ObjectAnnotation type as PyArrow StructType\n\n    Returns:\n        pa.StructType: ObjectAnnotation StructType\n    \"\"\"\n\n    pose_schema = pa.struct(\n        [\n            pa.field(\"cam_R_m2c\", pa.list_(pa.float64(), list_size=9)),\n            pa.field(\"cam_t_m2c\", pa.list_(pa.float64(), list_size=3)),\n        ]\n    )\n\n    return pa.struct(\n        [\n            # Object ID and View ID\n            pa.field(\"id\", pa.string()),\n            pa.field(\"view_id\", pa.string(), nullable=True),\n            # Bounding Box\n            pa.field(\"bbox\", BBoxType(), nullable=True),\n            pa.field(\"bbox_source\", pa.string(), nullable=True),\n            pa.field(\"bbox_confidence\", pa.float32(), nullable=True),\n            pa.field(\"is_group_of\", pa.bool_(), nullable=True),\n            pa.field(\"is_difficult\", pa.bool_(), nullable=True),\n            pa.field(\"is_truncated\", pa.bool_(), nullable=True),\n            # Mask\n            pa.field(\"mask\", CompressedRLEType(), nullable=True),\n            pa.field(\"mask_source\", pa.string(), nullable=True),\n            pa.field(\"area\", pa.float32(), nullable=True),\n            # 6D Poses\n            pa.field(\"pose\", pose_schema, nullable=True),\n            # Category\n            pa.field(\"category_id\", pa.int32(), nullable=True),\n            pa.field(\"category_name\", pa.string(), nullable=True),\n            pa.field(\"identity\", pa.string(), nullable=True),\n        ]\n    )\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.convert_field","title":"<code>convert_field(field_name, field_type, field_data)</code>","text":"<p>Convert PyArrow ExtensionTypes properly</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name</p> required <code>field_type</code> <code>pa.DataType</code> <p>Target PyArrow format</p> required <code>field_data</code> <code>list</code> <p>Data in Python format</p> required <p>Returns:</p> Type Description <code>pa.Array</code> <p>pa.Array: Data in target PyArrow format</p> Source code in <code>pixano/core/arrow_types/__init__.py</code> <pre><code>def convert_field(\n    field_name: str, field_type: pa.DataType, field_data: list\n) -&gt; pa.Array:\n\"\"\"Convert PyArrow ExtensionTypes properly\n\n    Args:\n        field_name (str): Name\n        field_type (pa.DataType): Target PyArrow format\n        field_data (list): Data in Python format\n\n    Returns:\n        pa.Array: Data in target PyArrow format\n    \"\"\"\n\n    # If target format is an ExtensionType\n    if isinstance(field_type, pa.ExtensionType):\n        storage = pa.array(field_data, type=field_type.storage_type)\n        return pa.ExtensionArray.from_storage(field_type, storage)\n\n    # If target format is an extension of ListType\n    elif pa.types.is_list(field_type):\n        native_arr = pa.array(field_data)\n        if isinstance(native_arr, pa.NullArray):\n            return pa.nulls(len(native_arr), field_type)\n        offsets = native_arr.offsets\n        values = native_arr.values.to_numpy(zero_copy_only=False)\n        return pa.ListArray.from_arrays(\n            offsets,\n            convert_field(f\"{field_name}.elements\", field_type.value_type, values),\n        )\n\n    # If target format is an extension of StructType\n    elif pa.types.is_struct(field_type):\n        native_arr = pa.array(field_data)\n        if isinstance(native_arr, pa.NullArray):\n            return pa.nulls(len(native_arr), field_type)\n        arrays = []\n        for subfield in field_type:\n            sub_arr = native_arr.field(subfield.name)\n            converted = convert_field(\n                f\"{field_name}.{subfield.name}\",\n                subfield.type,\n                sub_arr.to_numpy(zero_copy_only=False),\n            )\n            arrays.append(converted)\n        return pa.StructArray.from_arrays(arrays, fields=field_type)\n\n    # For other target formats\n    else:\n        return pa.array(field_data, type=field_type)\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.is_embedding_type","title":"<code>is_embedding_type(t)</code>","text":"<p>Returns True if value is an instance of EmbeddingType</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>pa.DataType</code> <p>Value to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Type checking response</p> Source code in <code>pixano/core/arrow_types/features.py</code> <pre><code>def is_embedding_type(t: pa.DataType) -&gt; bool:\n\"\"\"Returns True if value is an instance of EmbeddingType\n\n    Args:\n        t (pa.DataType): Value to check\n\n    Returns:\n        bool: Type checking response\n    \"\"\"\n    return isinstance(t, EmbeddingType)\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.is_image_type","title":"<code>is_image_type(t)</code>","text":"<p>Returns True if value is an instance of ImageType</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>pa.DataType</code> <p>Value to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Type checking response</p> Source code in <code>pixano/core/arrow_types/image.py</code> <pre><code>def is_image_type(t: pa.DataType) -&gt; bool:\n\"\"\"Returns True if value is an instance of ImageType\n\n    Args:\n        t (pa.DataType): Value to check\n\n    Returns:\n        bool: Type checking response\n    \"\"\"\n    return isinstance(t, ImageType)\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.is_number","title":"<code>is_number(t)</code>","text":"<p>Check if DataType is a a number (integer or float)</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>pa.DataType</code> <p>DataType to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if DataType is an integer or a float</p> Source code in <code>pixano/core/arrow_types/__init__.py</code> <pre><code>def is_number(t: pa.DataType) -&gt; bool:\n\"\"\"Check if DataType is a a number (integer or float)\n\n    Args:\n        t (pa.DataType): DataType to check\n\n    Returns:\n        bool: True if DataType is an integer or a float\n    \"\"\"\n\n    return pa.types.is_integer(t) or pa.types.is_floating(t)\n</code></pre>"},{"location":"code/core/arrow_types/#core.arrow_types.register_extension_types","title":"<code>register_extension_types()</code>","text":"<p>Register PyArrow ExtensionTypes</p> Source code in <code>pixano/core/arrow_types/__init__.py</code> <pre><code>def register_extension_types():\n\"\"\"Register PyArrow ExtensionTypes\"\"\"\n\n    types = [\n        BBoxType(),\n        CompressedRLEType(),\n        EmbeddingType(),\n        ImageType(),\n    ]\n\n    for t in types:\n        # Register ExtensionType\n        try:\n            pa.register_extension_type(t)\n        # If ExtensionType is already registered\n        except pa.ArrowKeyError:\n            pass\n</code></pre>"},{"location":"code/core/dataset/","title":"dataset","text":""},{"location":"code/core/dataset/#core.dataset","title":"<code>core.dataset</code>","text":""},{"location":"code/core/dataset/#core.dataset.Dataset","title":"<code>Dataset(path)</code>","text":"<p>Dataset class</p> <p>Attributes:</p> Name Type Description <code>_path</code> <code>Path</code> <p>Dataset path</p> <code>_info</code> <code>DatasetInfo</code> <p>Dataset info</p> <code>_partitioning</code> <code>ds.partitioning</code> <p>Dataset partitioning</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Dataset path</p> required Source code in <code>pixano/core/dataset.py</code> <pre><code>def __init__(self, path: Path):\n\"\"\"Initialize dataset\n\n    Args:\n        path (Path): Dataset path\n    \"\"\"\n\n    self._path = path\n    self._info = DatasetInfo.parse_file(self._path / \"spec.json\")\n</code></pre>"},{"location":"code/core/dataset/#core.dataset.DatasetInfo","title":"<code>DatasetInfo</code>","text":"<p>         Bases: <code>pydantic.BaseModel</code></p> <p>DatasetInfo</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Dataset ID</p> <code>name</code> <code>str</code> <p>Dataset name</p> <code>description</code> <code>str</code> <p>Dataset description</p> <code>num_elements</code> <code>int</code> <p>Number of elements in dataset</p> <code>preview</code> <code>str</code> <p>Dataset preview</p> <code>categories</code> <code>list[dict]</code> <p>Dataset categories</p>"},{"location":"code/core/dataset/#core.dataset.EmbeddingDataset","title":"<code>EmbeddingDataset(path)</code>","text":"<p>         Bases: <code>Dataset</code></p> <p>Embedding Dataset</p> <p>Attributes:</p> Name Type Description <code>_path</code> <code>Path</code> <p>Dataset path</p> <code>_info</code> <code>DatasetInfo</code> <p>Dataset info</p> <code>_partitioning</code> <code>ds.partitioning</code> <p>Dataset partitioning</p> Source code in <code>pixano/core/dataset.py</code> <pre><code>def __init__(self, path: Path):\n    self._path = path\n    self._info = DatasetInfo.parse_file(self._path / \"embed.json\")\n</code></pre>"},{"location":"code/core/dataset/#core.dataset.InferenceDataset","title":"<code>InferenceDataset(path)</code>","text":"<p>         Bases: <code>Dataset</code></p> <p>Inference Dataset</p> <p>Attributes:</p> Name Type Description <code>_path</code> <code>Path</code> <p>Dataset path</p> <code>_info</code> <code>DatasetInfo</code> <p>Dataset info</p> <code>_partitioning</code> <code>ds.partitioning</code> <p>Dataset partitioning</p> Source code in <code>pixano/core/dataset.py</code> <pre><code>def __init__(self, path: Path):\n    self._path = path\n    self._info = DatasetInfo.parse_file(self._path / \"infer.json\")\n</code></pre>"},{"location":"code/data/coco_loader/","title":"CocoLoader","text":""},{"location":"code/data/coco_loader/#data.COCOLoader","title":"<code>data.COCOLoader(name, description, splits)</code>","text":"<p>         Bases: <code>DataLoader</code></p> <p>Data Loader class for COCO instances dataset</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Dataset name</p> <code>description</code> <code>str</code> <p>Dataset description</p> <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> <code>schema</code> <code>pa.schema</code> <p>Dataset schema</p> <code>partitioning</code> <code>ds.partitioning</code> <p>Dataset partitioning</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name</p> required <code>description</code> <code>str</code> <p>Dataset description</p> required <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> required Source code in <code>pixano/data/coco_loader.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: str,\n    splits: list[str],\n):\n\"\"\"Initialize COCO Loader\n\n    Args:\n        name (str): Dataset name\n        description (str): Dataset description\n        splits (list[str]): Dataset splits\n    \"\"\"\n\n    # Dataset views\n    views = [pa.field(\"image\", arrow_types.ImageType())]\n\n    # Initialize Data Loader\n    super().__init__(name, description, splits, views)\n</code></pre>"},{"location":"code/data/coco_loader/#data.coco_loader.COCOLoader.export_dataset","title":"<code>export_dataset(input_dir, export_dir, input_dbs=[], portable=False)</code>","text":"<p>Export dataset back to original format</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>Path</code> <p>Input directory</p> required <code>export_dir</code> <code>Path</code> <p>Export directory</p> required <code>input_dbs</code> <code>list[str]</code> <p>Input databases to export, all if []. Defaults to [].</p> <code>[]</code> <code>portable</code> <code>bool</code> <p>True to export dataset portable media files. Defaults to False.</p> <code>False</code> Source code in <code>pixano/data/coco_loader.py</code> <pre><code>def export_dataset(\n    self,\n    input_dir: Path,\n    export_dir: Path,\n    input_dbs: list = [],\n    portable: bool = False,\n):\n\"\"\"Export dataset back to original format\n\n    Args:\n        input_dir (Path): Input directory\n        export_dir (Path): Export directory\n        input_dbs (list[str]): Input databases to export, all if []. Defaults to [].\n        portable (bool, optional): True to export dataset portable media files. Defaults to False.\n    \"\"\"\n\n    # Load spec.json\n    input_info = DatasetInfo.parse_file(input_dir / \"spec.json\")\n\n    # Create URI prefix\n    media_dir = input_dir / \"media\"\n    uri_prefix = media_dir.absolute().as_uri()\n    export_uri_prefix = (export_dir / \"media\").absolute().as_uri()\n\n    # If no splits provided, select all splits\n    if not self.splits:\n        splits = [s.name for s in os.scandir(input_dir / \"db\") if s.is_dir()]\n    # Else, format provided splits\n    else:\n        splits = [\n            f\"split={s}\" if not s.startswith(\"split=\") else s for s in self.splits\n        ]\n    # Check if the splits exist\n    for split in splits:\n        split_dir = input_dir / \"db\" / split\n        if not split_dir.exists():\n            raise Exception(f\"{split_dir} does not exist.\")\n        if not any(split_dir.iterdir()):\n            raise Exception(f\"{split_dir} is empty.\")\n\n    # If no input databases provided, select all input databases\n    if not input_dbs:\n        input_dbs = [\"db\"]\n        for inf_json in sorted(list(input_dir.glob(\"db_infer_*/infer.json\"))):\n            input_dbs.append(inf_json.parent.name)\n    # Else, format provided inference datasets\n    else:\n        input_dbs = [\n            f\"db_infer_{i}\" if not i.startswith(\"db\") else i for i in input_dbs\n        ]\n\n    # Check if the datasets exist\n    for ds in input_dbs:\n        ds_dir = input_dir / ds\n        if not ds_dir.exists():\n            raise Exception(f\"{ds_dir} does not exist.\")\n        if not any(ds_dir.iterdir()):\n            raise Exception(f\"{ds_dir} is empty.\")\n\n    # Create export directory\n    ann_dir = export_dir / f\"annotations_[{','.join(input_dbs)}]\"\n    ann_dir.mkdir(parents=True, exist_ok=True)\n\n    # Iterate on splits\n    for split in splits:\n        # List split files\n        files = (input_dir / \"db\" / split).glob(\"*.parquet\")\n        files = sorted(files, key=lambda x: natural_key(x.name))\n        split_name = split.replace(\"split=\", \"\")\n\n        # Create COCO json\n        coco_json = {\n            \"info\": {\n                \"description\": input_info.name,\n                \"url\": \"N/A\",\n                \"version\": f\"v{datetime.datetime.now().strftime('%y%m%d.%H%M%S')}\",\n                \"year\": datetime.date.today().year,\n                \"contributor\": \"Exported from Pixano\",\n                \"date_created\": datetime.date.today().isoformat(),\n            },\n            \"licences\": [\n                {\n                    \"url\": \"N/A\",\n                    \"id\": 1,\n                    \"name\": \"Unknown\",\n                },\n            ],\n            \"images\": [],\n            \"annotations\": [],\n            \"categories\": [],\n        }\n\n        # Iterate on files\n        for file in tqdm(files, desc=f\"Processing {split_name} split\", position=0):\n            seen_category_ids = [None]\n\n            # Load media table\n            media_fields = [\n                field.name\n                for field in self.schema\n                if arrow_types.is_image_type(field.type)\n            ]\n            media_table = pq.read_table(file).select([\"id\"] + media_fields)\n\n            # Load annotation tables\n            ann_files = [input_dir / ds / split / file.name for ds in input_dbs]\n            ann_tables = [pq.read_table(f).select([\"objects\"]) for f in ann_files]\n\n            # Iterate on rows\n            for row in tqdm(\n                range(media_table.num_rows),\n                desc=f\"Processing {file.name}\",\n                position=1,\n            ):\n                media_row = media_table.take([row])\n                ann_rows = [ann_table.take([row]) for ann_table in ann_tables]\n                images = {}\n\n                for field in media_fields:\n                    # Open image\n                    images[field] = media_row[field][0].as_py(uri_prefix)\n                    im_uri = (\n                        media_row[field][0].as_py(export_uri_prefix).uri\n                        if portable\n                        else images[field].uri\n                    )\n                    im_filename = Path(urlparse(im_uri).path).name\n                    im_w, im_h = images[field].size\n                    # Append image info\n                    coco_json[\"images\"].append(\n                        {\n                            \"license\": 1,\n                            \"coco_url\": im_uri,\n                            \"file_name\": im_filename,\n                            \"height\": im_h,\n                            \"width\": im_w,\n                            \"id\": media_row[\"id\"][0].as_py(),\n                        }\n                    )\n\n                for ann_row in ann_rows:\n                    anns = ann_row[\"objects\"][0].as_py()\n\n                    for ann in anns:\n                        # Append annotation\n                        im_w, im_h = images[ann[\"view_id\"]].size\n                        urle = rle_to_urle(ann[\"mask\"])\n                        bbox = (\n                            ann[\"bbox\"]\n                            if ann[\"bbox\"] != [0.0, 0.0, 0.0, 0.0]\n                            else urle_to_bbox(urle)\n                        )\n                        coco_json[\"annotations\"].append(\n                            {\n                                \"segmentation\": urle,\n                                \"area\": ann[\"area\"],\n                                \"iscrowd\": 0,\n                                \"image_id\": media_row[\"id\"][0].as_py(),\n                                \"bbox\": denormalize(bbox, im_h, im_w),\n                                \"category_id\": ann[\"category_id\"],\n                                \"category_name\": ann[\"category_name\"],\n                                \"id\": ann[\"id\"],\n                            }\n                        )\n                        # Append category if not seen yet\n                        if (\n                            ann[\"category_id\"] not in seen_category_ids\n                            and ann[\"category_name\"] is not None\n                        ):\n                            coco_json[\"categories\"].append(\n                                {\n                                    \"supercategory\": \"N/A\",\n                                    \"id\": ann[\"category_id\"],\n                                    \"name\": ann[\"category_name\"],\n                                },\n                            )\n                            seen_category_ids.append(ann[\"category_id\"])\n\n        # Sort categories\n        coco_json[\"categories\"] = sorted(\n            coco_json[\"categories\"], key=lambda c: c[\"id\"]\n        )\n\n        # Save COCO json\n        with open(ann_dir / f\"instances_{split_name}.json\", \"w\") as f:\n            json.dump(coco_json, f)\n\n        # Move media directory if portable and directory exists\n        if portable:\n            if media_dir.exists():\n                media_dir.rename(export_dir / \"media\")\n            else:\n                raise Exception(\n                    f\"Activated portable option for export but {media_dir} does not exist.\"\n                )\n</code></pre>"},{"location":"code/data/coco_loader/#data.coco_loader.COCOLoader.import_row","title":"<code>import_row(input_dirs, split, portable=False)</code>","text":"<p>Process dataset row for import</p> <p>Parameters:</p> Name Type Description Default <code>input_dirs</code> <code>dict[str, Path]</code> <p>Input directories</p> required <code>split</code> <code>str</code> <p>Dataset split</p> required <code>portable</code> <code>bool</code> <p>True to move or download media files inside dataset. Defaults to False.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>Processed rows</p> Source code in <code>pixano/data/coco_loader.py</code> <pre><code>def import_row(\n    self,\n    input_dirs: dict[str, Path],\n    split: str,\n    portable: bool = False,\n) -&gt; Iterator:\n\"\"\"Process dataset row for import\n\n    Args:\n        input_dirs (dict[str, Path]): Input directories\n        split (str): Dataset split\n        portable (bool, optional): True to move or download media files inside dataset. Defaults to False.\n\n    Yields:\n        Iterator: Processed rows\n    \"\"\"\n\n    # Open annotation files\n    with open(input_dirs[\"objects\"] / f\"instances_{split}.json\", \"r\") as f:\n        coco_instances = json.load(f)\n\n    # Group annotations by image ID\n    annotations = defaultdict(list)\n    for ann in coco_instances[\"annotations\"]:\n        annotations[ann[\"image_id\"]].append(ann)\n\n    # Process rows\n    for im in sorted(coco_instances[\"images\"], key=lambda x: natural_key(x[\"id\"])):\n        # Load image annotations\n        im_anns = annotations[im[\"id\"]]\n        # Load image\n        file_name_uri = urlparse(im[\"file_name\"])\n        if file_name_uri.scheme == \"\":\n            im_path = input_dirs[\"image\"] / split / im[\"file_name\"]\n        else:\n            im_path = Path(file_name_uri.path)\n\n        # Create image thumbnail\n        im_thumb = image_to_thumbnail(im_path.read_bytes())\n\n        # Set image URI\n        im_uri = (\n            f\"image/{split}/{im_path.name}\"\n            if portable\n            else im_path.absolute().as_uri()\n        )\n\n        # Fill row with ID, image, and list of image annotations\n        row = {\n            \"id\": str(im[\"id\"]),\n            \"image\": arrow_types.Image(im_uri, None, im_thumb).to_dict(),\n            \"objects\": [\n                arrow_types.ObjectAnnotation(\n                    id=str(ann[\"id\"]),\n                    view_id=\"image\",\n                    area=float(ann[\"area\"]) if ann[\"area\"] else None,\n                    bbox=normalize(ann[\"bbox\"], im[\"height\"], im[\"width\"]),\n                    mask=encode_rle(ann[\"segmentation\"], im[\"height\"], im[\"width\"]),\n                    is_group_of=bool(ann[\"iscrowd\"]) if ann[\"iscrowd\"] else None,\n                    category_id=int(ann[\"category_id\"]),\n                    category_name=coco_names_91(ann[\"category_id\"]),\n                ).dict()\n                for ann in im_anns\n            ],\n            \"split\": split,\n        }\n\n        # Return row\n        yield row\n</code></pre>"},{"location":"code/data/data_loader/","title":"DataLoader","text":""},{"location":"code/data/data_loader/#data.DataLoader","title":"<code>data.DataLoader(name, description, splits, views)</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract Data Loader class</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Dataset name</p> <code>description</code> <code>str</code> <p>Dataset description</p> <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> <code>schema</code> <code>pa.schema</code> <p>Dataset schema</p> <code>partitioning</code> <code>ds.partitioning</code> <p>Dataset partitioning</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name</p> required <code>description</code> <code>str</code> <p>Dataset description</p> required <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> required <code>views</code> <code>list[pa.field]</code> <p>Dataset views</p> required Source code in <code>pixano/data/data_loader.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: str,\n    splits: list[str],\n    views: list[pa.field],\n):\n\"\"\"Initialize Data Loader\n\n    Args:\n        name (str): Dataset name\n        description (str): Dataset description\n        splits (list[str]): Dataset splits\n        views (list[pa.field]): Dataset views\n    \"\"\"\n\n    # Dataset info\n    self.info = DatasetInfo(\n        id=shortuuid.uuid(),\n        name=name,\n        description=description,\n        num_elements=0,\n        preview=None,\n        categories=[],\n    )\n    self.splits = splits\n\n    # Dataset schema\n    fields = [\n        pa.field(\"split\", pa.string()),\n        pa.field(\"id\", pa.string()),\n        pa.field(\"objects\", pa.list_(arrow_types.ObjectAnnotationType())),\n    ]\n    fields.extend(views)\n    self.schema = pa.schema(fields)\n    self.partitioning = ds.partitioning(\n        pa.schema([(\"split\", pa.string())]), flavor=\"hive\"\n    )\n</code></pre>"},{"location":"code/data/data_loader/#data.data_loader.DataLoader.create_json","title":"<code>create_json(import_dir, categories=[])</code>","text":"<p>Create dataset spec.json</p> <p>Parameters:</p> Name Type Description Default <code>import_dir</code> <code>Path</code> <p>Import directory</p> required <code>categories</code> <code>list[dict]</code> <p>Dataset categories. Defaults to [].</p> <code>[]</code> Source code in <code>pixano/data/data_loader.py</code> <pre><code>def create_json(self, import_dir: Path, categories: list[dict] = []):\n\"\"\"Create dataset spec.json\n\n    Args:\n        import_dir (Path): Import directory\n        categories (list[dict], optional): Dataset categories. Defaults to [].\n    \"\"\"\n\n    with tqdm(desc=\"Creating dataset info file\", total=1) as progress:\n        # Read dataset\n        dataset = ds.dataset(import_dir / \"db\", partitioning=self.partitioning)\n\n        # Check number of rows in the created dataset\n        self.info.num_elements = dataset.count_rows()\n\n        # Add categories\n        if categories:\n            self.info.categories = categories\n\n        # Create spec.json\n        with open(import_dir / \"spec.json\", \"w\") as f:\n            json.dump(vars(self.info), f)\n        progress.update(1)\n</code></pre>"},{"location":"code/data/data_loader/#data.data_loader.DataLoader.create_preview","title":"<code>create_preview(import_dir)</code>","text":"<p>Create dataset preview image</p> <p>Parameters:</p> Name Type Description Default <code>import_dir</code> <code>Path</code> <p>Import directory</p> required Source code in <code>pixano/data/data_loader.py</code> <pre><code>def create_preview(self, import_dir: Path):\n\"\"\"Create dataset preview image\n\n    Args:\n        import_dir (Path): Import directory\n    \"\"\"\n\n    # Read dataset\n    dataset = ds.dataset(import_dir / \"db\", partitioning=self.partitioning)\n\n    # Get list of image fields\n    image_fields = []\n    for field in self.schema:\n        if arrow_types.is_image_type(field.type):\n            image_fields.append(field.name)\n\n    if image_fields:\n        with tqdm(desc=\"Creating dataset thumbnail\", total=1) as progress:\n            tile_w = 64\n            tile_h = 64\n            preview = Image.new(\"RGB\", (3 * tile_w, 2 * tile_h))\n            for i in range(6):\n                field = image_fields[i % len(image_fields)]\n                row_number = random.randrange(dataset.count_rows())\n                row = dataset.take([row_number]).to_pylist()[0]\n                with Image.open(BytesIO(row[field]._preview_bytes)) as im:\n                    preview.paste(im, ((i % 3) * tile_w, (int(i / 3) % 2) * tile_h))\n            preview.save(import_dir / \"preview.png\")\n            progress.update(1)\n</code></pre>"},{"location":"code/data/data_loader/#data.data_loader.DataLoader.create_stats","title":"<code>create_stats(import_dir)</code>","text":"<p>Create dataset statistics</p> <p>Parameters:</p> Name Type Description Default <code>import_dir</code> <code>Path</code> <p>Import directory</p> required Source code in <code>pixano/data/data_loader.py</code> <pre><code>def create_stats(self, import_dir: Path):\n\"\"\"Create dataset statistics\n\n    Args:\n        import_dir (Path): Import directory\n    \"\"\"\n\n    # Reset json file\n    open(import_dir / \"db_feature_statistics.json\", \"w\").close()\n    # Create objects stats\n    self.objects_stats(import_dir)\n    # Create image stats\n    self.image_stats(import_dir)\n</code></pre>"},{"location":"code/data/data_loader/#data.data_loader.DataLoader.export_dataset","title":"<code>export_dataset(input_dir, export_dir)</code>","text":"<p>Export dataset back to original format</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>Path</code> <p>Input directory</p> required <code>export_dir</code> <code>Path</code> <p>Export directory</p> required Source code in <code>pixano/data/data_loader.py</code> <pre><code>def export_dataset(self, input_dir: Path, export_dir: Path):\n\"\"\"Export dataset back to original format\n\n    Args:\n        input_dir (Path): Input directory\n        export_dir (Path): Export directory\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"code/data/data_loader/#data.data_loader.DataLoader.image_stats","title":"<code>image_stats(import_dir)</code>","text":"<p>Create dataset image statistics</p> <p>Parameters:</p> Name Type Description Default <code>import_dir</code> <code>Path</code> <p>Import directory</p> required Source code in <code>pixano/data/data_loader.py</code> <pre><code>def image_stats(self, import_dir: Path):\n\"\"\"Create dataset image statistics\n\n    Args:\n        import_dir (Path): Import directory\n    \"\"\"\n\n    # Read dataset\n    dataset = ds.dataset(import_dir / \"db\", partitioning=self.partitioning)\n\n    # Create URI prefix\n    media_dir = import_dir / \"media\"\n    uri_prefix = media_dir.absolute().as_uri()\n\n    # Iterate over dataset columns\n    for field in dataset.schema:\n        # If column has images\n        if arrow_types.is_image_type(field.type):\n            features = []\n            rows = dataset.to_batches(columns=[field.name, \"split\"], batch_size=1)\n\n            # Get features\n            for row in tqdm(\n                rows,\n                desc=f\"Computing {field.name} stats\",\n                total=dataset.count_rows(),\n            ):\n                # Open image\n                im = row[field.name][0].as_py(uri_prefix)\n                im_w, im_h = im.size\n                # Compute image features\n                aspect_ratio = round(im_w / im_h, 1)\n                features.append(\n                    {\n                        f\"{field.name} - aspect ratio\": aspect_ratio,\n                        \"split\": row[\"split\"][0].as_py(),\n                    }\n                )\n            features_df = pd.DataFrame.from_records(features).astype(\n                {\n                    f\"{field.name} - aspect ratio\": \"float\",\n                    \"split\": \"string\",\n                }\n            )\n\n            # Initialize stats\n            stats = [\n                {\n                    \"name\": f\"{field.name} - aspect ratio\",\n                    \"type\": \"numerical\",\n                    \"histogram\": [],\n                    \"range\": [\n                        features_df[f\"{field.name} - aspect ratio\"].min(),\n                        features_df[f\"{field.name} - aspect ratio\"].max(),\n                    ],\n                },\n            ]\n\n            # Save stats\n            self.save_stats(import_dir, stats, features_df)\n</code></pre>"},{"location":"code/data/data_loader/#data.data_loader.DataLoader.import_dataset","title":"<code>import_dataset(input_dirs, import_dir, portable=False, batch_size=2048)</code>","text":"<p>Import dataset to Pixano format</p> <p>Parameters:</p> Name Type Description Default <code>input_dirs</code> <code>dict[str, Path]</code> <p>Input directories</p> required <code>import_dir</code> <code>Path</code> <p>Import directory</p> required <code>portable</code> <code>int</code> <p>True to move or download files inside import directory. Defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Number of rows per file. Defaults to 2048.</p> <code>2048</code> Source code in <code>pixano/data/data_loader.py</code> <pre><code>def import_dataset(\n    self,\n    input_dirs: dict[str, Path],\n    import_dir: Path,\n    portable: bool = False,\n    batch_size: int = 2048,\n):\n\"\"\"Import dataset to Pixano format\n\n    Args:\n        input_dirs (dict[str, Path]): Input directories\n        import_dir (Path): Import directory\n        portable (int, optional): True to move or download files inside import directory. Defaults to False.\n        batch_size (int, optional): Number of rows per file. Defaults to 2048.\n    \"\"\"\n\n    # Check input directories\n    for source_path in input_dirs.values():\n        if not source_path.exists():\n            raise Exception(f\"{source_path} does not exist.\")\n        if not any(source_path.iterdir()):\n            raise Exception(f\"{source_path} is empty.\")\n\n    # Dataset categories\n    categories = []\n    seen_category_ids = [None]\n\n    # Iterate on splits\n    for split in self.splits:\n        batches = _batch_dict(\n            self.import_row(input_dirs, split, portable), batch_size\n        )\n        # Iterate on batches\n        for i, batch in tqdm(enumerate(batches), desc=f\"Converting {split} split\"):\n            # Append batch categories\n            for field in self.schema:\n                # If column has annotations\n                # TODO: Change to checking type when ObjectAnnotationType is rebuilt\n                if field.name == \"objects\":\n                    for row in batch[field.name]:\n                        for ann in row:\n                            if (\n                                ann[\"category_id\"] not in seen_category_ids\n                                and ann[\"category_name\"] is not None\n                            ):\n                                categories.append(\n                                    {\n                                        \"supercategory\": \"N/A\",\n                                        \"id\": ann[\"category_id\"],\n                                        \"name\": ann[\"category_name\"],\n                                    },\n                                )\n                                seen_category_ids.append(ann[\"category_id\"])\n\n            # Convert batch fields to PyArrow format\n            arrays = []\n            for field in self.schema:\n                arrays.append(\n                    arrow_types.convert_field(\n                        field_name=field.name,\n                        field_type=field.type,\n                        field_data=batch[field.name],\n                    )\n                )\n            # Save to file\n            ds.write_dataset(\n                data=pa.Table.from_arrays(arrays, schema=self.schema),\n                base_dir=import_dir / \"db\",\n                basename_template=f\"part-{{i}}-{i}.parquet\",\n                format=\"parquet\",\n                existing_data_behavior=\"overwrite_or_ignore\",\n                partitioning=self.partitioning,\n            )\n\n    # Sort categories\n    categories = sorted(categories, key=lambda c: c[\"id\"])\n\n    # Create spec.json\n    self.create_json(import_dir, categories)\n\n    # Create preview.png\n    self.create_preview(import_dir)\n\n    # Move media directories if portable\n    if portable:\n        for field in tqdm(self.schema, desc=f\"Moving media directories\"):\n            if arrow_types.is_image_type(field.type):\n                field_dir = import_dir / \"media\" / field.name\n                field_dir.mkdir(parents=True, exist_ok=True)\n                input_dirs[field.name].rename(field_dir)\n\n    # Create stats\n    self.create_stats(import_dir)\n</code></pre>"},{"location":"code/data/data_loader/#data.data_loader.DataLoader.import_row","title":"<code>import_row(input_dirs, split, portable=False)</code>  <code>abstractmethod</code>","text":"<p>Process dataset row for import</p> <p>Parameters:</p> Name Type Description Default <code>input_dirs</code> <code>dict[str, Path]</code> <p>Input directories</p> required <code>split</code> <code>str</code> <p>Dataset split</p> required <code>portable</code> <code>bool</code> <p>True to move or download media files inside dataset. Defaults to False.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>Processed rows</p> Source code in <code>pixano/data/data_loader.py</code> <pre><code>@abstractmethod\ndef import_row(\n    self,\n    input_dirs: dict[str, Path],\n    split: str,\n    portable: bool = False,\n) -&gt; Iterator:\n\"\"\"Process dataset row for import\n\n    Args:\n        input_dirs (dict[str, Path]): Input directories\n        split (str): Dataset split\n        portable (bool, optional): True to move or download media files inside dataset. Defaults to False.\n\n    Yields:\n        Iterator: Processed rows\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"code/data/data_loader/#data.data_loader.DataLoader.objects_stats","title":"<code>objects_stats(import_dir)</code>","text":"<p>Create dataset objects statistics</p> <p>Parameters:</p> Name Type Description Default <code>import_dir</code> <code>Path</code> <p>Import directory</p> required Source code in <code>pixano/data/data_loader.py</code> <pre><code>def objects_stats(self, import_dir: Path):\n\"\"\"Create dataset objects statistics\n\n    Args:\n        import_dir (Path): Import directory\n    \"\"\"\n\n    # Read dataset\n    dataset = ds.dataset(import_dir / \"db\", partitioning=self.partitioning)\n\n    # Create stats if objects field exist\n    objects = pa.field(\"objects\", pa.list_(arrow_types.ObjectAnnotationType()))\n    if objects in self.schema:\n        # Create dataframe\n        df = dataset.to_table(columns=[\"split\", \"objects\"]).to_pandas()\n        # Split objects in one object per row\n        df = df.explode(\"objects\")\n        # Remove empty objects\n        df = df[df[\"objects\"].notnull()]\n\n        # Get features\n        features = []\n        for split, object in tqdm(\n            zip(df[\"split\"], df[\"objects\"]),\n            desc=\"Computing objects stats\",\n            total=len(df.index),\n        ):\n            try:\n                area = 100 * (object[\"area\"] / np.prod(object[\"mask\"][\"size\"]))\n            except TypeError:\n                area = None\n            features.append(\n                {\n                    \"id\": object[\"id\"],\n                    \"view id\": object[\"view_id\"],\n                    \"objects - is group of\": object[\"is_group_of\"],\n                    \"objects - area (%)\": area,\n                    \"objects - category\": object[\"category_name\"],\n                    \"split\": split,\n                }\n            )\n        features_df = pd.DataFrame.from_records(features).astype(\n            {\n                \"id\": \"string\",\n                \"view id\": \"string\",\n                \"objects - is group of\": bool,\n                \"objects - area (%)\": float,\n                \"objects - category\": \"string\",\n                \"split\": \"string\",\n            }\n        )\n\n        # Initialize stats\n        stats = [\n            {\n                \"name\": \"objects - category\",\n                \"type\": \"categorical\",\n                \"histogram\": [],\n            },\n            {\n                \"name\": \"objects - is group of\",\n                \"type\": \"categorical\",\n                \"histogram\": [],\n            },\n            {\n                \"name\": \"objects - area (%)\",\n                \"type\": \"numerical\",\n                \"range\": [0.0, 100.0],\n                \"histogram\": [],\n            },\n        ]\n\n        # Save stats\n        self.save_stats(import_dir, stats, features_df)\n</code></pre>"},{"location":"code/data/data_loader/#data.data_loader.DataLoader.save_stats","title":"<code>save_stats(import_dir, stats, df)</code>","text":"<p>Compute and save stats to json</p> <p>Parameters:</p> Name Type Description Default <code>import_dir</code> <code>Path</code> <p>Import directory</p> required <code>stats</code> <code>list[dict]</code> <p>List of stats to save</p> required <code>df</code> <code>pd.DataFrame</code> <p>DataFrame to create stats from</p> required Source code in <code>pixano/data/data_loader.py</code> <pre><code>def save_stats(self, import_dir: Path, stats: list[dict], df: pd.DataFrame):\n\"\"\"Compute and save stats to json\n\n    Args:\n        import_dir (Path): Import directory\n        stats (list[dict]): List of stats to save\n        df (pd.DataFrame): DataFrame to create stats from\n    \"\"\"\n\n    # Compute stats\n    for split in self.splits:\n        for stat in stats:\n            split_df = df[df[\"split\"] == split]\n            stat[\"histogram\"].extend(compute_stats(split_df, split, stat))\n\n    # Check for existing db_feature_statistics.json\n    with open(import_dir / \"db_feature_statistics.json\", \"r\") as f:\n        try:\n            stat_json = json.load(f)\n            stat_json.extend(stats)\n        except ValueError:\n            stat_json = stats\n\n    # Add to db_feature_statistics.json\n    with open(import_dir / \"db_feature_statistics.json\", \"w\") as f:\n        json.dump(stat_json, f)\n</code></pre>"},{"location":"code/data/dota_loader/","title":"DOTALoader","text":""},{"location":"code/data/dota_loader/#data.DOTALoader","title":"<code>data.DOTALoader(name, description, splits)</code>","text":"<p>         Bases: <code>DataLoader</code></p> <p>Data Loader class for DOTA dataset</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Dataset name</p> <code>description</code> <code>str</code> <p>Dataset description</p> <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> <code>schema</code> <code>pa.schema</code> <p>Dataset schema</p> <code>partitioning</code> <code>ds.partitioning</code> <p>Dataset partitioning</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name</p> required <code>description</code> <code>str</code> <p>Dataset description</p> required <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> required Source code in <code>pixano/data/dota_loader.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: str,\n    splits: list[str],\n):\n\"\"\"Initialize COCO Loader\n\n    Args:\n        name (str): Dataset name\n        description (str): Dataset description\n        splits (list[str]): Dataset splits\n    \"\"\"\n\n    # Dataset views\n    views = [pa.field(\"image\", arrow_types.ImageType())]\n\n    # Initialize Data Loader\n    super().__init__(name, description, splits, views)\n</code></pre>"},{"location":"code/data/dota_loader/#data.dota_loader.DOTALoader.import_row","title":"<code>import_row(input_dirs, split, portable=False)</code>","text":"<p>Process dataset row for import</p> <p>Parameters:</p> Name Type Description Default <code>input_dirs</code> <code>dict[str, Path]</code> <p>Input directories</p> required <code>split</code> <code>str</code> <p>Dataset split</p> required <code>portable</code> <code>bool</code> <p>True to move or download media files inside dataset. Defaults to False.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>Processed rows</p> Source code in <code>pixano/data/dota_loader.py</code> <pre><code>def import_row(\n    self,\n    input_dirs: dict[str, Path],\n    split: str,\n    portable: bool = False,\n) -&gt; Iterator:\n\"\"\"Process dataset row for import\n\n    Args:\n        input_dirs (dict[str, Path]): Input directories\n        split (str): Dataset split\n        portable (bool, optional): True to move or download media files inside dataset. Defaults to False.\n\n    Yields:\n        Iterator: Processed rows\n    \"\"\"\n\n    # Get images paths\n    image_paths = glob.glob(str(input_dirs[\"image\"] / split / \"*.png\"))\n    image_paths = [Path(p) for p in sorted(image_paths, key=natural_key)]\n\n    # Process rows\n    for im_path in image_paths:\n        # Load image annotations\n        im_anns_file = (\n            input_dirs[\"objects\"]\n            / split\n            / \"hbb\"\n            / im_path.name.replace(\"png\", \"txt\")\n        )\n\n        # Allow DOTA largest images\n        Image.MAX_IMAGE_PIXELS = 806504000\n\n        # Get image dimensions and thumbnail\n        with Image.open(im_path) as im:\n            im_w, im_h = im.size\n            im_thumb = image_to_thumbnail(im)\n\n        # Set image URI\n        im_uri = (\n            f\"image/{split}/{im_path.name}\"\n            if portable\n            else im_path.absolute().as_uri()\n        )\n\n        # Fill row with ID, image, and list of image annotations\n        with open(im_anns_file) as im_anns:\n            row = {\n                \"id\": im_path.stem,\n                \"image\": arrow_types.Image(im_uri, None, im_thumb).to_dict(),\n                \"objects\": [\n                    arrow_types.ObjectAnnotation(\n                        id=shortuuid.uuid(),\n                        view_id=\"image\",\n                        bbox=normalize(\n                            xyxy_to_xywh(\n                                [\n                                    float(line.strip().split()[0]),\n                                    float(line.strip().split()[1]),\n                                    float(line.strip().split()[4]),\n                                    float(line.strip().split()[5]),\n                                ]\n                            ),\n                            im_h,\n                            im_w,\n                        ),\n                        is_difficult=bool(line.strip().split()[9]),\n                        category_id=dota_ids(str(line.strip().split()[8])),\n                        category_name=str(line.strip().split()[8]).replace(\n                            \"-\", \" \"\n                        ),\n                    ).dict()\n                    for line in im_anns\n                ],\n                \"split\": split,\n            }\n\n        # Return row\n        yield row\n</code></pre>"},{"location":"code/data/image_loader/","title":"ImageLoader","text":""},{"location":"code/data/image_loader/#data.ImageLoader","title":"<code>data.ImageLoader(name, description, splits)</code>","text":"<p>         Bases: <code>DataLoader</code></p> <p>Data Loader class for demo datasets</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Dataset name</p> <code>description</code> <code>str</code> <p>Dataset description</p> <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> <code>schema</code> <code>pa.schema</code> <p>Dataset schema</p> <code>partitioning</code> <code>ds.partitioning</code> <p>Dataset partitioning</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name</p> required <code>description</code> <code>str</code> <p>Dataset description</p> required <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> required Source code in <code>pixano/data/image_loader.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: str,\n    splits: list[str],\n):\n\"\"\"Initialize COCO Loader\n\n    Args:\n        name (str): Dataset name\n        description (str): Dataset description\n        splits (list[str]): Dataset splits\n    \"\"\"\n\n    # Dataset views\n    views = [pa.field(\"image\", arrow_types.ImageType())]\n\n    # Initialize Data Loader\n    super().__init__(name, description, splits, views)\n</code></pre>"},{"location":"code/data/image_loader/#data.image_loader.ImageLoader.import_row","title":"<code>import_row(input_dirs, split, portable=False)</code>","text":"<p>Process dataset row for import</p> <p>Parameters:</p> Name Type Description Default <code>input_dirs</code> <code>dict[str, Path]</code> <p>Input directories</p> required <code>split</code> <code>str</code> <p>Dataset split</p> required <code>portable</code> <code>bool</code> <p>True to move or download media files inside dataset. Defaults to False.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>Processed rows</p> Source code in <code>pixano/data/image_loader.py</code> <pre><code>def import_row(\n    self,\n    input_dirs: dict[str, Path],\n    split: str,\n    portable: bool = False,\n) -&gt; Iterator:\n\"\"\"Process dataset row for import\n\n    Args:\n        input_dirs (dict[str, Path]): Input directories\n        split (str): Dataset split\n        portable (bool, optional): True to move or download media files inside dataset. Defaults to False.\n\n    Yields:\n        Iterator: Processed rows\n    \"\"\"\n\n    # Get images paths\n    image_paths = []\n    for type in [\"*.png\", \"*.jpg\", \"*.jpeg\"]:\n        image_paths.extend(glob.glob(str(input_dirs[\"image\"] / split / type)))\n    image_paths = [Path(p) for p in sorted(image_paths, key=natural_key)]\n\n    # Process rows\n    for im_path in image_paths:\n        # Create image thumbnail\n        im_thumb = image_to_thumbnail(im_path.read_bytes())\n\n        # Set image URI\n        im_uri = (\n            f\"image/{split}/{im_path.name}\"\n            if portable\n            else im_path.absolute().as_uri()\n        )\n\n        # Fill row with ID, image, and list of image annotations\n        row = {\n            \"id\": im_path.name,\n            \"image\": arrow_types.Image(im_uri, None, im_thumb).to_dict(),\n            # TODO: find a way to return an empty list\n            \"objects\": [\n                arrow_types.ObjectAnnotation(\n                    id=shortuuid.uuid(),\n                    category_id=0,\n                ).dict()\n            ],\n            \"split\": split,\n        }\n\n        # Return row\n        yield row\n</code></pre>"},{"location":"code/data/legacy_loader/","title":"LegacyLoader","text":""},{"location":"code/data/legacy_loader/#data.LegacyLoader","title":"<code>data.LegacyLoader(name, description, views, json_files, splits)</code>","text":"<p>         Bases: <code>DataLoader</code></p> <p>Data Loader class for Pixano legacy format datasets</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Dataset name</p> <code>description</code> <code>str</code> <p>Dataset description</p> <code>views</code> <code>list[str]</code> <p>List of view names</p> <code>json_files</code> <code>dict[str, str]</code> <p>dict {viewId: } paths relative to workspace <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name</p> required <code>description</code> <code>str</code> <p>Dataset description</p> required <code>splits</code> <code>list[str]</code> <p>Dataset splits</p> required Source code in <code>pixano/data/legacy_loader.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: str,\n    views: list[str],\n    json_files: dict[str, str],\n    splits: list[str],\n):\n\"\"\"Initialize Pixano Legacy Loader\n\n    Args:\n        name (str): Dataset name\n        description (str): Dataset description\n        splits (list[str]): Dataset splits\n    \"\"\"\n    self.json_files = json_files\n    self.views = views\n\n    # Initialize Data Loader\n    super().__init__(\n        name,\n        description,\n        splits,\n        [pa.field(view, arrow_types.ImageType()) for view in views],\n    )\n</code></pre>"},{"location":"code/data/legacy_loader/#data.legacy_loader.LegacyLoader.import_row","title":"<code>import_row(input_dirs, split, portable=False)</code>","text":"<p>Process dataset row for import</p> <p>Parameters:</p> Name Type Description Default <code>input_dirs</code> <code>dict[str, Path]</code> <p>Input directory workspace (image directories are read from Pixano json files given at init)</p> required <code>split</code> <code>str</code> <p>Dataset split</p> required <code>portable</code> <code>bool</code> <p>True to move or download media files inside dataset. Defaults to False.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>Processed rows</p> Source code in <code>pixano/data/legacy_loader.py</code> <pre><code>def import_row(\n    self,\n    input_dirs: dict[str, Path],\n    split: str,\n    portable: bool = False,\n) -&gt; Iterator:\n\"\"\"Process dataset row for import\n\n    Args:\n        input_dirs (dict[str, Path]): Input directory workspace (image directories are read from Pixano json files given at init)\n        split (str): Dataset split\n        portable (bool, optional): True to move or download media files inside dataset. Defaults to False.\n\n    Yields:\n        Iterator: Processed rows\n    \"\"\"\n\n    category_ids = {}\n    feats = defaultdict(list)\n    for view in self.views:\n        # Open annotation files\n        with open(input_dirs[\"workspace\"] / self.json_files[view], \"r\") as f:\n            pix_json = json.load(f)\n\n            # Group annotations by image ID (timestamp)\n            annotations = defaultdict(list)\n            for ann in pix_json[\"annotations\"]:\n                annotations[str(ann[\"timestamp\"])].append(ann)\n\n            # Process rows\n            for im in sorted(\n                pix_json[\"data\"][\"children\"], key=lambda x: x[\"timestamp\"]\n            ):\n                # Load image\n                file_name_uri = urlparse(im[\"path\"])\n                if file_name_uri.scheme == \"\":\n                    im_path = input_dirs[\"workspace\"] / im[\"path\"]\n                else:\n                    im_path = Path(file_name_uri.path)\n\n                image = Image.open(BytesIO(im_path.read_bytes()))\n                im_w = image.width\n                im_h = image.height\n                im_thumb = image_to_thumbnail(image)\n\n                feats[str(im[\"timestamp\"])].append(\n                    {\n                        \"viewId\": view,\n                        \"width\": im_w,\n                        \"height\": im_h,\n                        \"im_thumb\": im_thumb,\n                        \"im_uri\": (\n                            f\"image/{split}/{im_path.name}\"\n                            if portable\n                            else im_path.absolute().as_uri()\n                        ),\n                        \"anns\": annotations[str(im[\"timestamp\"])],\n                    }\n                )\n\n    for timestamp in feats:\n        # Fill row with ID, image\n        row = {\n            \"id\": timestamp,\n            \"objects\": [],\n            \"split\": split,\n        }\n        for f in feats[timestamp]:\n            row[f[\"viewId\"]] = arrow_types.Image(\n                f[\"im_uri\"], None, f[\"im_thumb\"]\n            ).to_dict()\n\n            # Fill row with list of image annotations\n            for ann in f[\"anns\"]:\n                # collect categories to build category ids\n                if ann[\"category\"] not in category_ids:\n                    category_ids[ann[\"category\"]] = len(category_ids)\n\n                bbox = [0.0, 0.0, 0.0, 0.0]\n                mask = None\n\n                if \"geometry\" in ann:\n                    if (\n                        ann[\"geometry\"][\"type\"] == \"polygon\"\n                        and ann[\"geometry\"][\"vertices\"]\n                    ):\n                        # Polygon\n                        # we have normalized coords, we must denorm before making RLE\n                        if not isnan(ann[\"geometry\"][\"vertices\"][0]):\n                            if len(ann[\"geometry\"][\"vertices\"]) &gt; 4:\n                                denorm = denormalize(\n                                    ann[\"geometry\"][\"vertices\"],\n                                    f[\"height\"],\n                                    f[\"width\"],\n                                )\n                                rles = mask_api.frPyObjects(\n                                    [denorm], f[\"height\"], f[\"width\"]\n                                )\n                                mask = mask_api.merge(rles)\n                            else:\n                                print(\n                                    \"Polygon with 2 or less points. Discarded\\n\",\n                                    ann[\"geometry\"],\n                                )\n                    elif (\n                        ann[\"geometry\"][\"type\"] == \"mpolygon\"\n                        and ann[\"geometry\"][\"mvertices\"]\n                    ):\n                        # MultiPolygon\n                        if not isnan(ann[\"geometry\"][\"mvertices\"][0][0]):\n                            denorm = [\n                                denormalize(poly, f[\"height\"], f[\"width\"])\n                                for poly in ann[\"geometry\"][\"mvertices\"]\n                            ]\n                            rles = mask_api.frPyObjects(\n                                denorm, f[\"height\"], f[\"width\"]\n                            )\n                            mask = mask_api.merge(rles)\n                    elif (\n                        ann[\"geometry\"][\"type\"] == \"rectangle\"\n                        and ann[\"geometry\"][\"vertices\"]\n                    ):  # BBox\n                        if not isnan(ann[\"geometry\"][\"vertices\"][0]):\n                            denorm = denormalize(\n                                [ann[\"geometry\"][\"vertices\"]],\n                                f[\"height\"],\n                                f[\"width\"],\n                            )\n                            bbox = xyxy_to_xywh(denorm)\n                    elif (\n                        ann[\"geometry\"][\"type\"] == \"graph\"\n                        and ann[\"geometry\"][\"vertices\"]\n                    ):  # Keypoints\n                        print(\"Keypoints are not implemented yet\")\n                    else:\n                        # print('Unknown geometry', ann['geometry']['type'])  # log can be annoying if many...\n                        pass\n                else:\n                    # Ca peut etre un mask, ou 3d, trackink... etc.\n                    print(\"No geometry?\")\n\n                row[\"objects\"].append(\n                    arrow_types.ObjectAnnotation(\n                        id=str(ann[\"id\"]),\n                        view_id=f[\"viewId\"],\n                        bbox=bbox,\n                        mask=mask,\n                        is_group_of=bool(ann[\"iscrowd\"])\n                        if \"iscrowd\" in ann\n                        else None,\n                        category_id=category_ids[ann[\"category\"]],\n                        category_name=ann[\"category\"],\n                    ).dict()\n                )\n\n        # Return row\n        yield row\n</code></pre>"},{"location":"code/models/inference_model/","title":"InferenceModel","text":""},{"location":"code/models/inference_model/#models.InferenceModel","title":"<code>models.InferenceModel(name, id='', device='', source='', info='')</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract parent class for OfflineModel and OnlineModel</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>source</code> <code>str</code> <p>Model source</p> <code>info</code> <code>str</code> <p>Additional model info</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name</p> required <code>id</code> <code>str</code> <p>Model ID. Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device. Defaults to \"\".</p> <code>''</code> <code>source</code> <code>str</code> <p>Model source. Defaults to \"\".</p> <code>''</code> <code>info</code> <code>str</code> <p>Additional model info. Defaults to \"\".</p> <code>''</code> Source code in <code>pixano/models/inference_model.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    id: str = \"\",\n    device: str = \"\",\n    source: str = \"\",\n    info: str = \"\",\n) -&gt; None:\n\"\"\"Initialize model name and ID\n\n    Args:\n        name (str): Model name\n        id (str, optional): Model ID. Defaults to \"\".\n        device (str, optional): Model GPU or CPU device. Defaults to \"\".\n        source (str, optional): Model source. Defaults to \"\".\n        info (str, optional): Additional model info. Defaults to \"\".\n    \"\"\"\n\n    self.name = name\n    if id == \"\":\n        self.id = f\"{datetime.now().strftime('%y%m%d_%H%M%S')}_{name}\"\n    else:\n        self.id = id\n    self.device = device\n    self.source = source\n    self.info = info\n</code></pre>"},{"location":"code/models/inference_model/#models.inference_model.InferenceModel.create_json","title":"<code>create_json(output_dir, filename, spec_json, num_elements)</code>","text":"<p>Save output .json</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Output dataset directory</p> required <code>filename</code> <code>str</code> <p>Output .json filename</p> required <code>spec_json</code> <code>dict</code> <p>Input dataset .json</p> required <code>num_elements</code> <code>int</code> <p>Number of processed rows</p> required Source code in <code>pixano/models/inference_model.py</code> <pre><code>def create_json(\n    self,\n    output_dir: Path,\n    filename: str,\n    spec_json: dict,\n    num_elements: int,\n):\n\"\"\"Save output .json\n\n    Args:\n        output_dir (Path): Output dataset directory\n        filename (str): Output .json filename\n        spec_json (dict): Input dataset .json\n        num_elements (int): Number of processed rows\n    \"\"\"\n\n    # Load existing .json\n    if (output_dir / f\"{filename}.json\").is_file():\n        with open(output_dir / f\"{filename}.json\", \"r\") as f:\n            output_json = json.load(f)\n        output_json[\"num_elements\"] += num_elements\n\n    # Or create .json from scratch\n    else:\n        output_json = {\n            \"id\": spec_json[\"id\"],\n            \"name\": spec_json[\"name\"],\n            \"description\": spec_json[\"description\"],\n            \"num_elements\": num_elements,\n            \"model_id\": self.id,\n            \"model_name\": self.name,\n            \"model_source\": self.source,\n            \"model_info\": self.info,\n        }\n\n    # Save .json\n    with open(output_dir / f\"{filename}.json\", \"w\") as f:\n        json.dump(output_json, f)\n</code></pre>"},{"location":"code/models/inference_model/#models.inference_model.InferenceModel.embedding_batch","title":"<code>embedding_batch(batch, view, uri_prefix)</code>","text":"<p>Embedding precomputing for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>pa.RecordBatch</code> <p>Input batch</p> required <code>view</code> <code>str</code> <p>Dataset view</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <p>Returns:</p> Type Description <code>list[np.ndarray]</code> <p>list[np.ndarray]: Model embeddings as NumPy arrays</p> Source code in <code>pixano/models/inference_model.py</code> <pre><code>def embedding_batch(\n    self,\n    batch: pa.RecordBatch,\n    view: str,\n    uri_prefix: str,\n) -&gt; list[np.ndarray]:\n\"\"\"Embedding precomputing for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        view (str): Dataset view\n        uri_prefix (str): URI prefix for media files\n\n    Returns:\n        list[np.ndarray]: Model embeddings as NumPy arrays\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"code/models/inference_model/#models.inference_model.InferenceModel.export_to_onnx","title":"<code>export_to_onnx(library_dir)</code>","text":"<p>Export Torch model to ONNX</p> <p>Parameters:</p> Name Type Description Default <code>library_dir</code> <code>Path</code> <p>Dataset library directory</p> required Source code in <code>pixano/models/inference_model.py</code> <pre><code>def export_to_onnx(self, library_dir: Path):\n\"\"\"Export Torch model to ONNX\n\n    Args:\n        library_dir (Path): Dataset library directory\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"code/models/inference_model/#models.inference_model.InferenceModel.inference_batch","title":"<code>inference_batch(batch, view, uri_prefix, threshold=0.0)</code>","text":"<p>Inference preannotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>pa.RecordBatch</code> <p>Input batch</p> required <code>view</code> <code>str</code> <p>Dataset view</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>list[list[arrow_types.ObjectAnnotation]]</code> <p>list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation</p> Source code in <code>pixano/models/inference_model.py</code> <pre><code>def inference_batch(\n    self,\n    batch: pa.RecordBatch,\n    view: str,\n    uri_prefix: str,\n    threshold: float = 0.0,\n) -&gt; list[list[arrow_types.ObjectAnnotation]]:\n\"\"\"Inference preannotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        view (str): Dataset view\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n\n    Returns:\n        list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"code/models/inference_model/#models.inference_model.InferenceModel.process_dataset","title":"<code>process_dataset(input_dir, process_type, views, splits=[], batch_size=1, threshold=0.0)</code>","text":"<p>Process dataset for inference preannotation or embedding precomputing</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>Path</code> <p>Input dataset directory</p> required <code>process_type</code> <code>str</code> <p>Process type ('infer' for inference preannotation or 'embed' for embedding precomputing)</p> required <code>views</code> <code>list[str]</code> <p>Dataset views</p> required <code>splits</code> <code>list[str]</code> <p>Dataset splits, all if []. Defaults to [].</p> <code>[]</code> <code>batch_size</code> <code>int</code> <p>Rows per batch. Defaults to 1.</p> <code>1</code> <code>threshold</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Output dataset directory</p> Source code in <code>pixano/models/inference_model.py</code> <pre><code>def process_dataset(\n    self,\n    input_dir: Path,\n    process_type: str,\n    views: list[str],\n    splits: list[str] = [],\n    batch_size: int = 1,\n    threshold: float = 0.0,\n) -&gt; Path:\n\"\"\"Process dataset for inference preannotation or embedding precomputing\n\n    Args:\n        input_dir (Path): Input dataset directory\n        process_type (str): Process type ('infer' for inference preannotation or 'embed' for embedding precomputing)\n        views (list[str]): Dataset views\n        splits (list[str], optional): Dataset splits, all if []. Defaults to [].\n        batch_size (int, optional): Rows per batch. Defaults to 1.\n        threshold (float, optional): Confidence threshold for predictions. Defaults to 0.0.\n\n    Returns:\n        Path: Output dataset directory\n    \"\"\"\n\n    output_dir = input_dir / f\"db_{process_type}_{self.id}\"\n\n    # Load spec.json\n    with open(input_dir / \"spec.json\", \"r\") as f:\n        spec_json = json.load(f)\n\n    # Create URI prefix\n    media_dir = input_dir / \"media\"\n    uri_prefix = media_dir.absolute().as_uri()\n\n    # If no splits provided, select all splits\n    if not splits:\n        splits = [s.name for s in os.scandir(input_dir / \"db\") if s.is_dir()]\n    # Else, format provided splits\n    else:\n        splits = [\n            f\"split={s}\" if not s.startswith(\"split=\") else s for s in self.splits\n        ]\n    # Check if the splits exist\n    for split in splits:\n        split_dir = input_dir / \"db\" / split\n        if split_dir.exists():\n            raise Exception(f\"{split_dir} does not exist.\")\n        if not any(split_dir.iterdir()):\n            raise Exception(f\"{split_dir} is empty.\")\n\n    # Create schema\n    fields = [pa.field(\"id\", pa.string())]\n    if process_type == \"infer\":\n        fields.extend(\n            [\n                pa.field(\"objects\", pa.list_(arrow_types.ObjectAnnotationType())),\n            ]\n        )\n    elif process_type == \"embed\":\n        fields.extend(\n            [\n                pa.field(f\"{view}_embedding\", arrow_types.EmbeddingType())\n                for view in views\n            ]\n        )\n    schema = pa.schema(fields)\n\n    # Iterate on splits\n    for split in splits:\n        # List split files\n        files = (input_dir / \"db\" / split).glob(\"*.parquet\")\n        files = sorted(files, key=lambda x: natural_key(x.name))\n\n        # Create output split directory\n        split_dir = output_dir / split\n        split_dir.mkdir(parents=True, exist_ok=True)\n        split_name = split.replace(\"split=\", \"\")\n\n        # Check for already processed files\n        processed = [p.name for p in split_dir.glob(\"*.parquet\")]\n\n        # Iterate on files\n        for file in tqdm(files, desc=f\"Processing {split_name} split\", position=0):\n            # Process only remaining files\n            if file.name not in processed:\n                # Load file into batches\n                table = pq.read_table(file)\n                batches = table.to_batches(max_chunksize=batch_size)\n\n                # Iterate on batches\n                data = {field.name: [] for field in schema}\n                for batch in tqdm(\n                    batches, desc=f\"Processing {file.name}\", position=1\n                ):\n                    # Add row IDs\n                    data[\"id\"].extend([str(row) for row in batch[\"id\"]])\n                    # For inferences\n                    if process_type == \"infer\":\n                        # Iterate on views\n                        batch_inf = []\n                        for view in views:\n                            batch_inf.append(\n                                self.inference_batch(\n                                    batch, view, uri_prefix, threshold\n                                )\n                            )\n                        # Regroup view inferences by row\n                        data[\"objects\"].append(\n                            [\n                                inf.dict()\n                                for row in range(len(batch[\"id\"]))\n                                for view_inf in batch_inf\n                                for inf in view_inf[row]\n                            ]\n                        )\n                    # For embeddings\n                    elif process_type == \"embed\":\n                        # Iterate on views\n                        for view in views:\n                            view_emb = self.embedding_batch(batch, view, uri_prefix)\n                            for emb in view_emb:\n                                emb_bytes = BytesIO()\n                                np.save(emb_bytes, emb)\n                                data[f\"{view}_embedding\"].append(\n                                    emb_bytes.getvalue()\n                                )\n\n                # Convert ExtensionTypes\n                arrays = []\n                for field in schema:\n                    arrays.append(\n                        arrow_types.convert_field(\n                            field_name=field.name,\n                            field_type=field.type,\n                            field_data=data[field.name],\n                        )\n                    )\n\n                # Save to file\n                pq.write_table(\n                    pa.Table.from_arrays(arrays, schema=schema),\n                    split_dir / file.name,\n                )\n\n                # Save.json\n                self.create_json(\n                    output_dir=output_dir,\n                    filename=process_type,\n                    spec_json=spec_json,\n                    num_elements=pq.read_metadata(split_dir / file.name).num_rows,\n                )\n\n    return output_dir\n</code></pre>"},{"location":"code/transforms/boxes/","title":"boxes","text":""},{"location":"code/transforms/boxes/#transforms.boxes","title":"<code>transforms.boxes</code>","text":""},{"location":"code/transforms/boxes/#transforms.boxes.denormalize","title":"<code>denormalize(coord, height, width)</code>","text":"<p>Denormalize coordinates</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>list[float]</code> <p>Normalized coordinates</p> required <code>height</code> <code>int</code> <p>Height</p> required <code>width</code> <code>int</code> <p>Width</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Unnormalized coordinates</p> Source code in <code>pixano/transforms/boxes.py</code> <pre><code>def denormalize(coord: list[float], height: int, width: int) -&gt; list[float]:\n\"\"\"Denormalize coordinates\n\n    Args:\n        coord (list[float]): Normalized coordinates\n        height (int): Height\n        width (int): Width\n\n    Returns:\n        list[float]: Unnormalized coordinates\n    \"\"\"\n\n    denorm = []\n\n    for i, c in enumerate(coord):\n        if i % 2 == 0:\n            denorm.append(c * width)\n        else:\n            denorm.append(c * height)\n\n    return denorm\n</code></pre>"},{"location":"code/transforms/boxes/#transforms.boxes.format_bbox","title":"<code>format_bbox(bbox, is_predicted=False, confidence=None)</code>","text":"<p>Convert bounding box to frontend format</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>list[float]</code> <p>Bounding box</p> required <code>is_predicted</code> <code>bool</code> <p>True for prediction, False for ground truth. Defaults to False.</p> <code>False</code> <code>confidence</code> <code>float</code> <p>Bounding box confidence. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Bounding box in frontend format</p> Source code in <code>pixano/transforms/boxes.py</code> <pre><code>def format_bbox(bbox, is_predicted=False, confidence=None) -&gt; dict:\n\"\"\"Convert bounding box to frontend format\n\n    Args:\n        bbox (list[float]): Bounding box\n        is_predicted (bool, optional): True for prediction, False for ground truth. Defaults to False.\n        confidence (float, optional): Bounding box confidence. Defaults to None.\n\n    Returns:\n        dict: Bounding box in frontend format\n    \"\"\"\n\n    return {\n        \"x\": float(bbox[0]),\n        \"y\": float(bbox[1]),\n        \"width\": float(bbox[2]),\n        \"height\": float(bbox[3]),\n        \"is_predict\": is_predicted,\n        \"confidence\": confidence,\n    }\n</code></pre>"},{"location":"code/transforms/boxes/#transforms.boxes.mask_to_bbox","title":"<code>mask_to_bbox(mask)</code>","text":"<p>Returns the smallest bounding box containing all the mask pixels</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>np.ndarray</code> <p>Mask as NumPy Array</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Normalized xywh bounding box</p> Source code in <code>pixano/transforms/boxes.py</code> <pre><code>def mask_to_bbox(mask: np.ndarray) -&gt; list[float]:\n\"\"\"Returns the smallest bounding box containing all the mask pixels\n\n    Args:\n        mask (np.ndarray): Mask as NumPy Array\n\n    Returns:\n        list[float]: Normalized xywh bounding box\n    \"\"\"\n\n    height, width = mask.shape\n    bool_mask = np.array(mask).astype(bool)\n\n    # Find all columns and rows that contain ones\n    rows = np.any(bool_mask, axis=1)\n    cols = np.any(bool_mask, axis=0)\n\n    # Find the min and max col/row index that contain ones\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    # Calculate bbox height and width\n    w = (cmax - cmin + 1) / width\n    h = (rmax - rmin + 1) / height\n\n    return [cmin / width, rmin / height, w, h]\n</code></pre>"},{"location":"code/transforms/boxes/#transforms.boxes.normalize","title":"<code>normalize(coord, height, width)</code>","text":"<p>Normalize coordinates</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>list[float]</code> <p>Unnormalized coordinates</p> required <code>height</code> <code>int</code> <p>Height</p> required <code>width</code> <code>int</code> <p>Width</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Normalized coordinates</p> Source code in <code>pixano/transforms/boxes.py</code> <pre><code>def normalize(coord: list[float], height: int, width: int) -&gt; list[float]:\n\"\"\"Normalize coordinates\n\n    Args:\n        coord (list[float]): Unnormalized coordinates\n        height (int): Height\n        width (int): Width\n\n    Returns:\n        list[float]: Normalized coordinates\n    \"\"\"\n\n    norm = []\n\n    for i, c in enumerate(coord):\n        if i % 2 == 0:\n            norm.append(c / width)\n        else:\n            norm.append(c / height)\n\n    return norm\n</code></pre>"},{"location":"code/transforms/boxes/#transforms.boxes.urle_to_bbox","title":"<code>urle_to_bbox(urle)</code>","text":"<p>Returns the smallest bounding box containing all the mask pixels</p> <p>Parameters:</p> Name Type Description Default <code>urle</code> <code>dict</code> <p>Mask as uncompressed RLE</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Normalized xywh bounding box</p> Source code in <code>pixano/transforms/boxes.py</code> <pre><code>def urle_to_bbox(urle: dict) -&gt; list[float]:\n\"\"\"Returns the smallest bounding box containing all the mask pixels\n\n    Args:\n        urle (dict): Mask as uncompressed RLE\n\n    Returns:\n        list[float]: Normalized xywh bounding box\n    \"\"\"\n\n    return mask_to_bbox(rle_to_mask(urle_to_rle(urle)))\n</code></pre>"},{"location":"code/transforms/boxes/#transforms.boxes.xywh_to_xyxy","title":"<code>xywh_to_xyxy(xywh)</code>","text":"<p>Convert bounding box coordinates from xywh (using top left point as reference) to xyxy</p> <p>Parameters:</p> Name Type Description Default <code>xywh</code> <code>list[float]</code> <p>xywh coordinates</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: xyxy coordinates</p> Source code in <code>pixano/transforms/boxes.py</code> <pre><code>def xywh_to_xyxy(xywh: list[float]) -&gt; list[float]:\n\"\"\"Convert bounding box coordinates from xywh (using top left point as reference) to xyxy\n\n    Args:\n        xywh (list[float]): xywh coordinates\n\n    Returns:\n        list[float]: xyxy coordinates\n    \"\"\"\n\n    return [\n        float(xywh[0]),\n        float(xywh[1]),\n        float(xywh[0] + xywh[2]),\n        float(xywh[1] + xywh[3]),\n    ]\n</code></pre>"},{"location":"code/transforms/boxes/#transforms.boxes.xyxy_to_xywh","title":"<code>xyxy_to_xywh(xyxy)</code>","text":"<p>Convert bounding box coordinates from xyxy to xywh (using top left point as reference)</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>list[float]</code> <p>xyxy coordinates</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: xywh coordinates</p> Source code in <code>pixano/transforms/boxes.py</code> <pre><code>def xyxy_to_xywh(xyxy: list[float]) -&gt; list[float]:\n\"\"\"Convert bounding box coordinates from xyxy to xywh (using top left point as reference)\n\n    Args:\n        xyxy (list[float]): xyxy coordinates\n\n    Returns:\n        list[float]: xywh coordinates\n    \"\"\"\n\n    return [\n        float(xyxy[0]),\n        float(xyxy[1]),\n        float(xyxy[2] - xyxy[0]),\n        float(xyxy[3] - xyxy[1]),\n    ]\n</code></pre>"},{"location":"code/transforms/image/","title":"image","text":""},{"location":"code/transforms/image/#transforms.image","title":"<code>transforms.image</code>","text":""},{"location":"code/transforms/image/#transforms.image.binary_to_url","title":"<code>binary_to_url(im_bytes)</code>","text":"<p>Encode image from binary to base 64 URL</p> <p>Parameters:</p> Name Type Description Default <code>im_bytes</code> <code>bytes</code> <p>Image as binary</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Image as base 64</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def binary_to_url(im_bytes: bytes) -&gt; str:\n\"\"\"Encode image from binary to base 64 URL\n\n    Args:\n        im_bytes (bytes): Image as binary\n\n    Returns:\n        str: Image as base 64\n    \"\"\"\n\n    if im_bytes is not None:\n        encoded = base64.b64encode(im_bytes).decode(\"utf-8\")\n        return f\"data:image;base64,{encoded}\"\n    else:\n        return \"\"\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.depth_array_to_gray","title":"<code>depth_array_to_gray(depth, valid_start=0.2, valid_end=1, scale=1.0)</code>","text":"<p>Encode depth array to gray levels</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>np.ndarray</code> <p>Depth array</p> required <code>valid_start</code> <code>float</code> <p>Valid start. Defaults to 0.2.</p> <code>0.2</code> <code>valid_end</code> <code>float</code> <p>Valid end. Defaults to 1.</p> <code>1</code> <code>scale</code> <code>float</code> <p>Scale. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: Depth array in gray levels</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def depth_array_to_gray(\n    depth: np.ndarray,\n    valid_start: float = 0.2,\n    valid_end: float = 1,\n    scale: float = 1.0,\n) -&gt; np.ndarray:\n\"\"\"Encode depth array to gray levels\n\n    Args:\n        depth (np.ndarray): Depth array\n        valid_start (float, optional): Valid start. Defaults to 0.2.\n        valid_end (float, optional): Valid end. Defaults to 1.\n        scale (float, optional): Scale. Defaults to 1.0.\n\n    Returns:\n        np.ndarray: Depth array in gray levels\n    \"\"\"\n\n    mask = depth &gt; 1\n\n    # Scale gives depth in mm\n    depth_n = depth * scale * 0.001\n\n    if mask.sum() &gt; 0:\n        depth_n[mask] -= depth_n[mask].min()\n        depth_n[mask] /= depth_n[mask].max() / (valid_end - valid_start)\n        depth_n[mask] += valid_start\n\n    depth_n *= 255\n    depth_n = cv2.applyColorMap(\n        depth_n[:, :, np.newaxis].astype(np.uint8), cv2.COLORMAP_PLASMA\n    )\n\n    return depth_n\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.depth_file_to_binary","title":"<code>depth_file_to_binary(depth_path)</code>","text":"<p>Encode depth file to RGB image in binary</p> <p>Parameters:</p> Name Type Description Default <code>depth_path</code> <code>str</code> <p>Depth file path</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Depth file as RGB image in binary</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def depth_file_to_binary(depth_path: str) -&gt; bytes:\n\"\"\"Encode depth file to RGB image in binary\n\n    Args:\n        depth_path (str): Depth file path\n\n    Returns:\n        bytes: Depth file as RGB image in binary\n    \"\"\"\n\n    depth = cv2.imread(depth_path, cv2.IMREAD_ANYDEPTH).astype(np.float32)\n    depth = depth_array_to_gray(depth)\n    depth_rgb = Image.fromarray(depth)\n\n    return image_to_binary(depth_rgb)\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.encode_rle","title":"<code>encode_rle(mask, height, width)</code>","text":"<p>Encode mask from polygons / uncompressed RLE / RLE to RLE</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list[list] | dict</code> <p>Mask as polygons / uncompressed RLE / RLE</p> required <code>height</code> <code>int</code> <p>Image height</p> required <code>width</code> <code>int</code> <p>Image width</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mask as RLE</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def encode_rle(mask: list[list] | dict, height: int, width: int) -&gt; dict:\n\"\"\"Encode mask from polygons / uncompressed RLE / RLE to RLE\n\n    Args:\n        mask (list[list] | dict): Mask as polygons / uncompressed RLE / RLE\n        height (int): Image height\n        width (int): Image width\n\n    Returns:\n        dict: Mask as RLE\n    \"\"\"\n\n    if isinstance(mask, list):\n        rle = polygons_to_rle(mask, height, width)\n    elif isinstance(mask, dict):\n        if isinstance(mask[\"counts\"], list):\n            rle = urle_to_rle(mask)\n        else:\n            rle = mask\n    else:\n        rle = None\n    return rle\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.image_to_binary","title":"<code>image_to_binary(image, format='PNG')</code>","text":"<p>Encode image from Pillow to binary</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image.Image</code> <p>Image as Pillow</p> required <code>format</code> <code>str</code> <p>Image file extension. Defaults to \"PNG\".</p> <code>'PNG'</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Image as binary</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def image_to_binary(image: Image.Image, format: str = \"PNG\") -&gt; bytes:\n\"\"\"Encode image from Pillow to binary\n\n    Args:\n        image (Image.Image): Image as Pillow\n        format (str, optional): Image file extension. Defaults to \"PNG\".\n\n    Returns:\n        bytes: Image as binary\n    \"\"\"\n\n    with BytesIO() as output_bytes:\n        image.save(output_bytes, format)\n        im_bytes = output_bytes.getvalue()\n\n    return im_bytes\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.image_to_thumbnail","title":"<code>image_to_thumbnail(image)</code>","text":"<p>Generate image thumbnail</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>bytes | Image.Image</code> <p>Image as binary or as Pillow</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Image thumbnail as binary</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def image_to_thumbnail(image: bytes | Image.Image) -&gt; bytes:\n\"\"\"Generate image thumbnail\n\n    Args:\n        image (bytes | Image.Image): Image as binary or as Pillow\n\n    Returns:\n        bytes: Image thumbnail as binary\n    \"\"\"\n\n    if isinstance(image, bytes):\n        image = Image.open(BytesIO(image))\n\n    image.thumbnail((128, 128))\n    return image_to_binary(image)\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.mask_to_polygons","title":"<code>mask_to_polygons(mask)</code>","text":"<p>Encode mask from NumPy array to polygons</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>np.ndarray</code> <p>Mask as NumPy array</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Mask as polygons</p> <code>bool</code> <code>bool</code> <p>Mask has holes</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def mask_to_polygons(mask: np.ndarray) -&gt; tuple[list, bool]:\n\"\"\"Encode mask from NumPy array to polygons\n\n    Args:\n        mask (np.ndarray): Mask as NumPy array\n\n    Returns:\n        list: Mask as polygons\n        bool: Mask has holes\n    \"\"\"\n\n    if mask is not None:\n        # Some versions of cv2 does not support incontiguous arr\n        mask = np.ascontiguousarray(mask)\n\n        # cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level hierarchy.\n        # External contours (boundary) of the object are placed in hierarchy-1.\n        # Internal contours (holes) are placed in hierarchy-2.\n        # cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\n        res = cv2.findContours(\n            mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE\n        )\n        hierarchy = res[-1]\n\n        # If mask is empty\n        if hierarchy is None:\n            return [], False\n\n        # Check if mask has holes\n        has_holes = (hierarchy.reshape(-1, 4)[:, 3] &gt;= 0).sum() &gt; 0\n\n        res = res[-2]\n        res = [x.flatten() for x in res]\n\n        # The coordinates from OpenCV are integers in range [0, W-1 or H-1].\n        # We add 0.5 to turn them into real-value coordinate space. A better solution\n        # would be to first +0.5 and then dilate the returned polygon by 0.5.\n        res = [x + 0.5 for x in res if len(x) &gt;= 6]\n\n        return res, has_holes\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.mask_to_rle","title":"<code>mask_to_rle(mask)</code>","text":"<p>Encode mask from Pillow or NumPy array to RLE</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Image.Image</code> <p>Mask as Pillow or NumPy array</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mask as RLE</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def mask_to_rle(mask: Image.Image) -&gt; dict:\n\"\"\"Encode mask from Pillow or NumPy array to RLE\n\n    Args:\n        mask (Image.Image): Mask as Pillow or NumPy array\n\n    Returns:\n        dict: Mask as RLE\n    \"\"\"\n\n    if mask is not None:\n        mask_array = np.asfortranarray(mask)\n        return mask_api.encode(mask_array)\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.polygons_to_rle","title":"<code>polygons_to_rle(polygons, height, width)</code>","text":"<p>Encode mask from polygons to RLE</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>list[list]</code> <p>Mask as polygons</p> required <code>height</code> <code>int</code> <p>Image height</p> required <code>width</code> <code>int</code> <p>Image width</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mask as RLE</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def polygons_to_rle(polygons: list[list], height: int, width: int) -&gt; dict:\n\"\"\"Encode mask from polygons to RLE\n\n    Args:\n        polygons (list[list]): Mask as polygons\n        height (int): Image height\n        width (int): Image width\n\n    Returns:\n        dict: Mask as RLE\n    \"\"\"\n\n    if polygons is not None:\n        rles = mask_api.frPyObjects(polygons, height, width)\n        return mask_api.merge(rles)\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.rle_to_mask","title":"<code>rle_to_mask(rle)</code>","text":"<p>Decode mask from RLE to NumPy array</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict</code> <p>Mask as RLE</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: Mask as NumPy array</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def rle_to_mask(rle: dict) -&gt; np.ndarray:\n\"\"\"Decode mask from RLE to NumPy array\n\n    Args:\n        rle (dict): Mask as RLE\n\n    Returns:\n        np.ndarray: Mask as NumPy array\n    \"\"\"\n\n    if rle is not None:\n        return mask_api.decode(rle)\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.rle_to_polygons","title":"<code>rle_to_polygons(rle)</code>","text":"<p>Encode mask from RLE to polygons</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict</code> <p>Mask as RLE</p> required <p>Returns:</p> Type Description <code>list[list]</code> <p>list[list]: Mask as polygons</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def rle_to_polygons(rle: dict) -&gt; list[list]:\n\"\"\"Encode mask from RLE to polygons\n\n    Args:\n        rle (dict): Mask as RLE\n\n    Returns:\n        list[list]: Mask as polygons\n    \"\"\"\n\n    if rle is not None and \"size\" in rle:\n        h, w = rle[\"size\"]\n        polygons, _ = mask_to_polygons(rle_to_mask(rle))\n\n        # Normalize point coordinates\n        for p in polygons:\n            p[::2] /= w\n            p[1::2] /= h\n\n        # Cast to python list\n        polygons = [p.tolist() for p in polygons]\n\n        return polygons\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.rle_to_urle","title":"<code>rle_to_urle(rle)</code>","text":"<p>Encode mask from RLE to uncompressed RLE</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>dict</code> <p>Mask as RLE</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mask as uncompressed RLE</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def rle_to_urle(rle: dict) -&gt; dict:\n\"\"\"Encode mask from RLE to uncompressed RLE\n\n    Args:\n        rle (dict): Mask as RLE\n\n    Returns:\n        dict: Mask as uncompressed RLE\n    \"\"\"\n\n    if rle is not None:\n        mask = rle_to_mask(rle)\n        urle = {\"counts\": [], \"size\": list(mask.shape)}\n        counts = urle.get(\"counts\")\n\n        for i, (value, elements) in enumerate(groupby(mask.ravel(order=\"F\"))):\n            if i == 0 and value == 1:\n                counts.append(0)\n            counts.append(len(list(elements)))\n\n        return urle\n</code></pre>"},{"location":"code/transforms/image/#transforms.image.urle_to_rle","title":"<code>urle_to_rle(urle)</code>","text":"<p>Encode mask from uncompressed RLE to RLE</p> <p>Parameters:</p> Name Type Description Default <code>urle</code> <code>dict</code> <p>Mask as uncompressed RLE</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mask as RLE</p> Source code in <code>pixano/transforms/image.py</code> <pre><code>def urle_to_rle(urle: dict) -&gt; dict:\n\"\"\"Encode mask from uncompressed RLE to RLE\n\n    Args:\n        urle (dict): Mask as uncompressed RLE\n\n    Returns:\n        dict: Mask as RLE\n    \"\"\"\n\n    if urle is not None:\n        height, width = urle[\"size\"]\n        return mask_api.frPyObjects(urle, height, width)\n</code></pre>"},{"location":"code/transforms/labels/","title":"labels","text":""},{"location":"code/transforms/labels/#transforms.labels","title":"<code>transforms.labels</code>","text":""},{"location":"code/transforms/labels/#transforms.labels.coco_ids_80to91","title":"<code>coco_ids_80to91(id)</code>","text":"<p>Return COCO category ID (80 to 91 classes)</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>Category ID (80 classes)</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Category ID (91 classes)</p> Source code in <code>pixano/transforms/labels.py</code> <pre><code>def coco_ids_80to91(id: int) -&gt; int:\n\"\"\"Return COCO category ID (80 to 91 classes)\n\n    Args:\n        id (int): Category ID (80 classes)\n\n    Returns:\n        int: Category ID (91 classes)\n    \"\"\"\n\n    coco_dict = {\n        1: 1,\n        2: 2,\n        3: 3,\n        4: 4,\n        5: 5,\n        6: 6,\n        7: 7,\n        8: 8,\n        9: 9,\n        10: 10,\n        11: 11,\n        12: 13,\n        13: 14,\n        14: 15,\n        15: 16,\n        16: 17,\n        17: 18,\n        18: 19,\n        19: 20,\n        20: 21,\n        21: 22,\n        22: 23,\n        23: 24,\n        24: 25,\n        25: 27,\n        26: 28,\n        27: 31,\n        28: 32,\n        29: 33,\n        30: 34,\n        31: 35,\n        32: 36,\n        33: 37,\n        34: 38,\n        35: 39,\n        36: 40,\n        37: 41,\n        38: 42,\n        39: 43,\n        40: 44,\n        41: 46,\n        42: 47,\n        43: 48,\n        44: 49,\n        45: 50,\n        46: 51,\n        47: 52,\n        48: 53,\n        49: 54,\n        50: 55,\n        51: 56,\n        52: 57,\n        53: 58,\n        54: 59,\n        55: 60,\n        56: 61,\n        57: 62,\n        58: 63,\n        59: 64,\n        60: 65,\n        61: 67,\n        62: 70,\n        63: 72,\n        64: 73,\n        65: 74,\n        66: 75,\n        67: 76,\n        68: 77,\n        69: 78,\n        70: 79,\n        71: 80,\n        72: 81,\n        73: 82,\n        74: 84,\n        75: 85,\n        76: 86,\n        77: 87,\n        78: 88,\n        79: 89,\n        80: 90,\n    }\n\n    return coco_dict[int(id)]\n</code></pre>"},{"location":"code/transforms/labels/#transforms.labels.coco_names_80","title":"<code>coco_names_80(id)</code>","text":"<p>Return COCO category name (80 classes)</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>Category ID</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Category name</p> Source code in <code>pixano/transforms/labels.py</code> <pre><code>def coco_names_80(id: int) -&gt; str:\n\"\"\"Return COCO category name (80 classes)\n\n    Args:\n        id (int): Category ID\n\n    Returns:\n        str: Category name\n    \"\"\"\n\n    coco_dict = {\n        1: \"person\",\n        2: \"bicycle\",\n        3: \"car\",\n        4: \"motorcycle\",\n        5: \"airplane\",\n        6: \"bus\",\n        7: \"train\",\n        8: \"truck\",\n        9: \"boat\",\n        10: \"traffic light\",\n        11: \"fire hydrant\",\n        12: \"stop sign\",\n        13: \"parking meter\",\n        14: \"bench\",\n        15: \"bird\",\n        16: \"cat\",\n        17: \"dog\",\n        18: \"horse\",\n        19: \"sheep\",\n        20: \"cow\",\n        21: \"elephant\",\n        22: \"bear\",\n        23: \"zebra\",\n        24: \"giraffe\",\n        25: \"backpack\",\n        26: \"umbrella\",\n        27: \"handbag\",\n        28: \"tie\",\n        29: \"suitcase\",\n        30: \"frisbee\",\n        31: \"skis\",\n        32: \"snowboard\",\n        33: \"sports ball\",\n        34: \"kite\",\n        35: \"baseball bat\",\n        36: \"baseball glove\",\n        37: \"skateboard\",\n        38: \"surfboard\",\n        39: \"tennis racket\",\n        40: \"bottle\",\n        41: \"wine glass\",\n        42: \"cup\",\n        43: \"fork\",\n        44: \"knife\",\n        45: \"spoon\",\n        46: \"bowl\",\n        47: \"banana\",\n        48: \"apple\",\n        49: \"sandwich\",\n        50: \"orange\",\n        51: \"broccoli\",\n        52: \"carrot\",\n        53: \"hot dog\",\n        54: \"pizza\",\n        55: \"donut\",\n        56: \"cake\",\n        57: \"chair\",\n        58: \"couch\",\n        59: \"potted plant\",\n        60: \"bed\",\n        61: \"dining table\",\n        62: \"toilet\",\n        63: \"tv\",\n        64: \"laptop\",\n        65: \"mouse\",\n        66: \"remote\",\n        67: \"keyboard\",\n        68: \"cell phone\",\n        69: \"microwave\",\n        70: \"oven\",\n        71: \"toaster\",\n        72: \"sink\",\n        73: \"refrigerator\",\n        74: \"book\",\n        75: \"clock\",\n        76: \"vase\",\n        77: \"scissors\",\n        78: \"teddy bear\",\n        79: \"hair drier\",\n        80: \"toothbrush\",\n    }\n\n    return coco_dict[int(id)]\n</code></pre>"},{"location":"code/transforms/labels/#transforms.labels.coco_names_91","title":"<code>coco_names_91(id)</code>","text":"<p>Return COCO category name (91 classes)</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>Category ID</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Category name</p> Source code in <code>pixano/transforms/labels.py</code> <pre><code>def coco_names_91(id: int) -&gt; str:\n\"\"\"Return COCO category name (91 classes)\n\n    Args:\n        id (int): Category ID\n\n    Returns:\n        str: Category name\n    \"\"\"\n\n    coco_dict = {\n        1: \"person\",\n        2: \"bicycle\",\n        3: \"car\",\n        4: \"motorcycle\",\n        5: \"airplane\",\n        6: \"bus\",\n        7: \"train\",\n        8: \"truck\",\n        9: \"boat\",\n        10: \"traffic light\",\n        11: \"fire hydrant\",\n        12: \"street sign\",\n        13: \"stop sign\",\n        14: \"parking meter\",\n        15: \"bench\",\n        16: \"bird\",\n        17: \"cat\",\n        18: \"dog\",\n        19: \"horse\",\n        20: \"sheep\",\n        21: \"cow\",\n        22: \"elephant\",\n        23: \"bear\",\n        24: \"zebra\",\n        25: \"giraffe\",\n        26: \"hat\",\n        27: \"backpack\",\n        28: \"umbrella\",\n        29: \"shoe\",\n        30: \"eye glasses\",\n        31: \"handbag\",\n        32: \"tie\",\n        33: \"suitcase\",\n        34: \"frisbee\",\n        35: \"skis\",\n        36: \"snowboard\",\n        37: \"sports ball\",\n        38: \"kite\",\n        39: \"baseball bat\",\n        40: \"baseball glove\",\n        41: \"skateboard\",\n        42: \"surfboard\",\n        43: \"tennis racket\",\n        44: \"bottle\",\n        45: \"plate\",\n        46: \"wine glass\",\n        47: \"cup\",\n        48: \"fork\",\n        49: \"knife\",\n        50: \"spoon\",\n        51: \"bowl\",\n        52: \"banana\",\n        53: \"apple\",\n        54: \"sandwich\",\n        55: \"orange\",\n        56: \"broccoli\",\n        57: \"carrot\",\n        58: \"hot dog\",\n        59: \"pizza\",\n        60: \"donut\",\n        61: \"cake\",\n        62: \"chair\",\n        63: \"couch\",\n        64: \"potted plant\",\n        65: \"bed\",\n        66: \"mirror\",\n        67: \"dining table\",\n        68: \"window\",\n        69: \"desk\",\n        70: \"toilet\",\n        71: \"door\",\n        72: \"tv\",\n        73: \"laptop\",\n        74: \"mouse\",\n        75: \"remote\",\n        76: \"keyboard\",\n        77: \"cell phone\",\n        78: \"microwave\",\n        79: \"oven\",\n        80: \"toaster\",\n        81: \"sink\",\n        82: \"refrigerator\",\n        83: \"blender\",\n        84: \"book\",\n        85: \"clock\",\n        86: \"vase\",\n        87: \"scissors\",\n        88: \"teddy bear\",\n        89: \"hair drier\",\n        90: \"toothbrush\",\n        91: \"hair brush\",\n    }\n\n    return coco_dict[int(id)]\n</code></pre>"},{"location":"code/transforms/labels/#transforms.labels.dota_ids","title":"<code>dota_ids(name)</code>","text":"<p>Return DOTAv2 category ID (18 classes)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>int</code> <p>Category name</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>int</code> <p>Category ID</p> Source code in <code>pixano/transforms/labels.py</code> <pre><code>def dota_ids(name: str) -&gt; int:\n\"\"\"Return DOTAv2 category ID (18 classes)\n\n    Args:\n        name (int): Category name\n\n    Returns:\n        str: Category ID\n    \"\"\"\n\n    dota_dict = {\n        \"plane\": 1,\n        \"ship\": 2,\n        \"storage tank\": 3,\n        \"baseball diamond\": 4,\n        \"tennis court\": 5,\n        \"basketball court\": 6,\n        \"ground track field\": 7,\n        \"harbor\": 8,\n        \"bridge\": 9,\n        \"large vehicle\": 10,\n        \"small vehicle\": 11,\n        \"helicopter\": 12,\n        \"roundabout\": 13,\n        \"soccer ball field\": 14,\n        \"swimming pool\": 15,\n        \"container crane\": 16,\n        \"airport\": 17,\n        \"helipad\": 18,\n    }\n\n    return dota_dict[str(name).replace(\"-\", \" \")]\n</code></pre>"},{"location":"code/transforms/labels/#transforms.labels.voc_names","title":"<code>voc_names(id)</code>","text":"<p>Return VOC category name (20 classes)</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>Category ID</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Category name</p> Source code in <code>pixano/transforms/labels.py</code> <pre><code>def voc_names(id: int) -&gt; str:\n\"\"\"Return VOC category name (20 classes)\n\n    Args:\n        id (int): Category ID\n\n    Returns:\n        str: Category name\n    \"\"\"\n\n    voc_dict = {\n        1: \"aeroplane\",\n        2: \"bicycle\",\n        3: \"bird\",\n        4: \"boat\",\n        5: \"bottle\",\n        6: \"bus\",\n        7: \"car\",\n        8: \"cat\",\n        9: \"chair\",\n        10: \"cow\",\n        11: \"dining table\",\n        12: \"dog\",\n        13: \"horse\",\n        14: \"motorbike\",\n        15: \"person\",\n        16: \"potted plant\",\n        17: \"sheep\",\n        18: \"sofa\",\n        19: \"train\",\n        20: \"tv / monitor\",\n    }\n\n    return voc_dict[int(id)]\n</code></pre>"},{"location":"user/","title":"Getting started with Pixano","text":"<ul> <li>Install Pixano</li> <li>Use your existing datasets<ul> <li>Check out this Jupyter notebook for importing your datasets</li> <li>Check out this Jupyter notebook for exporting your datasets</li> </ul> </li> <li>Use the Pixano apps<ul> <li>Learn how to launch an app</li> <li>Read our user guide for Pixano Explorer</li> <li>Read our user guide for Pixano Annotator</li> </ul> </li> </ul>"},{"location":"user/annotator/","title":"Using Pixano Annotator","text":""},{"location":"user/annotator/#home-page","title":"Home page","text":"<p>From the Annotator home page, you will be greeted with a list of all the Pixano format datasets found in the directory you provided.</p> <p>For each dataset, you can see its name, the number of elements inside it, and a thumbnail composed from six sample elements.</p> <p>You can hover over a dataset name to check the dataset description if it has one.</p> <p>You can click on a dataset to open its annotation page on its first element.</p>"},{"location":"user/annotator/#annotation-page","title":"Annotation page","text":""},{"location":"user/annotator/#element-view","title":"Element view","text":"<p>The selected element (image or images for multi-view datasets) is displayed.</p> <p>You can zoom in and out with the mouse wheel.</p> <p>You can grab and move images with the the middle click or with the Pan tool available in the left toolbar. </p> <p>You can double click on an image to move it above other images.</p> <p>Annotations, in form of segmentations mask, are displayed. Each object category is given a color.</p> <p>On the top of the image, when you have an input Tool selected, a panel is displayed that allow to choose a category.</p>"},{"location":"user/annotator/#left-toolbar","title":"Left toolbar","text":"<p>A toolbar is available on the left side of the page with the following tools:</p> <ul> <li>Pan: Allows you to grab and move an image</li> <li>Points: Allows you to place input points to interactively segment your image<ul> <li>You can place positive points with the + tool (points shown in green) to indicate what must be included in the segmentation</li> <li>You can place negative points with - tool (points shown in red) to indicate what must not be included in the segmentation</li> <li>You can hover over any point and press the Del key to remove it</li> <li>You can click and hold on any point to relocate it</li> </ul> </li> <li>Rectangle: Allows you to draw rectangles to interactively segment your image<ul> <li>You can click and drag on the image to draw a rectangle</li> <li>There can only be one rectangle at a time, so drawing a new rectangle will discard the previous one</li> </ul> </li> </ul> <p>The Points and Rectangle tools depend on an ONNX segmentation model you have to provide. Please look at the interactive annotation documentation and notebook for more information. </p> <p>More tools will be coming soon.</p>"},{"location":"user/annotator/#center-toolbar","title":"Center toolbar","text":"<p>When an annotation tool is selected, a toolbar is available on the center on the page with the following tools:</p> <ul> <li>A text box to enter the label name for your annotation or select the label from the list of existing labels</li> <li>If the Points tool is selected, a + icon and a - icon to quickly switch between positive and negative points</li> <li>A Validate icon to validate your annotation with the entered label</li> </ul>"},{"location":"user/annotator/#right-toolbar","title":"Right toolbar","text":"<p>A toolbar is available on the right side of the page with the following tabs:</p> <ul> <li>Labels: This tab displays your annotations grouped by views and by labels<ul> <li>Each annotation group can be opened or closed by clicking on it</li> <li>Each annotation and annotation group can be shown or hidden on the relevant image by clicking on the Visibility icon</li> <li>Each annotation can be deleted by clicking on the Delete icon</li> <li>Each annotation is represented by its unique ID.</li> </ul> </li> <li>Dataset: This tab allows you to navigate through the dataset<ul> <li>Each element of the dataset is displayed with its ID and its thumbnail</li> <li>The list will automatically expand as you scroll down</li> <li>You can click on any element to change current element</li> </ul> </li> </ul>"},{"location":"user/annotator/#annotating","title":"Annotating","text":"<p>You can currently annotate with the Points (+ and -), and Rectangle tools available in the left toolbar as described above.</p> <p>When using these tools, the generated annotation will be displayed in green. You can use the tools together to refine your annotation.</p> <p>You can press the Enter key to validate your annotation, or the Esc key to reset all your Points and Rectangle inputs.</p>"},{"location":"user/annotator/#saving","title":"Saving","text":"<p>To save your annotations, a Save icon is available in the top right as well. It will be highlighted if there are unsaved changes.</p> <p>If you try to go back to the home page or change element with unsaved changes, you will see a confirmation window. You can choose \"OK\" to discard your changes, or cancel to be able to go back save them.</p>"},{"location":"user/annotator/#going-home","title":"Going home","text":"<p>To go back to the home page, click on \"Pixano Annotator\" in the top left or on the Close icon in the top right.</p>"},{"location":"user/explorer/","title":"Using Pixano Explorer","text":""},{"location":"user/explorer/#home-page","title":"Home page","text":"<p>From the Explorer home page, you will be greeted with a list of all the Pixano format datasets found in the directory you provided.</p> <p>For each dataset, you can see its name, the number of elements inside it, and a thumbnail composed from six sample elements.</p> <p>You can hover over a dataset name to check the dataset description if it has one.</p> <p>You can click on a dataset to open its exploration page.</p>"},{"location":"user/explorer/#exploration-page","title":"Exploration page","text":""},{"location":"user/explorer/#statistics","title":"Statistics","text":"<p>Available statistics will be displayed on the left side of the dataset page.</p> <p>You can hover over different elements in the statistics to get more detailed information.</p> <p>Filtering your dataset based on the selected statistics will soon be available.</p>"},{"location":"user/explorer/#elements-list","title":"Elements list","text":"<p>The dataset elements will be displayed on the right side of the dataset page.</p> <p>Elements are displayed in scrollable pages of up to 100 elements.</p> <p>You can navigate between pages with the Previous and Next buttons at the bottom.</p> <p>You can click on any element to open it in the exploration page.</p> <p>For each element, you can see columns for its ID, a thumbnail for each of its media, and the split it comes from.</p> <p>Filtering your dataset based on these columns will soon be available.</p>"},{"location":"user/explorer/#going-home","title":"Going home","text":"<p>To go back to the home page, click on \"Pixano Annotator\" in the top left or on the Close icon in the top right.</p>"},{"location":"user/explorer/#element-view-page","title":"Element view page","text":""},{"location":"user/explorer/#element-view","title":"Element view","text":"<p>The selected element (image or images for multi-view datasets) is displayed.</p> <p>You can zoom in and out with the mouse wheel.</p> <p>You can grab and move images.</p> <p>You can double click on an image to move it above other images.</p> <p>Annotations, in form of segmentations mask and bounding boxes, are displayed. Each object category is given a color.</p> <p>For bounding boxes, you can see the category, and the bounding box confidence in case of inferences, on their top left corners.</p>"},{"location":"user/explorer/#right-toolbar","title":"Right toolbar","text":"<p>A toolbar is available on the right side of the page with the following sections:</p> <ul> <li> <p>A Data section to display information on the element, like its ID</p> </li> <li> <p>A Tools section to filter the annotations</p> <ul> <li>The Show all annotations checkbox allows you to toggle annotations visibility</li> <li>The Show bounding boxes checkbox allows you to toggle bounding box visibility</li> <li>The Mask opacity slider allows you to adjust the opacity of segmentations masks</li> <li>The Confidence threshold slider allows you to adjust the threshold displaying inferences</li> <li>The Labels list allows you to see the number of annotations for any label, and to toggle annotations visibility for individual labels by click on their names</li> </ul> </li> </ul> <p>More options to display ground truths and inference annotations separately and with different colors will be coming soon.</p> <p>More options for multi-view datasets will be coming soon.</p>"},{"location":"user/explorer/#going-home_1","title":"Going home","text":"<p>To go back to the home page, click on \"Pixano Annotator\" in the top left.</p> <p>To go back to the exploration page, click on the dataset name in the top left or on the Close icon in the top right, or press the Esc key.</p>"},{"location":"user/export/","title":"Exporting datasets","text":"<p>Please refer to this Jupyter notebook for information on how to export your datasets.</p>"},{"location":"user/import/","title":"Importing datasets","text":"<p>Please refer to this Jupyter notebook for information on how to import your datasets.</p>"},{"location":"user/install/","title":"Installing Pixano","text":"<p>As Pixano requires specific versions for its dependencies, we recommend creating a new Python virtual environment to install it.</p> <p>For example, with conda:</p> <pre><code>conda create -n pixano_env python=3.10\nconda activate pixano_env\n</code></pre> <p>Then, you can install the Pixano package inside that environment with pip:</p> <pre><code>pip install pixano\n</code></pre>"},{"location":"user/launch/","title":"Launching an app","text":""},{"location":"user/launch/#from-a-terminal","title":"From a terminal","text":"<p>You can start the Pixano Explorer and Annotator apps with the following commands:</p> <pre><code>pixano-explorer &lt;path/to/your/datasets&gt;\n</code></pre> <pre><code>pixano-annotator &lt;path/to/your/datasets&gt;\n</code></pre> <p>You will then be provided with a URL to open in your browser to use the app.</p>"},{"location":"user/launch/#from-a-notebook","title":"From a notebook","text":"<p>If you are using a notebook, you can start the Explorer and Annotator apps by running the following cells:</p> <pre><code>from pixano.apps import ExplorerApp\nexplorer = ExplorerApp(&lt;path/to/your/datasets&gt;)\n</code></pre> <pre><code>from pixano.apps import AnnotatorApp\nannotator = AnnotatorApp(&lt;path/to/your/datasets&gt;)\n</code></pre> <p>You can then use the apps directly from the notebook in another cell with:</p> <pre><code>explorer.display()\n</code></pre> <pre><code>annotator.display()\n</code></pre>"}]}